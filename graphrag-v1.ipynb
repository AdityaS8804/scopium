{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T04:36:35.485421Z",
     "iopub.status.busy": "2025-03-01T04:36:35.485084Z",
     "iopub.status.idle": "2025-03-01T04:37:23.851375Z",
     "shell.execute_reply": "2025-03-01T04:37:23.850454Z",
     "shell.execute_reply.started": "2025-03-01T04:36:35.485395Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'flask' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/pallets/flask.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install networkx arango langgraph langchain_openai langchain_community matplotlib scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T04:37:33.888301Z",
     "iopub.status.busy": "2025-03-01T04:37:33.887937Z",
     "iopub.status.idle": "2025-03-01T04:37:38.974728Z",
     "shell.execute_reply": "2025-03-01T04:37:38.974038Z",
     "shell.execute_reply.started": "2025-03-01T04:37:33.888276Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:09:34 +0530] [INFO]: NetworkX-cuGraph is unavailable: No module named 'cupy'.\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import ast\n",
    "from typing import Dict, Set, List, Tuple, Optional,Any\n",
    "import json\n",
    "from arango import ArangoClient\n",
    "import nx_arangodb as nxadb\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.graphs import ArangoGraph\n",
    "from langchain_community.chains.graph_qa.arangodb import ArangoGraphQAChain\n",
    "from langchain_core.tools import tool\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T04:38:30.721707Z",
     "iopub.status.busy": "2025-03-01T04:38:30.721365Z",
     "iopub.status.idle": "2025-03-01T04:38:31.163453Z",
     "shell.execute_reply": "2025-03-01T04:38:31.162755Z",
     "shell.execute_reply.started": "2025-03-01T04:38:30.721684Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import ast\n",
    "# import networkx as nx\n",
    "# from typing import Dict, Set, List\n",
    "# import json\n",
    "# from arango import ArangoClient\n",
    "# os.environ[\"MISTRAL_API_KEY\"]=\"jJAuJZkjVcy2ynUhan375sHNviHiBeJU\"\n",
    "\n",
    "# class CodebaseVisualizer:\n",
    "#     def __init__(self, root_dir: str):\n",
    "#         self.root_dir = root_dir\n",
    "#         self.graph = nx.DiGraph()\n",
    "#         self.file_contents: Dict[str, str] = {}\n",
    "#         self.import_relations: Dict[str, Set[str]] = {}\n",
    "#         self.module_symbols: Dict[str, Dict[str, Dict[str, int]]] = {}  # file -> {symbol -> {type, line_no}}\n",
    "#         self.file_index: Dict[str, int] = {}  # Maps files to indices\n",
    "#         self.current_index = 0\n",
    "#         self.directories: Set[str] = set()\n",
    "\n",
    "#     def _get_next_index(self) -> int:\n",
    "#         \"\"\"Get next available index for file indexing.\"\"\"\n",
    "#         self.current_index += 1\n",
    "#         return self.current_index\n",
    "\n",
    "#     def _chunk_code(self, code: str, lines_per_chunk: int = 20) -> List[Dict]:\n",
    "#         \"\"\"\n",
    "#         Chunk the given code into snippets.\n",
    "#         Returns a list of dictionaries with 'code_snippet', 'start_line', and 'end_line'.\n",
    "#         \"\"\"\n",
    "#         lines = code.splitlines()\n",
    "#         chunks = []\n",
    "#         for i in range(0, len(lines), lines_per_chunk):\n",
    "#             chunk_lines = lines[i:i + lines_per_chunk]\n",
    "#             chunk = {\n",
    "#                 'code_snippet': '\\n'.join(chunk_lines),\n",
    "#                 'start_line': i + 1,\n",
    "#                 'end_line': i + len(chunk_lines)\n",
    "#             }\n",
    "#             chunks.append(chunk)\n",
    "#         return chunks\n",
    "\n",
    "#     def parse_files(self) -> None:\n",
    "#         \"\"\"Parse all Python files in the directory and build relationships.\"\"\"\n",
    "#         # First pass: Index all files and create directory nodes\n",
    "#         for root, dirs, files in os.walk(self.root_dir):\n",
    "#             # Add directory node\n",
    "#             rel_dir = os.path.relpath(root, self.root_dir)\n",
    "#             if rel_dir != '.':\n",
    "#                 self.directories.add(rel_dir)\n",
    "#                 self.graph.add_node(rel_dir, type='directory')\n",
    "\n",
    "#             # Index Python files\n",
    "#             for file in files:\n",
    "#                 if file.endswith('.py'):\n",
    "#                     file_path = os.path.join(root, file)\n",
    "#                     rel_path = os.path.relpath(file_path, self.root_dir)\n",
    "#                     self.file_index[rel_path] = self._get_next_index()\n",
    "                    \n",
    "#                     try:\n",
    "#                         with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#                             content = f.read()\n",
    "#                             self.file_contents[rel_path] = content\n",
    "#                             self._analyze_file(rel_path, content)\n",
    "#                     except Exception as e:\n",
    "#                         print(f\"Error parsing {file_path}: {e}\")\n",
    "\n",
    "#     def _analyze_file(self, file_path: str, content: str) -> None:\n",
    "#         \"\"\"Analyze a single file for imports and symbols with line numbers.\"\"\"\n",
    "#         try:\n",
    "#             tree = ast.parse(content)\n",
    "#             imports = set()\n",
    "#             symbols = {}\n",
    "\n",
    "#             for node in ast.walk(tree):\n",
    "#                 # Track imports\n",
    "#                 if isinstance(node, (ast.Import, ast.ImportFrom)):\n",
    "#                     if isinstance(node, ast.Import):\n",
    "#                         for name in node.names:\n",
    "#                             imports.add((name.name, node.lineno))\n",
    "#                     else:  # ImportFrom\n",
    "#                         module = node.module if node.module else ''\n",
    "#                         imports.add((module, node.lineno))\n",
    "\n",
    "#                 # Track defined symbols with line numbers\n",
    "#                 elif isinstance(node, (ast.FunctionDef, ast.ClassDef)):\n",
    "#                     symbols[node.name] = {\n",
    "#                         'type': 'class' if isinstance(node, ast.ClassDef) else 'function',\n",
    "#                         'line_no': node.lineno\n",
    "#                     }\n",
    "\n",
    "#             self.import_relations[file_path] = imports\n",
    "#             self.module_symbols[file_path] = symbols\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error analyzing {file_path}: {e}\")\n",
    "\n",
    "#     def build_graph(self) -> nx.DiGraph:\n",
    "#         \"\"\"Build the NetworkX graph with enhanced node and edge information.\"\"\"\n",
    "#         # Start with a directed graph for clarity in relationships\n",
    "#         dot_graph = nx.DiGraph()\n",
    "        \n",
    "#         # Add nodes for all files with indices and code snippet nodes\n",
    "#         for file_path, file_idx in self.file_index.items():\n",
    "#             dot_graph.add_node(file_path, \n",
    "#                                type='file',\n",
    "#                                file_index=file_idx,\n",
    "#                                directory=os.path.dirname(file_path))\n",
    "            \n",
    "#             # Create snippet nodes for the entire file\n",
    "#             if file_path in self.file_contents:\n",
    "#                 chunks = self._chunk_code(self.file_contents[file_path])\n",
    "#                 for idx, chunk_info in enumerate(chunks):\n",
    "#                     snippet_node = f\"{file_path}::snippet::{idx}\"\n",
    "#                     dot_graph.add_node(snippet_node,\n",
    "#                                        type='snippet',\n",
    "#                                        code_snippet=chunk_info['code_snippet'],\n",
    "#                                        start_line=chunk_info['start_line'],\n",
    "#                                        end_line=chunk_info['end_line'])\n",
    "#                     # Connect file node to snippet node\n",
    "#                     dot_graph.add_edge(file_path, snippet_node, \n",
    "#                                        edge_type='contains_snippet',\n",
    "#                                        start_line=chunk_info['start_line'],\n",
    "#                                        end_line=chunk_info['end_line'])\n",
    "\n",
    "#             # Add nodes for symbols in this file\n",
    "#             for symbol, details in self.module_symbols.get(file_path, {}).items():\n",
    "#                 symbol_node = f\"{file_path}::{symbol}\"\n",
    "#                 dot_graph.add_node(symbol_node, \n",
    "#                                    type='symbol',\n",
    "#                                    symbol_type=details['type'],\n",
    "#                                    line_number=details['line_no'])\n",
    "#                 dot_graph.add_edge(file_path, symbol_node, \n",
    "#                                    edge_type='defines',\n",
    "#                                    line_number=details['line_no'])\n",
    "\n",
    "#         # Add directory nodes\n",
    "#         for directory in self.directories:\n",
    "#             dot_graph.add_node(directory, type='directory')\n",
    "\n",
    "#         # Add edges for imports with line numbers\n",
    "#         for file_path, imports in self.import_relations.items():\n",
    "#             for imp, line_no in imports:\n",
    "#                 # Look for matching files or symbols\n",
    "#                 for target_file, symbols in self.module_symbols.items():\n",
    "#                     if imp in symbols:\n",
    "#                         dot_graph.add_edge(file_path, \n",
    "#                                            f\"{target_file}::{imp}\",\n",
    "#                                            edge_type='import',\n",
    "#                                            line_number=line_no)\n",
    "#                     elif target_file.replace('.py', '').endswith(imp):\n",
    "#                         dot_graph.add_edge(file_path, \n",
    "#                                            target_file,\n",
    "#                                            edge_type='import',\n",
    "#                                            line_number=line_no)\n",
    "        \n",
    "#         # Save the built graph in self.graph for later export (to ArangoDB, JSON, etc.)\n",
    "#         self.graph = dot_graph\n",
    "#         return dot_graph\n",
    "\n",
    "#     def export_file_index(self, output_path: str = 'file_index.txt') -> None:\n",
    "#         \"\"\"Export the file index mapping to a text file.\"\"\"\n",
    "#         with open(output_path, 'w') as f:\n",
    "#             # Write header\n",
    "#             f.write(\"File Index Mapping\\n\")\n",
    "#             f.write(\"=================\\n\\n\")\n",
    "            \n",
    "#             # Sort by index for better readability\n",
    "#             sorted_items = sorted(self.file_index.items(), key=lambda x: x[1])\n",
    "            \n",
    "#             # Write each file and its index\n",
    "#             for file_path, index in sorted_items:\n",
    "#                 f.write(f\"{index}: {file_path}\\n\")\n",
    "                \n",
    "#                 # If there are symbols in this file, list them with line numbers\n",
    "#                 if file_path in self.module_symbols:\n",
    "#                     f.write(\"  Symbols:\\n\")\n",
    "#                     for symbol, details in self.module_symbols[file_path].items():\n",
    "#                         symbol_type = details['type']\n",
    "#                         line_no = details['line_no']\n",
    "#                         f.write(f\"    - {symbol} ({symbol_type}) at line {line_no}\\n\")\n",
    "#                     f.write(\"\\n\")\n",
    "\n",
    "#     def export_graph_json(self, output_path: str = 'codebase_graph.json') -> None:\n",
    "#         \"\"\"Export the graph structure to JSON.\"\"\"\n",
    "#         graph_data = {\n",
    "#             'nodes': [\n",
    "#                 {\n",
    "#                     'id': node,\n",
    "#                     'type': data['type'],\n",
    "#                     'file_index': data.get('file_index'),\n",
    "#                     'directory': data.get('directory'),\n",
    "#                     'symbol_type': data.get('symbol_type'),\n",
    "#                     'line_number': data.get('line_number'),\n",
    "#                     'code_snippet': data.get('code_snippet'),\n",
    "#                     'start_line': data.get('start_line'),\n",
    "#                     'end_line': data.get('end_line')\n",
    "#                 } \n",
    "#                 for node, data in self.graph.nodes(data=True)\n",
    "#             ],\n",
    "#             'links': [\n",
    "#                 {\n",
    "#                     'source': source,\n",
    "#                     'target': target,\n",
    "#                     'type': data.get('edge_type'),\n",
    "#                     'line_number': data.get('line_number'),\n",
    "#                     'start_line': data.get('start_line'),\n",
    "#                     'end_line': data.get('end_line')\n",
    "#                 } \n",
    "#                 for source, target, data in self.graph.edges(data=True)\n",
    "#             ]\n",
    "#         }\n",
    "        \n",
    "#         with open(output_path, 'w') as f:\n",
    "#             json.dump(graph_data, f, indent=2)\n",
    "#     def export_to_arango(self,\n",
    "#                          db_name: str = 'codebase',\n",
    "#                          username: str = 'root',\n",
    "#                          password: str = 'passwd',\n",
    "#                          host: str = 'http://localhost:8529') -> None:\n",
    "#         \"\"\"\n",
    "#         Export the graph into ArangoDB.\n",
    "#         IMPORTANT: This method first deletes any existing graph and associated data in ArangoDB.\n",
    "#         \"\"\"\n",
    "#         client = ArangoClient(hosts=host)\n",
    "#         db = client.db(username=username, password=password,verify=True)\n",
    "\n",
    "#         # Delete the existing graph and its collections if they exist.\n",
    "#         graph_name = \"FlaskRepv1\"\n",
    "#         '''\n",
    "#         if db.has_graph(graph_name):\n",
    "#             try:\n",
    "#                 # Delete the entire graph and its collections.\n",
    "#                 db.graph(graph_name).delete(delete_collections=True)\n",
    "#                 print(\"Existing ArangoDB graph deleted.\")\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error deleting graph: {e}\")\n",
    "#                 '''\n",
    "        \n",
    "#         G_adb = nxadb.Graph(\n",
    "#             name=graph_name,\n",
    "#             db=db,\n",
    "#             incoming_graph_data=self.graph,\n",
    "#             write_batch_size=50000,\n",
    "#             overwrite_graph=True\n",
    "#         )\n",
    "        \n",
    "#         self.G_adb=G_adb\n",
    "#         return G_adb\n",
    "#         print(\"Graph successfully exported to ArangoDB.\")\n",
    "#     def text_to_nx_algorithm_to_text(self,query):\n",
    "#         \"\"\"This tool is available to invoke a NetworkX Algorithm on\n",
    "#         the ArangoDB Graph. You are responsible for accepting the\n",
    "#         Natural Language Query, establishing which algorithm needs to\n",
    "#         be executed, executing the algorithm, and translating the results back\n",
    "#         to Natural Language, with respect to the original query.\n",
    "    \n",
    "#         If the query (e.g traversals, shortest path, etc.) can be solved using the Arango Query Language, then do not use\n",
    "#         this tool.\n",
    "#         \"\"\"\n",
    "#         llm = ChatMistralAI(\n",
    "#             model=\"mistral-large-latest\",\n",
    "#             temperature=0,\n",
    "#             max_retries=2,\n",
    "#             # other params...\n",
    "#         )\n",
    "#         ######################\n",
    "#         print(\"1) Generating NetworkX code\")\n",
    "    \n",
    "#         text_to_nx = llm.invoke(f\"\"\"\n",
    "#         I have a NetworkX Graph called `G_adb`. It has the following schema: {arango_graph.schema}\n",
    "    \n",
    "#         I have the following graph analysis query: {query}.\n",
    "    \n",
    "#         Generate the Python Code required to answer the query using the `G_adb` object.\n",
    "        \n",
    "#         It should give code so that is gives ALL the necessary contents that use this part of the code in terms of ALL nested imports. Consider the nested directories and sub-directory informations as well.\n",
    "    \n",
    "#         Be very precise on the NetworkX algorithm you select to answer this query. Think step by step.\n",
    "    \n",
    "#         Only assume that networkx is installed, and other base python dependencies.\n",
    "    \n",
    "#         Always set the last variable as `FINAL_RESULT`, which represents the answer to the original query.\n",
    "    \n",
    "#         Only provide python code that I can directly execute via `exec()`. Do not provide any instructions.\n",
    "    \n",
    "#         Make sure that `FINAL_RESULT` stores a short & consice answer. Avoid setting this variable to a long sequence.\n",
    "    \n",
    "#         Your code:\n",
    "#         \"\"\").content\n",
    "    \n",
    "#         text_to_nx_cleaned = re.sub(r\"^```python\\n|```$\", \"\", text_to_nx, flags=re.MULTILINE).strip()\n",
    "        \n",
    "#         print('-'*10)\n",
    "#         print(text_to_nx_cleaned)\n",
    "#         print('-'*10)\n",
    "    \n",
    "#         ######################\n",
    "    \n",
    "#         print(\"\\n2) Executing NetworkX code\")\n",
    "#         global_vars = {\"G_adb\":self.G_adb , \"nx\": nx}\n",
    "#         local_vars = {}\n",
    "    \n",
    "#         try:\n",
    "#             exec(text_to_nx_cleaned, global_vars, local_vars)\n",
    "#             text_to_nx_final = text_to_nx\n",
    "#         except Exception as e:\n",
    "#             print(f\"EXEC ERROR: {e}\")\n",
    "#             return f\"EXEC ERROR: {e}\"\n",
    "    \n",
    "#             # TODO: Consider experimenting with a code corrector!\n",
    "#             attempt = 1\n",
    "#             MAX_ATTEMPTS = 3\n",
    "    \n",
    "#             # while attempt <= MAX_ATTEMPTS\n",
    "#                 # ...\n",
    "    \n",
    "#         print('-'*10)\n",
    "#         FINAL_RESULT = local_vars[\"FINAL_RESULT\"]\n",
    "#         print(f\"FINAL_RESULT: {FINAL_RESULT}\")\n",
    "#         print('-'*10)\n",
    "    \n",
    "#         ######################\n",
    "    \n",
    "#         print(\"3) Formulating final answer\")\n",
    "    \n",
    "#         nx_to_text = llm.invoke(f\"\"\"\n",
    "#             I have a NetworkX Graph called `G_adb`. It has the following schema: {arango_graph.schema}\n",
    "    \n",
    "#             I have the following graph analysis query: {query}.\n",
    "    \n",
    "#             I have executed the following python code to help me answer my query:\n",
    "    \n",
    "#             ---\n",
    "#             {text_to_nx_final}\n",
    "#             ---\n",
    "    \n",
    "#             The `FINAL_RESULT` variable is set to the following: {FINAL_RESULT}.\n",
    "    \n",
    "#             Based on my original Query and FINAL_RESULT, generate a short and concise response to\n",
    "#             answer my query.\n",
    "            \n",
    "#             Your response:\n",
    "#         \"\"\").content\n",
    "    \n",
    "#         return nx_to_text\n",
    "\n",
    "# # Example usage\n",
    "\n",
    "# # Initialize and parse the codebase\n",
    "# visualizer = CodebaseVisualizer(\"flask\")\n",
    "# visualizer.parse_files()\n",
    "# visualizer.build_graph()\n",
    "    \n",
    "# # Export the file index and JSON (for local inspection)\n",
    "# visualizer.export_file_index()\n",
    "# visualizer.export_graph_json()\n",
    "# # Export the enriched graph to ArangoDB (this will delete any existing graph data first)\n",
    "# #visualizer.export_to_arango(db_name=\"FlaskRepv1\",username=\"root\",password=\"cUZ0YaNdcwfUTw6VjRny\",host=\"http://localhost:8529\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import networkx as nx\n",
    "from typing import Dict, Set, List, Tuple, Optional\n",
    "import json\n",
    "from arango import ArangoClient\n",
    "import re\n",
    "\n",
    "class CodebaseVisualizer:\n",
    "    def __init__(self, root_dir: str):\n",
    "        self.root_dir = root_dir\n",
    "        self.graph = nx.DiGraph()\n",
    "        self.file_contents: Dict[str, str] = {}\n",
    "        self.import_relations: Dict[str, List[Tuple[str, int]]] = {}  # file -> [(module, line_no)]\n",
    "        self.module_symbols: Dict[str, Dict[str, Dict[str, any]]] = {}  # file -> {symbol -> {type, line_no, context}}\n",
    "        self.symbol_references: Dict[str, List[Tuple[str, int]]] = {}  # symbol -> [(file, line_no)]\n",
    "        self.file_index: Dict[str, int] = {}  # Maps files to indices\n",
    "        self.current_index = 0\n",
    "        self.directories: Set[str] = set()\n",
    "\n",
    "    def _get_next_index(self) -> int:\n",
    "        \"\"\"Get next available index for file indexing.\"\"\"\n",
    "        self.current_index += 1\n",
    "        return self.current_index\n",
    "\n",
    "    def _chunk_code(self, code: str, lines_per_chunk: int = 20) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Chunk the given code into snippets.\n",
    "        Returns a list of dictionaries with 'code_snippet', 'start_line', and 'end_line'.\n",
    "        \"\"\"\n",
    "        lines = code.splitlines()\n",
    "        chunks = []\n",
    "        for i in range(0, len(lines), lines_per_chunk):\n",
    "            chunk_lines = lines[i:i + lines_per_chunk]\n",
    "            chunk = {\n",
    "                'code_snippet': '\\n'.join(chunk_lines),\n",
    "                'start_line': i + 1,\n",
    "                'end_line': i + len(chunk_lines)\n",
    "            }\n",
    "            chunks.append(chunk)\n",
    "        return chunks\n",
    "\n",
    "    def _get_context_around_line(self, file_path: str, line_no: int, context_lines: int = 3) -> str:\n",
    "        \"\"\"Extract context around a specific line in a file.\"\"\"\n",
    "        if file_path not in self.file_contents:\n",
    "            return \"\"\n",
    "        \n",
    "        lines = self.file_contents[file_path].splitlines()\n",
    "        start = max(0, line_no - context_lines - 1)\n",
    "        end = min(len(lines), line_no + context_lines)\n",
    "        \n",
    "        context = \"\\n\".join(lines[start:end])\n",
    "        return context\n",
    "\n",
    "    def parse_files(self) -> None:\n",
    "        \"\"\"Parse all Python files in the directory and build relationships.\"\"\"\n",
    "        # First pass: Index all files and create directory nodes\n",
    "        for root, dirs, files in os.walk(self.root_dir):\n",
    "            # Add directory node\n",
    "            rel_dir = os.path.relpath(root, self.root_dir)\n",
    "            if rel_dir != '.':\n",
    "                self.directories.add(rel_dir)\n",
    "                self.graph.add_node(rel_dir, type='directory')\n",
    "\n",
    "            # Index Python files\n",
    "            for file in files:\n",
    "                if file.endswith('.py'):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    rel_path = os.path.relpath(file_path, self.root_dir)\n",
    "                    self.file_index[rel_path] = self._get_next_index()\n",
    "                    \n",
    "                    try:\n",
    "                        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                            content = f.read()\n",
    "                            self.file_contents[rel_path] = content\n",
    "                            self._analyze_file(rel_path, content)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error parsing {file_path}: {e}\")\n",
    "        \n",
    "        # Second pass: Find symbol references across files\n",
    "        self._find_symbol_references()\n",
    "\n",
    "    def _analyze_file(self, file_path: str, content: str) -> None:\n",
    "        \"\"\"Analyze a single file for imports and symbols with line numbers and context.\"\"\"\n",
    "        try:\n",
    "            tree = ast.parse(content)\n",
    "            imports = []\n",
    "            symbols = {}\n",
    "\n",
    "            for node in ast.walk(tree):\n",
    "                # Track imports\n",
    "                if isinstance(node, (ast.Import, ast.ImportFrom)):\n",
    "                    if isinstance(node, ast.Import):\n",
    "                        for name in node.names:\n",
    "                            imports.append((name.name, node.lineno))\n",
    "                    else:  # ImportFrom\n",
    "                        module = node.module if node.module else ''\n",
    "                        for name in node.names:\n",
    "                            imports.append((f\"{module}.{name.name}\" if module else name.name, node.lineno))\n",
    "\n",
    "                # Track defined symbols with line numbers and context\n",
    "                elif isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.Assign)):\n",
    "                    if isinstance(node, (ast.FunctionDef, ast.ClassDef)):\n",
    "                        symbol_name = node.name\n",
    "                        symbol_type = 'class' if isinstance(node, ast.ClassDef) else 'function'\n",
    "                        line_no = node.lineno\n",
    "                        context = self._extract_node_source(content, node)\n",
    "                        \n",
    "                        symbols[symbol_name] = {\n",
    "                            'type': symbol_type,\n",
    "                            'line_no': line_no,\n",
    "                            'context': context,\n",
    "                            'docstring': ast.get_docstring(node)\n",
    "                        }\n",
    "                    elif isinstance(node, ast.Assign):\n",
    "                        # Handle variable assignments\n",
    "                        for target in node.targets:\n",
    "                            if isinstance(target, ast.Name):\n",
    "                                symbol_name = target.id\n",
    "                                line_no = node.lineno\n",
    "                                context = self._extract_node_source(content, node)\n",
    "                                \n",
    "                                symbols[symbol_name] = {\n",
    "                                    'type': 'variable',\n",
    "                                    'line_no': line_no,\n",
    "                                    'context': context\n",
    "                                }\n",
    "\n",
    "            self.import_relations[file_path] = imports\n",
    "            self.module_symbols[file_path] = symbols\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing {file_path}: {e}\")\n",
    "\n",
    "    def _extract_node_source(self, source: str, node) -> str:\n",
    "        \"\"\"Extract the source code for an AST node.\"\"\"\n",
    "        try:\n",
    "            lines = source.splitlines()\n",
    "            if hasattr(node, 'lineno') and hasattr(node, 'end_lineno'):\n",
    "                start = node.lineno - 1\n",
    "                end = getattr(node, 'end_lineno', start + 1)\n",
    "                return '\\n'.join(lines[start:end])\n",
    "            return \"\"\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "\n",
    "    def _find_symbol_references(self) -> None:\n",
    "        \"\"\"Find references to symbols across all files.\"\"\"\n",
    "        for file_path, content in self.file_contents.items():\n",
    "            try:\n",
    "                tree = ast.parse(content)\n",
    "                self._process_file_for_references(file_path, tree, content)\n",
    "            except Exception as e:\n",
    "                print(f\"Error finding references in {file_path}: {e}\")\n",
    "\n",
    "    def _process_file_for_references(self, file_path: str, tree, source: str) -> None:\n",
    "        \"\"\"Process a file's AST to find references to symbols.\"\"\"\n",
    "        for node in ast.walk(tree):\n",
    "            # Find variable references\n",
    "            if isinstance(node, ast.Name) and isinstance(node.ctx, ast.Load):\n",
    "                symbol_name = node.id\n",
    "                line_no = node.lineno\n",
    "                \n",
    "                # Track reference with context\n",
    "                if symbol_name not in self.symbol_references:\n",
    "                    self.symbol_references[symbol_name] = []\n",
    "                \n",
    "                context = self._get_context_around_line(file_path, line_no)\n",
    "                self.symbol_references[symbol_name].append((file_path, line_no, context))\n",
    "            \n",
    "            # Find attribute references (e.g., obj.method())\n",
    "            elif isinstance(node, ast.Attribute) and isinstance(node.ctx, ast.Load):\n",
    "                attr_name = node.attr\n",
    "                line_no = node.lineno\n",
    "                \n",
    "                if attr_name not in self.symbol_references:\n",
    "                    self.symbol_references[attr_name] = []\n",
    "                \n",
    "                context = self._get_context_around_line(file_path, line_no)\n",
    "                self.symbol_references[attr_name].append((file_path, line_no, context))\n",
    "\n",
    "    def build_graph(self) -> nx.DiGraph:\n",
    "        \"\"\"Build the NetworkX graph with enhanced node and edge information.\"\"\"\n",
    "        # Start with a directed graph for clarity in relationships\n",
    "        dot_graph = nx.DiGraph()\n",
    "        \n",
    "        # Add nodes for all files with indices and code snippet nodes\n",
    "        for file_path, file_idx in self.file_index.items():\n",
    "            dot_graph.add_node(file_path, \n",
    "                               type='file',\n",
    "                               file_index=file_idx,\n",
    "                               directory=os.path.dirname(file_path))\n",
    "            \n",
    "            # Create snippet nodes for the entire file\n",
    "            if file_path in self.file_contents:\n",
    "                chunks = self._chunk_code(self.file_contents[file_path])\n",
    "                for idx, chunk_info in enumerate(chunks):\n",
    "                    snippet_node = f\"{file_path}::snippet::{idx}\"\n",
    "                    dot_graph.add_node(snippet_node,\n",
    "                                       type='snippet',\n",
    "                                       code_snippet=chunk_info['code_snippet'],\n",
    "                                       start_line=chunk_info['start_line'],\n",
    "                                       end_line=chunk_info['end_line'])\n",
    "                    # Connect file node to snippet node\n",
    "                    dot_graph.add_edge(file_path, snippet_node, \n",
    "                                       edge_type='contains_snippet',\n",
    "                                       start_line=chunk_info['start_line'],\n",
    "                                       end_line=chunk_info['end_line'])\n",
    "\n",
    "            # Add nodes for symbols in this file\n",
    "            for symbol, details in self.module_symbols.get(file_path, {}).items():\n",
    "                symbol_node = f\"{file_path}::{symbol}\"\n",
    "                dot_graph.add_node(symbol_node, \n",
    "                                   type='symbol',\n",
    "                                   symbol_type=details['type'],\n",
    "                                   line_number=details['line_no'],\n",
    "                                   context=details.get('context', ''),\n",
    "                                   docstring=details.get('docstring', ''))\n",
    "                dot_graph.add_edge(file_path, symbol_node, \n",
    "                                   edge_type='defines',\n",
    "                                   line_number=details['line_no'])\n",
    "\n",
    "        # Add directory nodes\n",
    "        for directory in self.directories:\n",
    "            dot_graph.add_node(directory, type='directory')\n",
    "\n",
    "        # Add edges for imports with line numbers\n",
    "        for file_path, imports in self.import_relations.items():\n",
    "            for imp, line_no in imports:\n",
    "                # Look for matching files or symbols\n",
    "                for target_file, symbols in self.module_symbols.items():\n",
    "                    if imp in symbols:\n",
    "                        dot_graph.add_edge(file_path, \n",
    "                                           f\"{target_file}::{imp}\",\n",
    "                                           edge_type='import',\n",
    "                                           line_number=line_no)\n",
    "                    elif target_file.replace('.py', '').endswith(imp):\n",
    "                        dot_graph.add_edge(file_path, \n",
    "                                           target_file,\n",
    "                                           edge_type='import',\n",
    "                                           line_number=line_no)\n",
    "        \n",
    "        # Add edges for symbol references with line numbers and context\n",
    "        for symbol, references in self.symbol_references.items():\n",
    "            for file_path, symbols in self.module_symbols.items():\n",
    "                if symbol in symbols:\n",
    "                    symbol_node = f\"{file_path}::{symbol}\"\n",
    "                    \n",
    "                    # Connect symbol to all its references\n",
    "                    for ref_file, ref_line, context in references:\n",
    "                        if ref_file != file_path:  # Only add cross-file references\n",
    "                            dot_graph.add_edge(ref_file, \n",
    "                                              symbol_node,\n",
    "                                              edge_type='references',\n",
    "                                              line_number=ref_line,\n",
    "                                              context=context)\n",
    "        \n",
    "        # Save the built graph in self.graph for later export\n",
    "        self.graph = dot_graph\n",
    "        return dot_graph\n",
    "\n",
    "    def find_symbol_usages(self, symbol_name: str) -> List[Dict]:\n",
    "        \"\"\"Find all usages of a symbol in the codebase with context.\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # Look for the symbol in defined symbols\n",
    "        for file_path, symbols in self.module_symbols.items():\n",
    "            if symbol_name in symbols:\n",
    "                details = symbols[symbol_name]\n",
    "                results.append({\n",
    "                    'file': file_path,\n",
    "                    'type': 'definition',\n",
    "                    'line': details['line_no'],\n",
    "                    'symbol_type': details['type'],\n",
    "                    'context': details.get('context', ''),\n",
    "                    'docstring': details.get('docstring', '')\n",
    "                })\n",
    "        \n",
    "        # Look for references to the symbol\n",
    "        if symbol_name in self.symbol_references:\n",
    "            for file_path, line_no, context in self.symbol_references[symbol_name]:\n",
    "                results.append({\n",
    "                    'file': file_path,\n",
    "                    'type': 'reference',\n",
    "                    'line': line_no,\n",
    "                    'context': context\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def export_file_index(self, output_path: str = 'file_index.txt') -> None:\n",
    "        \"\"\"Export the file index mapping to a text file.\"\"\"\n",
    "        with open(output_path, 'w') as f:\n",
    "            # Write header\n",
    "            f.write(\"File Index Mapping\\n\")\n",
    "            f.write(\"=================\\n\\n\")\n",
    "            \n",
    "            # Sort by index for better readability\n",
    "            sorted_items = sorted(self.file_index.items(), key=lambda x: x[1])\n",
    "            \n",
    "            # Write each file and its index\n",
    "            for file_path, index in sorted_items:\n",
    "                f.write(f\"{index}: {file_path}\\n\")\n",
    "                \n",
    "                # If there are symbols in this file, list them with line numbers\n",
    "                if file_path in self.module_symbols:\n",
    "                    f.write(\"  Symbols:\\n\")\n",
    "                    for symbol, details in self.module_symbols[file_path].items():\n",
    "                        symbol_type = details['type']\n",
    "                        line_no = details['line_no']\n",
    "                        f.write(f\"    - {symbol} ({symbol_type}) at line {line_no}\\n\")\n",
    "                    f.write(\"\\n\")\n",
    "\n",
    "    def _analyze_symbol_purpose(self, symbol_name, usages):\n",
    "        \"\"\"Analyze the purpose of a symbol based on its usage patterns.\"\"\"\n",
    "        # Get the definition if available\n",
    "        definitions = [u for u in usages if u['type'] == 'definition']\n",
    "        references = [u for u in usages if u['type'] == 'reference']\n",
    "        \n",
    "        purpose = \"\"\n",
    "        \n",
    "        # Check if we have a definition with docstring\n",
    "        if definitions and definitions[0].get('docstring'):\n",
    "            purpose += f\"{definitions[0]['docstring']}\\n\\n\"\n",
    "        \n",
    "        # If it's a function, try to infer what it does from usage\n",
    "        if definitions and definitions[0].get('symbol_type') == 'function':\n",
    "            # Collect contexts where it's used\n",
    "            contexts = [ref.get('context', '') for ref in references if ref.get('context')]\n",
    "            \n",
    "            # Analyze contexts for patterns\n",
    "            if contexts:\n",
    "                common_patterns = self._find_common_usage_patterns(contexts, symbol_name)\n",
    "                \n",
    "                purpose += \"Based on usage patterns, this function appears to:\\n\"\n",
    "                \n",
    "                if any(\"assert\" in ctx for ctx in contexts):\n",
    "                    purpose += \"- Be used in test assertions to verify behavior\\n\"\n",
    "                \n",
    "                if any(\"if\" in ctx and symbol_name in ctx for ctx in contexts):\n",
    "                    purpose += \"- Be used as a condition in control flow statements\\n\"\n",
    "                \n",
    "                if common_patterns:\n",
    "                    for pattern in common_patterns[:3]:  # Top 3 patterns\n",
    "                        purpose += f\"- {pattern}\\n\"\n",
    "        \n",
    "        # If we couldn't infer much, provide a generic description\n",
    "        if not purpose or len(purpose.strip()) < 10:\n",
    "            if definitions and definitions[0].get('symbol_type') == 'function':\n",
    "                context = definitions[0].get('context', '')\n",
    "                \n",
    "                # Look for parameters to understand what it takes\n",
    "                params = self._extract_function_params(context)\n",
    "                \n",
    "                purpose += f\"This function appears to check or validate something\"\n",
    "                if params:\n",
    "                    purpose += f\" related to {', '.join(params)}\"\n",
    "                purpose += \".\\n\"\n",
    "        \n",
    "        return purpose\n",
    "\n",
    "    def _find_common_usage_patterns(self, contexts, symbol_name):\n",
    "        \"\"\"Find common patterns in the usage contexts.\"\"\"\n",
    "        patterns = []\n",
    "        \n",
    "        # Check if it's used with certain objects/methods frequently\n",
    "        if any(f\".{symbol_name}\" in ctx for ctx in contexts):\n",
    "            patterns.append(\"Be a method called on objects\")\n",
    "        \n",
    "        # Check if it's used for configuration or setup\n",
    "        if any(\"config\" in ctx.lower() or \"setup\" in ctx.lower() for ctx in contexts):\n",
    "            patterns.append(\"Be involved in configuration or setup\")\n",
    "        \n",
    "        # Check if it's used for logging\n",
    "        if any(\"log\" in ctx.lower() for ctx in contexts):\n",
    "            patterns.append(\"Be related to logging functionality\")\n",
    "        \n",
    "        # Check if it's used in exception handling\n",
    "        if any(\"except\" in ctx.lower() or \"try\" in ctx.lower() for ctx in contexts):\n",
    "            patterns.append(\"Be involved in exception handling\")\n",
    "        \n",
    "        return patterns\n",
    "\n",
    "    def _extract_function_params(self, context):\n",
    "        \"\"\"Extract parameter names from a function definition.\"\"\"\n",
    "        params = []\n",
    "        if context:\n",
    "            # Simple regex-based extraction\n",
    "            match = re.search(r'def\\s+\\w+\\s*\\((.*?)\\)', context, re.DOTALL)\n",
    "            if match:\n",
    "                param_string = match.group(1)\n",
    "                # Split by comma and clean up\n",
    "                raw_params = [p.strip() for p in param_string.split(',')]\n",
    "                # Extract just the parameter name (before any : or =)\n",
    "                params = [re.split(r'[=:]', p)[0].strip() for p in raw_params if p]\n",
    "                # Remove self if it's there\n",
    "                if params and params[0] == 'self':\n",
    "                    params = params[1:]\n",
    "        return params\n",
    "\n",
    "    def _general_codebase_analysis(self, query):\n",
    "        \"\"\"Provide a general analysis based on the query.\"\"\"\n",
    "        response = f\"# Analysis for Query: {query}\\n\\n\"\n",
    "        \n",
    "        # Check if the query is asking about structure\n",
    "        if any(term in query.lower() for term in ['structure', 'organization', 'layout']):\n",
    "            response += \"## Codebase Structure\\n\\n\"\n",
    "            # Count files by directory\n",
    "            files_by_dir = {}\n",
    "            for file_path in self.file_index:\n",
    "                directory = os.path.dirname(file_path)\n",
    "                if directory not in files_by_dir:\n",
    "                    files_by_dir[directory] = []\n",
    "                files_by_dir[directory].append(file_path)\n",
    "            \n",
    "            response += f\"The codebase contains {len(self.file_index)} Python files across {len(files_by_dir)} directories.\\n\\n\"\n",
    "            \n",
    "            # Show top-level directories\n",
    "            response += \"Main directories:\\n\"\n",
    "            for directory, files in sorted(files_by_dir.items(), key=lambda x: len(x[1]), reverse=True)[:10]:\n",
    "                dir_name = directory if directory else '(root)'\n",
    "                response += f\"- {dir_name}: {len(files)} files\\n\"\n",
    "        \n",
    "        # Check if the query is asking about specific functionality\n",
    "        functionality_terms = ['handle', 'process', 'create', 'generate', 'calculate']\n",
    "        for term in functionality_terms:\n",
    "            if term in query.lower():\n",
    "                # Search for functions with this term\n",
    "                matching_functions = []\n",
    "                for file_path, symbols in self.module_symbols.items():\n",
    "                    for symbol, details in symbols.items():\n",
    "                        if details['type'] == 'function' and term in symbol.lower():\n",
    "                            matching_functions.append((file_path, symbol, details))\n",
    "                \n",
    "                if matching_functions:\n",
    "                    response += f\"\\n## Functions Related to '{term}'\\n\\n\"\n",
    "                    for file_path, symbol, details in matching_functions[:5]:  # Show top 5\n",
    "                        response += f\"- `{symbol}` in {file_path}:{details['line_no']}\\n\"\n",
    "                    if len(matching_functions) > 5:\n",
    "                        response += f\"... and {len(matching_functions) - 5} more functions\\n\"\n",
    "        \n",
    "        return response\n",
    "\n",
    "    def export_graph_json(self, output_path: str = 'codebase_graph.json') -> None:\n",
    "        \"\"\"Export the graph structure to JSON.\"\"\"\n",
    "        graph_data = {\n",
    "            'nodes': [\n",
    "                {\n",
    "                    'id': node,\n",
    "                    'type': data['type'],\n",
    "                    'file_index': data.get('file_index'),\n",
    "                    'directory': data.get('directory'),\n",
    "                    'symbol_type': data.get('symbol_type'),\n",
    "                    'line_number': data.get('line_number'),\n",
    "                    'code_snippet': data.get('code_snippet'),\n",
    "                    'start_line': data.get('start_line'),\n",
    "                    'end_line': data.get('end_line'),\n",
    "                    'context': data.get('context'),\n",
    "                    'docstring': data.get('docstring')\n",
    "                } \n",
    "                for node, data in self.graph.nodes(data=True)\n",
    "            ],\n",
    "            'links': [\n",
    "                {\n",
    "                    'source': source,\n",
    "                    'target': target,\n",
    "                    'type': data.get('edge_type'),\n",
    "                    'line_number': data.get('line_number'),\n",
    "                    'start_line': data.get('start_line'),\n",
    "                    'end_line': data.get('end_line'),\n",
    "                    'context': data.get('context')\n",
    "                } \n",
    "                for source, target, data in self.graph.edges(data=True)\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(graph_data, f, indent=2)\n",
    "\n",
    "    def export_to_arango(self,\n",
    "                         db_name: str = 'codebase',\n",
    "                         username: str = 'root',\n",
    "                         password: str = 'passwd',\n",
    "                         host: str = 'http://localhost:8529') -> None:\n",
    "        \"\"\"Export the graph into ArangoDB.\"\"\"\n",
    "        client = ArangoClient(hosts=host)\n",
    "        db = client.db(username=username, password=password, verify=True)\n",
    "\n",
    "        # Delete the existing graph and its collections if they exist.\n",
    "        graph_name = \"FlaskRepv1\"\n",
    "        \n",
    "        # Import networkx_to_arangodb if it's available\n",
    "        try:\n",
    "            import networkx_to_arangodb as nxadb # type: ignore\n",
    "            G_adb = nxadb.Graph(\n",
    "                name=graph_name,\n",
    "                db=db,\n",
    "                incoming_graph_data=self.graph,\n",
    "                write_batch_size=50000,\n",
    "                overwrite_graph=True\n",
    "            )\n",
    "            \n",
    "            self.G_adb = G_adb\n",
    "            print(\"Graph successfully exported to ArangoDB.\")\n",
    "            return G_adb\n",
    "        except ImportError:\n",
    "            print(\"networkx_to_arangodb module not found. Please install it to export to ArangoDB.\")\n",
    "            return None\n",
    "\n",
    "    def find_variable_usages(self, variable_name: str) -> dict:\n",
    "        \"\"\"Find all usages of a variable in the codebase with detailed context.\"\"\"\n",
    "        results = {\n",
    "            'definitions': [],\n",
    "            'references': [],\n",
    "            'summary': {}\n",
    "        }\n",
    "        \n",
    "        # Find variable definitions\n",
    "        for file_path, symbols in self.module_symbols.items():\n",
    "            if variable_name in symbols:\n",
    "                details = symbols[variable_name]\n",
    "                if details['type'] == 'variable':\n",
    "                    context_lines = self._get_context_around_line(file_path, details['line_no'])\n",
    "                    results['definitions'].append({\n",
    "                        'file': file_path,\n",
    "                        'line': details['line_no'],\n",
    "                        'context': context_lines\n",
    "                    })\n",
    "        \n",
    "        # Find variable references\n",
    "        if variable_name in self.symbol_references:\n",
    "            for file_path, line_no, context in self.symbol_references[variable_name]:\n",
    "                results['references'].append({\n",
    "                    'file': file_path,\n",
    "                    'line': line_no,\n",
    "                    'context': context\n",
    "                })\n",
    "        \n",
    "        # Generate summary\n",
    "        results['summary'] = {\n",
    "            'definition_count': len(results['definitions']),\n",
    "            'reference_count': len(results['references']),\n",
    "            'files_with_definitions': list(set(d['file'] for d in results['definitions'])),\n",
    "            'files_with_references': list(set(r['file'] for r in results['references']))\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def text_to_nx_algorithm_to_text(self, query):\n",
    "        \"\"\"\n",
    "        Enhanced analysis function that uses the local graph to answer symbol queries\n",
    "        without requiring external API calls.\n",
    "        \"\"\"\n",
    "        # Check if this is a query about a symbol's usage\n",
    "        symbol_pattern = r'(?:find usages? of|what is the use of) ([\\w_]+)'\n",
    "        symbol_match = re.search(symbol_pattern, query.lower())\n",
    "        \n",
    "        if symbol_match:\n",
    "            symbol_name = symbol_match.group(1)\n",
    "            usages = self.find_symbol_usages(symbol_name)\n",
    "            \n",
    "            if not usages:\n",
    "                return f\"No usages found for symbol '{symbol_name}' in the codebase.\"\n",
    "            \n",
    "            # Analyze the usage patterns\n",
    "            definitions = [u for u in usages if u['type'] == 'definition']\n",
    "            references = [u for u in usages if u['type'] == 'reference']\n",
    "            \n",
    "            # Organize by files\n",
    "            files_with_usages = {}\n",
    "            for usage in usages:\n",
    "                file_path = usage['file']\n",
    "                if file_path not in files_with_usages:\n",
    "                    files_with_usages[file_path] = {'definitions': [], 'references': []}\n",
    "                \n",
    "                usage_type = usage['type']\n",
    "                files_with_usages[file_path][usage_type + 's'].append(usage)\n",
    "            \n",
    "            # Start building response\n",
    "            response = f\"# Analysis of '{symbol_name}' in the Codebase\\n\\n\"\n",
    "            \n",
    "            # Add definition section if available\n",
    "            if definitions:\n",
    "                definition = definitions[0]  # Get primary definition\n",
    "                response += f\"## Definition\\n\\n\"\n",
    "                symbol_type = definition.get('symbol_type', 'function')\n",
    "                response += f\"**Type:** {symbol_type}\\n\"\n",
    "                response += f\"**Defined in:** {definition['file']}:{definition['line']}\\n\\n\"\n",
    "                \n",
    "                if definition.get('docstring'):\n",
    "                    response += f\"**Docstring:**\\n```\\n{definition['docstring']}\\n```\\n\\n\"\n",
    "                \n",
    "                if definition.get('context'):\n",
    "                    response += f\"**Implementation:**\\n```python\\n{definition['context']}\\n```\\n\\n\"\n",
    "            \n",
    "            # Analyze purpose based on usage patterns\n",
    "            response += f\"## Purpose and Usage\\n\\n\"\n",
    "            \n",
    "            # Try to infer purpose based on context\n",
    "            purpose_analysis = self._analyze_symbol_purpose(symbol_name, usages)\n",
    "            response += purpose_analysis + \"\\n\\n\"\n",
    "            \n",
    "            # List usage examples\n",
    "            response += f\"## Usage Examples\\n\\n\"\n",
    "            \n",
    "            # Show top 3-5 most representative usage examples\n",
    "            example_count = min(5, len(references))\n",
    "            for i in range(example_count):\n",
    "                if i < len(references):\n",
    "                    ref = references[i]\n",
    "                    response += f\"### Example {i+1}: {ref['file']}:{ref['line']}\\n\\n\"\n",
    "                    if ref.get('context'):\n",
    "                        response += f\"```python\\n{ref['context']}\\n```\\n\\n\"\n",
    "            \n",
    "            # Add summary stats\n",
    "            response += f\"## Summary Statistics\\n\\n\"\n",
    "            response += f\"- Total occurrences: {len(usages)}\\n\"\n",
    "            response += f\"- Definitions: {len(definitions)}\\n\"\n",
    "            response += f\"- References: {len(references)}\\n\"\n",
    "            response += f\"- Files containing this symbol: {len(files_with_usages)}\\n\"\n",
    "            \n",
    "            return response\n",
    "        \n",
    "        # Check if this is a query about a variable's usage\n",
    "        var_pattern = r'find (?:where|how) ([\\w_]+) is (?:used|referenced|defined)'\n",
    "        var_match = re.search(var_pattern, query.lower())\n",
    "        \n",
    "        if var_match:\n",
    "            var_name = var_match.group(1)\n",
    "            usages = self.find_variable_usages(var_name)\n",
    "            \n",
    "            if not usages['definitions'] and not usages['references']:\n",
    "                return f\"No usages found for variable '{var_name}' in the codebase.\"\n",
    "            \n",
    "            # Format a nice response\n",
    "            response = f\"Analysis of variable '{var_name}':\\n\\n\"\n",
    "            \n",
    "            if usages['definitions']:\n",
    "                response += f\"DEFINITIONS ({len(usages['definitions'])}):\\n\"\n",
    "                for def_info in usages['definitions']:\n",
    "                    response += f\"In {def_info['file']}:{def_info['line']}\\n\"\n",
    "                    if def_info.get('context'):\n",
    "                        response += f\"```python\\n{def_info['context']}\\n```\\n\"\n",
    "                response += \"\\n\"\n",
    "            \n",
    "            if usages['references']:\n",
    "                response += f\"REFERENCES ({len(usages['references'])}):\\n\"\n",
    "                for ref_info in usages['references']:\n",
    "                    response += f\"In {ref_info['file']}:{ref_info['line']}\\n\"\n",
    "                    if ref_info.get('context'):\n",
    "                        response += f\"```python\\n{ref_info['context']}\\n```\\n\"\n",
    "            \n",
    "            return response\n",
    "        \n",
    "        # Handle \"what is\" questions with more general analysis\n",
    "        what_is_pattern = r'what is ([\\w_]+)'\n",
    "        what_is_match = re.search(what_is_pattern, query.lower())\n",
    "        \n",
    "        if what_is_match:\n",
    "            item_name = what_is_match.group(1)\n",
    "            \n",
    "            # Try to find it as a symbol\n",
    "            symbol_usages = self.find_symbol_usages(item_name)\n",
    "            \n",
    "            if symbol_usages:\n",
    "                # It's a symbol, use the symbol analysis\n",
    "                return self.text_to_nx_algorithm_to_text(f\"find usages of {item_name}\")\n",
    "            \n",
    "            # Try to find it as a file or directory\n",
    "            file_matches = [f for f in self.file_index.keys() if item_name in f]\n",
    "            dir_matches = [d for d in self.directories if item_name in d]\n",
    "            \n",
    "            if file_matches or dir_matches:\n",
    "                response = f\"# Analysis of '{item_name}' in the Codebase\\n\\n\"\n",
    "                \n",
    "                if file_matches:\n",
    "                    response += f\"## Matching Files\\n\\n\"\n",
    "                    for file in file_matches[:10]:  # Limit to 10 files\n",
    "                        response += f\"- {file}\\n\"\n",
    "                    if len(file_matches) > 10:\n",
    "                        response += f\"... and {len(file_matches) - 10} more files\\n\"\n",
    "                    response += \"\\n\"\n",
    "                \n",
    "                if dir_matches:\n",
    "                    response += f\"## Matching Directories\\n\\n\"\n",
    "                    for directory in dir_matches:\n",
    "                        response += f\"- {directory}\\n\"\n",
    "                    response += \"\\n\"\n",
    "                \n",
    "                return response\n",
    "            \n",
    "            return f\"Could not find '{item_name}' as a symbol, file, or directory in the codebase.\"\n",
    "        \n",
    "        # For any other type of query, perform a more general analysis\n",
    "        return self._general_codebase_analysis(query)\n",
    "\n",
    "    def analyze_has_level_handler(self):\n",
    "        \"\"\"Specialized analysis for has_level_handler in the Flask codebase.\"\"\"\n",
    "        symbol_name = \"has_level_handler\"\n",
    "        usages = self.find_symbol_usages(symbol_name)\n",
    "        \n",
    "        if not usages:\n",
    "            return f\"No usages found for '{symbol_name}' in the codebase.\"\n",
    "        \n",
    "        # Separate definitions and references\n",
    "        definitions = [u for u in usages if u['type'] == 'definition']\n",
    "        references = [u for u in usages if u['type'] == 'reference']\n",
    "        \n",
    "        # Build a comprehensive analysis\n",
    "        response = f\"# Analysis of 'has_level_handler' in Flask\\n\\n\"\n",
    "        \n",
    "        # 1. Add definition and implementation\n",
    "        if definitions:\n",
    "            definition = definitions[0]\n",
    "            response += \"## Definition\\n\\n\"\n",
    "            response += f\"Defined in: {definition['file']}:{definition['line']}\\n\"\n",
    "            \n",
    "            if definition.get('context'):\n",
    "                response += \"\\n```python\\n\" + definition['context'] + \"\\n```\\n\\n\"\n",
    "        \n",
    "        # 2. Explain purpose\n",
    "        response += \"## Purpose\\n\\n\"\n",
    "        response += \"The `has_level_handler` function checks whether a logger has a handler that will handle \"\n",
    "        response += \"messages of a certain level. It's primarily used to determine if a default handler needs \"\n",
    "        response += \"to be added to a logger.\\n\\n\"\n",
    "        \n",
    "        # 3. Analyze usage in logging.py\n",
    "        logging_references = [r for r in references if 'logging.py' in r['file']]\n",
    "        if logging_references:\n",
    "            response += \"## Usage in Flask's Logging System\\n\\n\"\n",
    "            response += \"In Flask's logging module, `has_level_handler` is used to determine whether to add \"\n",
    "            response += \"a default handler to a logger. If the logger doesn't have an appropriate handler, \"\n",
    "            response += \"Flask adds one automatically.\\n\\n\"\n",
    "            \n",
    "            for ref in logging_references:\n",
    "                response += f\"### In {ref['file']}:{ref['line']}\\n\\n\"\n",
    "                if ref.get('context'):\n",
    "                    response += \"```python\\n\" + ref['context'] + \"\\n```\\n\\n\"\n",
    "                response += \"Here, Flask is checking if the logger needs a default handler and adding one if necessary.\\n\\n\"\n",
    "        \n",
    "        # 4. Analyze usage in tests\n",
    "        test_references = [r for r in references if 'test_' in r['file']]\n",
    "        if test_references:\n",
    "            response += \"## Testing\\n\\n\"\n",
    "            response += f\"The function is tested in {len(test_references)} locations within the test suite. \"\n",
    "            response += \"These tests verify the function works correctly with different logger configurations:\\n\\n\"\n",
    "            \n",
    "            test_scenarios = [\n",
    "                \"- Testing with a logger that has no handlers\",\n",
    "                \"- Testing with a logger that has handlers at the appropriate level\",\n",
    "                \"- Testing with a logger with propagation disabled\",\n",
    "                \"- Testing with handlers at different logging levels\"\n",
    "            ]\n",
    "            \n",
    "            response += \"\\n\".join(test_scenarios) + \"\\n\\n\"\n",
    "            \n",
    "            # Show a representative test\n",
    "            if test_references:\n",
    "                response += \"### Example Test\\n\\n\"\n",
    "                ref = test_references[0]\n",
    "                if ref.get('context'):\n",
    "                    response += \"```python\\n\" + ref['context'] + \"\\n```\\n\\n\"\n",
    "        \n",
    "        # 5. Add a summary\n",
    "        response += \"## Summary\\n\\n\"\n",
    "        response += f\"`has_level_handler` is a utility function used {len(references)} times in the Flask codebase. \"\n",
    "        response += \"It's a critical component of Flask's logging system that ensures log messages are properly handled. \"\n",
    "        response += \"The function prevents Flask from adding redundant handlers while ensuring that logs are captured \"\n",
    "        response += \"at the appropriate level.\"\n",
    "        \n",
    "        return response  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T04:38:34.048801Z",
     "iopub.status.busy": "2025-03-01T04:38:34.048501Z",
     "iopub.status.idle": "2025-03-01T04:38:59.455318Z",
     "shell.execute_reply": "2025-03-01T04:38:59.454208Z",
     "shell.execute_reply.started": "2025-03-01T04:38:34.048779Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#visualizer.export_to_arango(db_name=\"FlaskRepv1\",username=\"root\",password=\"cUZ0YaNdcwfUTw6VjRny\",host=\"https://d2eeb8083350.arangodb.cloud:8529\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T04:39:43.873809Z",
     "iopub.status.busy": "2025-03-01T04:39:43.873462Z",
     "iopub.status.idle": "2025-03-01T04:39:44.082162Z",
     "shell.execute_reply": "2025-03-01T04:39:44.081407Z",
     "shell.execute_reply.started": "2025-03-01T04:39:43.873780Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# visualizer = CodebaseVisualizer(\"flask\")\n",
    "# visualizer.parse_files()\n",
    "# G = visualizer.build_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T04:39:46.921588Z",
     "iopub.status.busy": "2025-03-01T04:39:46.921252Z",
     "iopub.status.idle": "2025-03-01T04:40:44.405541Z",
     "shell.execute_reply": "2025-03-01T04:40:44.404664Z",
     "shell.execute_reply.started": "2025-03-01T04:39:46.921552Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# class GraphVisualizer:\n",
    "#     def __init__(self, graph: nx.Graph):\n",
    "#         self.graph = graph\n",
    "#         self.pos = None\n",
    "        \n",
    "#     def set_layout(self, layout_type: str = 'spring', **layout_params) -> None:\n",
    "#         \"\"\"\n",
    "#         Set the layout for the graph visualization.\n",
    "        \n",
    "#         Args:\n",
    "#             layout_type: Type of layout ('spring', 'circular', 'kamada_kawai', \n",
    "#                         'random', 'shell', 'spectral')\n",
    "#             layout_params: Additional parameters for the layout algorithm\n",
    "#         \"\"\"\n",
    "#         layout_funcs = {\n",
    "#             'spring': nx.spring_layout,\n",
    "#             'circular': nx.circular_layout,\n",
    "#             'kamada_kawai': nx.kamada_kawai_layout,\n",
    "#             'random': nx.random_layout,\n",
    "#             'shell': nx.shell_layout,\n",
    "#             'spectral': nx.spectral_layout\n",
    "#         }\n",
    "        \n",
    "#         if layout_type not in layout_funcs:\n",
    "#             raise ValueError(f\"Unsupported layout type. Choose from: {list(layout_funcs.keys())}\")\n",
    "            \n",
    "#         self.pos = layout_funcs[layout_type](self.graph, **layout_params)\n",
    "    \n",
    "#     def _get_node_colors(self) -> Dict[str, str]:\n",
    "#         \"\"\"Extract node colors from graph attributes or generate defaults.\"\"\"\n",
    "#         colors = {}\n",
    "#         for node in self.graph.nodes():\n",
    "#             # Check for color in node attributes\n",
    "#             attrs = self.graph.nodes[node]\n",
    "#             if 'fillcolor' in attrs:\n",
    "#                 colors[node] = attrs['fillcolor']\n",
    "#             elif 'color' in attrs:\n",
    "#                 colors[node] = attrs['color']\n",
    "#             else:\n",
    "#                 colors[node] = 'lightblue'  # default color\n",
    "#         return colors\n",
    "    \n",
    "#     def _get_node_sizes(self) -> Dict[str, float]:\n",
    "#         \"\"\"Extract or compute node sizes.\"\"\"\n",
    "#         sizes = {}\n",
    "#         for node in self.graph.nodes():\n",
    "#             attrs = self.graph.nodes[node]\n",
    "#             if 'size' in attrs:\n",
    "#                 sizes[node] = attrs['size']\n",
    "#             else:\n",
    "#                 # Default size based on node degree\n",
    "#                 sizes[node] = 1000 * (1 + self.graph.degree(node) / 10)\n",
    "#         return sizes\n",
    "    \n",
    "#     def _get_edge_colors(self) -> Dict[Tuple[str, str], str]:\n",
    "#         \"\"\"Extract edge colors from graph attributes or generate defaults.\"\"\"\n",
    "#         colors = {}\n",
    "#         for u, v in self.graph.edges():\n",
    "#             edge_data = self.graph.get_edge_data(u, v)\n",
    "#             if 'color' in edge_data:\n",
    "#                 colors[(u, v)] = edge_data['color']\n",
    "#             else:\n",
    "#                 colors[(u, v)] = 'gray'  # default color\n",
    "#         return colors\n",
    "    \n",
    "#     def _get_node_labels(self) -> Dict[str, str]:\n",
    "#         \"\"\"Extract node labels from graph attributes.\"\"\"\n",
    "#         labels = {}\n",
    "#         for node in self.graph.nodes():\n",
    "#             attrs = self.graph.nodes[node]\n",
    "#             if 'label' in attrs:\n",
    "#                 labels[node] = attrs['label']\n",
    "#             else:\n",
    "#                 labels[node] = str(node)\n",
    "#         return labels\n",
    "    \n",
    "#     def _get_edge_labels(self) -> Dict[Tuple[str, str], str]:\n",
    "#         \"\"\"Extract edge labels from graph attributes.\"\"\"\n",
    "#         labels = {}\n",
    "#         for u, v in self.graph.edges():\n",
    "#             edge_data = self.graph.get_edge_data(u, v)\n",
    "#             if 'label' in edge_data:\n",
    "#                 labels[(u, v)] = edge_data['label']\n",
    "#         return labels\n",
    "\n",
    "#     def visualize(self, \n",
    "#                  figsize: Tuple[int, int] = (12, 8),\n",
    "#                  node_size: Optional[Dict[str, float]] = None,\n",
    "#                  node_color: Optional[Dict[str, str]] = None,\n",
    "#                  edge_color: Optional[Dict[Tuple[str, str], str]] = None,\n",
    "#                  with_labels: bool = True,\n",
    "#                  font_size: int = 8,\n",
    "#                  title: Optional[str] = None,\n",
    "#                  show_edge_labels: bool = True,\n",
    "#                  alpha: float = 0.7,\n",
    "#                  save_path: Optional[str] = None) -> None:\n",
    "#         \"\"\"\n",
    "#         Visualize the graph with customizable options.\n",
    "        \n",
    "#         Args:\n",
    "#             figsize: Size of the figure (width, height)\n",
    "#             node_size: Dictionary mapping nodes to their sizes\n",
    "#             node_color: Dictionary mapping nodes to their colors\n",
    "#             edge_color: Dictionary mapping edges to their colors\n",
    "#             with_labels: Whether to show node labels\n",
    "#             font_size: Size of the font for labels\n",
    "#             title: Title of the graph\n",
    "#             show_edge_labels: Whether to show edge labels\n",
    "#             alpha: Transparency of nodes\n",
    "#             save_path: Path to save the visualization (if None, displays instead)\n",
    "#         \"\"\"\n",
    "#         if self.pos is None:\n",
    "#             self.set_layout('spring')\n",
    "            \n",
    "#         plt.figure(figsize=figsize)\n",
    "        \n",
    "#         # Get or use provided node attributes\n",
    "#         node_colors = node_color if node_color is not None else self._get_node_colors()\n",
    "#         node_sizes = node_size if node_size is not None else self._get_node_sizes()\n",
    "#         edge_colors = edge_color if edge_color is not None else self._get_edge_colors()\n",
    "        \n",
    "#         # Draw nodes\n",
    "#         nx.draw_networkx_nodes(self.graph, self.pos,\n",
    "#                              node_color=[node_colors[node] for node in self.graph.nodes()],\n",
    "#                              node_size=[node_sizes[node] for node in self.graph.nodes()],\n",
    "#                              alpha=alpha)\n",
    "        \n",
    "#         # Draw edges\n",
    "#         for (u, v) in self.graph.edges():\n",
    "#             nx.draw_networkx_edges(self.graph, self.pos,\n",
    "#                                  edgelist=[(u, v)],\n",
    "#                                  edge_color=edge_colors.get((u, v), 'gray'),\n",
    "#                                  alpha=0.5)\n",
    "        \n",
    "#         # Add labels if requested\n",
    "#         if with_labels:\n",
    "#             labels = self._get_node_labels()\n",
    "#             nx.draw_networkx_labels(self.graph, self.pos, labels,\n",
    "#                                   font_size=font_size)\n",
    "        \n",
    "#         # Add edge labels if requested\n",
    "#         if show_edge_labels:\n",
    "#             edge_labels = self._get_edge_labels()\n",
    "#             if edge_labels:\n",
    "#                 nx.draw_networkx_edge_labels(self.graph, self.pos,\n",
    "#                                            edge_labels=edge_labels,\n",
    "#                                            font_size=font_size-2)\n",
    "        \n",
    "#         if title:\n",
    "#             plt.title(title)\n",
    "        \n",
    "#         plt.axis('off')\n",
    "        \n",
    "#         if save_path:\n",
    "#             plt.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "#             plt.close()\n",
    "#         else:\n",
    "#             plt.show()\n",
    "\n",
    "# # Example usage:\n",
    "# '''\n",
    "# # Create a sample graph\n",
    "# G = nx.Graph()\n",
    "# G.add_nodes_from([\n",
    "#     (1, {'fillcolor': 'lightblue', 'label': 'Node 1'}),\n",
    "#     (2, {'fillcolor': 'lightgreen', 'label': 'Node 2'}),\n",
    "#     (3, {'fillcolor': 'lightred', 'label': 'Node 3'})\n",
    "# ])\n",
    "# G.add_edges_from([\n",
    "#     (1, 2, {'color': 'blue', 'label': 'Edge 1-2'}),\n",
    "#     (2, 3, {'color': 'red', 'label': 'Edge 2-3'})\n",
    "# ])\n",
    "# '''\n",
    "# # Create visualizer and display graph\n",
    "# visualizer = GraphVisualizer(G)\n",
    "# visualizer.set_layout('spring', k=2)  # k controls the spacing between nodes\n",
    "# visualizer.visualize(\n",
    "#     figsize=(10, 8),\n",
    "#     font_size=10,\n",
    "#     title=\"Sample Graph Visualization\",\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Network' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 281\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output_html\n\u001b[1;32m    280\u001b[0m \u001b[38;5;66;03m# Visualize the entire codebase graph (with node limit for performance)\u001b[39;00m\n\u001b[0;32m--> 281\u001b[0m \u001b[43mvisualize_codebase_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mflask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit_nodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# Visualize a specific symbol (like \"has_level_handler\")\u001b[39;00m\n\u001b[1;32m    284\u001b[0m visualizer \u001b[38;5;241m=\u001b[39m CodebaseVisualizer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflask\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 19\u001b[0m, in \u001b[0;36mvisualize_codebase_graph\u001b[0;34m(codebase_path, output_html, limit_nodes)\u001b[0m\n\u001b[1;32m     16\u001b[0m visualizer\u001b[38;5;241m.\u001b[39mexport_graph_json(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcodebase_graph.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Create a pyvis network\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m net \u001b[38;5;241m=\u001b[39m \u001b[43mNetwork\u001b[49m(height\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m900px\u001b[39m\u001b[38;5;124m\"\u001b[39m, width\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m100\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m, bgcolor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#222222\u001b[39m\u001b[38;5;124m\"\u001b[39m, font_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Configure physics\u001b[39;00m\n\u001b[1;32m     22\u001b[0m net\u001b[38;5;241m.\u001b[39mbarnes_hut(gravity\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5000\u001b[39m, central_gravity\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, spring_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Network' is not defined"
     ]
    }
   ],
   "source": [
    "def visualize_codebase_graph(codebase_path, output_html=\"codebase_graph.html\", limit_nodes=None):\n",
    "    \"\"\"\n",
    "    Visualize the codebase graph using pyvis.\n",
    "    \n",
    "    Args:\n",
    "        codebase_path: Path to the codebase directory\n",
    "        output_html: Output HTML file for the visualization\n",
    "        limit_nodes: Optional limit on the number of nodes to display (for large codebases)\n",
    "    \"\"\"\n",
    "    # Initialize and build the graph\n",
    "    visualizer = CodebaseVisualizer(codebase_path)\n",
    "    visualizer.parse_files()\n",
    "    graph = visualizer.build_graph()\n",
    "    \n",
    "    # Export graph to JSON (optional)\n",
    "    visualizer.export_graph_json('codebase_graph.json')\n",
    "    \n",
    "    # Create a pyvis network\n",
    "    net = Network(height=\"900px\", width=\"100%\", bgcolor=\"#222222\", font_color=\"white\")\n",
    "    \n",
    "    # Configure physics\n",
    "    net.barnes_hut(gravity=-5000, central_gravity=0.3, spring_length=200)\n",
    "    \n",
    "    # Define node groups and colors\n",
    "    node_colors = {\n",
    "        'file': '#4287f5',\n",
    "        'directory': '#42f5a7',\n",
    "        'symbol': '#f542cb',\n",
    "        'snippet': '#f5a742'\n",
    "    }\n",
    "    \n",
    "    # If we need to limit nodes for performance\n",
    "    if limit_nodes and len(graph.nodes()) > limit_nodes:\n",
    "        # Focus on file and directory nodes, and limit symbol nodes\n",
    "        important_nodes = [node for node, data in graph.nodes(data=True) \n",
    "                          if data.get('type') in ['file', 'directory']]\n",
    "        \n",
    "        # Add some symbol nodes to reach the limit\n",
    "        symbols = [node for node, data in graph.nodes(data=True) \n",
    "                  if data.get('type') == 'symbol']\n",
    "        \n",
    "        # Take a subset of symbols based on connectivity\n",
    "        symbol_importance = sorted(\n",
    "            [(node, graph.degree(node)) for node in symbols],\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        important_symbols = [node for node, _ in symbol_importance[:limit_nodes - len(important_nodes)]]\n",
    "        selected_nodes = important_nodes + important_symbols\n",
    "        \n",
    "        # Create a subgraph\n",
    "        graph = graph.subgraph(selected_nodes)\n",
    "    \n",
    "    # Add nodes with appropriate styles\n",
    "    for node, node_data in graph.nodes(data=True):\n",
    "        node_type = node_data.get('type', 'unknown')\n",
    "        label = os.path.basename(node) if '/' in node else node\n",
    "        \n",
    "        # Truncate very long labels\n",
    "        if len(label) > 30:\n",
    "            label = label[:27] + \"...\"\n",
    "        \n",
    "        # Create hover title with more details\n",
    "        title = f\"<div style='max-width:300px;'>\"\n",
    "        title += f\"<b>{node}</b><br>\"\n",
    "        title += f\"Type: {node_type}<br>\"\n",
    "        \n",
    "        if node_type == 'file':\n",
    "            title += f\"Directory: {node_data.get('directory', 'N/A')}<br>\"\n",
    "            \n",
    "        elif node_type == 'symbol':\n",
    "            title += f\"Symbol type: {node_data.get('symbol_type', 'N/A')}<br>\"\n",
    "            title += f\"Line: {node_data.get('line_number', 'N/A')}<br>\"\n",
    "            \n",
    "            if node_data.get('docstring'):\n",
    "                docstring = node_data['docstring']\n",
    "                if len(docstring) > 200:\n",
    "                    docstring = docstring[:197] + \"...\"\n",
    "                title += f\"Docstring: {docstring}<br>\"\n",
    "                \n",
    "        elif node_type == 'snippet':\n",
    "            title += f\"Lines: {node_data.get('start_line', 'N/A')}-{node_data.get('end_line', 'N/A')}<br>\"\n",
    "            \n",
    "            if node_data.get('code_snippet'):\n",
    "                snippet = node_data['code_snippet'].replace('\\n', '<br>')\n",
    "                if len(snippet) > 300:\n",
    "                    snippet = snippet[:297] + \"...\"\n",
    "                title += f\"Code:<br><pre>{snippet}</pre>\"\n",
    "                \n",
    "        title += \"</div>\"\n",
    "        \n",
    "        # Determine node size based on type and connections\n",
    "        size = 15  # Default size\n",
    "        if node_type == 'directory':\n",
    "            size = 25\n",
    "        elif node_type == 'file':\n",
    "            size = 20\n",
    "        elif node_type == 'symbol' and node_data.get('symbol_type') == 'class':\n",
    "            size = 18\n",
    "        \n",
    "        # Add node with appropriate styling\n",
    "        net.add_node(\n",
    "            node, \n",
    "            label=label, \n",
    "            title=title,\n",
    "            color=node_colors.get(node_type, '#999999'),\n",
    "            size=size,\n",
    "            shape='dot' if node_type != 'directory' else 'diamond'\n",
    "        )\n",
    "    \n",
    "    # Add edges with appropriate styles\n",
    "    for source, target, edge_data in graph.edges(data=True):\n",
    "        edge_type = edge_data.get('edge_type', 'unknown')\n",
    "        \n",
    "        # Style edges differently based on type\n",
    "        if edge_type == 'import':\n",
    "            color = '#f5f542'\n",
    "            width = 2\n",
    "            dashes = False\n",
    "        elif edge_type == 'references':\n",
    "            color = '#f54242'\n",
    "            width = 1\n",
    "            dashes = [5, 5]\n",
    "        elif edge_type == 'defines':\n",
    "            color = '#42f55a'\n",
    "            width = 3\n",
    "            dashes = False\n",
    "        elif edge_type == 'contains_snippet':\n",
    "            color = '#42c8f5'\n",
    "            width = 1\n",
    "            dashes = [2, 2]\n",
    "        else:\n",
    "            color = '#999999'\n",
    "            width = 1\n",
    "            dashes = False\n",
    "        \n",
    "        # Create hover title with edge details\n",
    "        title = f\"<div><b>{edge_type}</b><br>\"\n",
    "        \n",
    "        if edge_data.get('line_number'):\n",
    "            title += f\"Line: {edge_data['line_number']}<br>\"\n",
    "            \n",
    "        if edge_data.get('context'):\n",
    "            context = edge_data['context']\n",
    "            if len(context) > 200:\n",
    "                context = context[:197] + \"...\"\n",
    "            title += f\"Context: {context}\"\n",
    "            \n",
    "        title += \"</div>\"\n",
    "        \n",
    "        # Add edge with styling\n",
    "        net.add_edge(\n",
    "            source, \n",
    "            target, \n",
    "            title=title,\n",
    "            color=color,\n",
    "            width=width,\n",
    "            dashes=dashes\n",
    "        )\n",
    "    \n",
    "    # Enable physics, navigation and interaction options\n",
    "    net.toggle_physics(True)\n",
    "    net.show_buttons(filter_=['physics'])\n",
    "    \n",
    "    # Save the visualization\n",
    "    net.save_graph(output_html)\n",
    "    print(f\"Graph visualization saved to {output_html}\")\n",
    "    \n",
    "    return output_html\n",
    "\n",
    "def visualize_symbol_subgraph(visualizer, symbol_name, output_html=\"symbol_graph.html\"):\n",
    "    \"\"\"\n",
    "    Create a focused visualization of a symbol and its relationships.\n",
    "    \n",
    "    Args:\n",
    "        visualizer: Initialized CodebaseVisualizer instance\n",
    "        symbol_name: Name of the symbol to visualize\n",
    "        output_html: Output HTML file for the visualization\n",
    "    \"\"\"\n",
    "    # Get the full graph\n",
    "    full_graph = visualizer.graph\n",
    "    \n",
    "    # Find all nodes related to this symbol\n",
    "    symbol_nodes = [node for node in full_graph.nodes() if f\"::{symbol_name}\" in node]\n",
    "    \n",
    "    if not symbol_nodes:\n",
    "        print(f\"Symbol '{symbol_name}' not found in the graph.\")\n",
    "        return None\n",
    "    \n",
    "    # Get nodes that are connected to symbol nodes (1-hop neighborhood)\n",
    "    related_nodes = set(symbol_nodes)\n",
    "    for node in symbol_nodes:\n",
    "        # Add predecessors (nodes that reference this symbol)\n",
    "        related_nodes.update(full_graph.predecessors(node))\n",
    "        # Add successors (nodes that this symbol references)\n",
    "        related_nodes.update(full_graph.successors(node))\n",
    "    \n",
    "    # Create a subgraph\n",
    "    subgraph = full_graph.subgraph(related_nodes)\n",
    "    \n",
    "    # Create a pyvis network\n",
    "    net = Network(height=\"750px\", width=\"100%\", bgcolor=\"#222222\", font_color=\"white\")\n",
    "    net.barnes_hut(gravity=-2000, central_gravity=0.3, spring_length=150)\n",
    "    \n",
    "    # Node colors by type\n",
    "    node_colors = {\n",
    "        'file': '#4287f5',\n",
    "        'directory': '#42f5a7',\n",
    "        'symbol': '#f542cb',\n",
    "        'snippet': '#f5a742'\n",
    "    }\n",
    "    \n",
    "    # Add nodes with appropriate styles\n",
    "    for node, node_data in subgraph.nodes(data=True):\n",
    "        node_type = node_data.get('type', 'unknown')\n",
    "        label = os.path.basename(node) if '/' in node else node\n",
    "        \n",
    "        # Make the focus symbol nodes larger and highlighted\n",
    "        if node in symbol_nodes:\n",
    "            size = 30\n",
    "            color = '#ff0000'  # Bright red for focus\n",
    "        else:\n",
    "            size = 15\n",
    "            color = node_colors.get(node_type, '#999999')\n",
    "        \n",
    "        # Add hover information\n",
    "        title = f\"<div style='max-width:300px;'>\"\n",
    "        title += f\"<b>{node}</b><br>\"\n",
    "        title += f\"Type: {node_type}<br>\"\n",
    "        \n",
    "        if node_type == 'symbol':\n",
    "            title += f\"Symbol type: {node_data.get('symbol_type', 'N/A')}<br>\"\n",
    "            if node_data.get('docstring'):\n",
    "                title += f\"Docstring: {node_data.get('docstring', '')}<br>\"\n",
    "                \n",
    "        title += \"</div>\"\n",
    "        \n",
    "        # Add node with styling\n",
    "        net.add_node(\n",
    "            node,\n",
    "            label=label,\n",
    "            title=title,\n",
    "            color=color,\n",
    "            size=size\n",
    "        )\n",
    "    \n",
    "    # Add edges with styles\n",
    "    for source, target, edge_data in subgraph.edges(data=True):\n",
    "        edge_type = edge_data.get('edge_type', 'unknown')\n",
    "        \n",
    "        # Style edges by type\n",
    "        if edge_type == 'import':\n",
    "            color = '#f5f542'  # Yellow\n",
    "        elif edge_type == 'references':\n",
    "            color = '#f54242'  # Red\n",
    "        elif edge_type == 'defines':\n",
    "            color = '#42f55a'  # Green\n",
    "        else:\n",
    "            color = '#999999'  # Gray\n",
    "        \n",
    "        # Add edge with details in hover\n",
    "        title = f\"{edge_type}\"\n",
    "        if edge_data.get('line_number'):\n",
    "            title += f\" (line {edge_data['line_number']})\"\n",
    "            \n",
    "        net.add_edge(source, target, title=title, color=color)\n",
    "    \n",
    "    # Enable physics and navigation\n",
    "    net.toggle_physics(True)\n",
    "    net.show_buttons(filter_=['physics'])\n",
    "    \n",
    "    # Save the visualization\n",
    "    net.save_graph(output_html)\n",
    "    print(f\"Symbol graph visualization saved to {output_html}\")\n",
    "    \n",
    "    return output_html\n",
    "\n",
    "\n",
    "# Visualize the entire codebase graph (with node limit for performance)\n",
    "visualize_codebase_graph(\"flask\", limit_nodes=200)\n",
    "\n",
    "# Visualize a specific symbol (like \"has_level_handler\")\n",
    "visualizer = CodebaseVisualizer(\"flask\")\n",
    "visualizer.parse_files()\n",
    "G = visualizer.build_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T04:43:05.876305Z",
     "iopub.status.busy": "2025-03-01T04:43:05.875977Z",
     "iopub.status.idle": "2025-03-01T04:43:06.776671Z",
     "shell.execute_reply": "2025-03-01T04:43:06.775886Z",
     "shell.execute_reply.started": "2025-03-01T04:43:05.876281Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "db = ArangoClient(hosts=\"https://d2eeb8083350.arangodb.cloud:8529\").db(username=\"root\", password=\"cUZ0YaNdcwfUTw6VjRny\", verify=True)\n",
    "\n",
    "print(db)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T04:45:32.004484Z",
     "iopub.status.busy": "2025-03-01T04:45:32.004188Z",
     "iopub.status.idle": "2025-03-01T04:45:35.899436Z",
     "shell.execute_reply": "2025-03-01T04:45:35.898520Z",
     "shell.execute_reply.started": "2025-03-01T04:45:32.004463Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "G_adb = nxadb.Graph(\n",
    "    name=\"FlaskRepv1\",\n",
    "    db=db,\n",
    "    incoming_graph_data=G,\n",
    "    write_batch_size=1000, # feel free to modify\n",
    "    overwrite_graph=True\n",
    ")\n",
    "\n",
    "print(G_adb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run from loaded database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T12:32:04.625116Z",
     "iopub.status.busy": "2025-02-23T12:32:04.624787Z",
     "iopub.status.idle": "2025-02-23T12:32:05.367964Z",
     "shell.execute_reply": "2025-02-23T12:32:05.367236Z",
     "shell.execute_reply.started": "2025-02-23T12:32:04.625091Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "db = ArangoClient(hosts=\"https://d2eeb8083350.arangodb.cloud:8529\").db(username=\"root\", password=\"cUZ0YaNdcwfUTw6VjRny\", verify=True)\n",
    "\n",
    "print(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "G_adb = nxadb.Graph(\n",
    "    name=\"FlaskRepv1\",\n",
    "    db=db,\n",
    "    #incoming_graph_data=G,\n",
    "    #write_batch_size=50000 # feel free to modify\n",
    ")\n",
    "\n",
    "print(G_adb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T04:43:20.247877Z",
     "iopub.status.busy": "2025-03-01T04:43:20.247588Z",
     "iopub.status.idle": "2025-03-01T04:43:23.818006Z",
     "shell.execute_reply": "2025-03-01T04:43:23.817097Z",
     "shell.execute_reply.started": "2025-03-01T04:43:20.247857Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "arango_graph = ArangoGraph(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T04:43:25.534742Z",
     "iopub.status.busy": "2025-03-01T04:43:25.534423Z",
     "iopub.status.idle": "2025-03-01T04:43:25.538505Z",
     "shell.execute_reply": "2025-03-01T04:43:25.537663Z",
     "shell.execute_reply.started": "2025-03-01T04:43:25.534715Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"MISTRAL_API_KEY\"]=\"jJAuJZkjVcy2ynUhan375sHNviHiBeJU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T04:43:27.851350Z",
     "iopub.status.busy": "2025-03-01T04:43:27.851042Z",
     "iopub.status.idle": "2025-03-01T04:43:29.376117Z",
     "shell.execute_reply": "2025-03-01T04:43:29.375233Z",
     "shell.execute_reply.started": "2025-03-01T04:43:27.851326Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain_mistralai import ChatMistralAI\n",
    "\n",
    "llm = ChatMistralAI(\n",
    "    model=\"mistral-large-latest\",\n",
    "    temperature=0,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "llm.invoke(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T04:43:31.747340Z",
     "iopub.status.busy": "2025-03-01T04:43:31.747045Z",
     "iopub.status.idle": "2025-03-01T04:43:31.757820Z",
     "shell.execute_reply": "2025-03-01T04:43:31.756946Z",
     "shell.execute_reply.started": "2025-03-01T04:43:31.747317Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# @tool\n",
    "# def text_to_nx_algorithm_to_text(query):\n",
    "#     \"\"\"This tool is available to invoke a NetworkX Algorithm on\n",
    "#     the ArangoDB Graph. You are responsible for accepting the\n",
    "#     Natural Language Query, establishing which algorithm needs to\n",
    "#     be executed, executing the algorithm, and translating the results back\n",
    "#     to Natural Language, with respect to the original query.\n",
    "\n",
    "#     If the query (e.g traversals, shortest path, etc.) can be solved using the Arango Query Language, then do not use\n",
    "#     this tool.\n",
    "#     \"\"\"\n",
    "#     llm = ChatMistralAI(\n",
    "#         model=\"mistral-large-latest\",\n",
    "#         temperature=0,\n",
    "#         max_retries=2,\n",
    "#         # other params...\n",
    "#     )\n",
    "#     ######################\n",
    "#     print(\"1) Generating NetworkX code\")\n",
    "\n",
    "#     text_to_nx = llm.invoke(f\"\"\"\n",
    "#     I have a NetworkX Graph called `G_adb`. It has the following schema: {arango_graph.schema}\n",
    "\n",
    "#     I have the following graph analysis query: {query}.\n",
    "\n",
    "#     Generate the Python Code required to answer the query using the `G_adb` object.\n",
    "    \n",
    "#     It should give code so that is gives ALL the necessary contents that use this part of the code in terms of ALL nested imports. Consider the nested directories and sub-directory informations as well.\n",
    "\n",
    "#     Be very precise on the NetworkX algorithm you select to answer this query. Think step by step.\n",
    "\n",
    "#     Only assume that networkx is installed, and other base python dependencies.\n",
    "\n",
    "#     Always set the last variable as `FINAL_RESULT`, which represents the answer to the original query.\n",
    "\n",
    "#     Only provide python code that I can directly execute via `exec()`. Do not provide any instructions.\n",
    "\n",
    "#     Make sure that `FINAL_RESULT` stores a short & consice answer. Avoid setting this variable to a long sequence.\n",
    "\n",
    "#     Your code:\n",
    "#     \"\"\").content\n",
    "\n",
    "#     text_to_nx_cleaned = re.sub(r\"^```python\\n|```$\", \"\", text_to_nx, flags=re.MULTILINE).strip()\n",
    "    \n",
    "#     print('-'*10)\n",
    "#     print(text_to_nx_cleaned)\n",
    "#     print('-'*10)\n",
    "\n",
    "#     ######################\n",
    "\n",
    "#     print(\"\\n2) Executing NetworkX code\")\n",
    "#     global_vars = {\"G_adb\": G_adb, \"nx\": nx}\n",
    "#     local_vars = {}\n",
    "\n",
    "#     try:\n",
    "#         exec(text_to_nx_cleaned, global_vars, local_vars)\n",
    "#         text_to_nx_final = text_to_nx\n",
    "#     except Exception as e:\n",
    "#         print(f\"EXEC ERROR: {e}\")\n",
    "#         return f\"EXEC ERROR: {e}\"\n",
    "\n",
    "#         # TODO: Consider experimenting with a code corrector!\n",
    "#         attempt = 1\n",
    "#         MAX_ATTEMPTS = 3\n",
    "\n",
    "#         # while attempt <= MAX_ATTEMPTS\n",
    "#             # ...\n",
    "\n",
    "#     print('-'*10)\n",
    "#     FINAL_RESULT = local_vars[\"FINAL_RESULT\"]\n",
    "#     print(f\"FINAL_RESULT: {FINAL_RESULT}\")\n",
    "#     print('-'*10)\n",
    "\n",
    "#     ######################\n",
    "\n",
    "#     print(\"3) Formulating final answer\")\n",
    "\n",
    "#     nx_to_text = llm.invoke(f\"\"\"\n",
    "#         I have a NetworkX Graph called `G_adb`. It has the following schema: {arango_graph.schema}\n",
    "\n",
    "#         I have the following graph analysis query: {query}.\n",
    "\n",
    "#         I have executed the following python code to help me answer my query:\n",
    "\n",
    "#         ---\n",
    "#         {text_to_nx_final}\n",
    "#         ---\n",
    "\n",
    "#         The `FINAL_RESULT` variable is set to the following: {FINAL_RESULT}.\n",
    "\n",
    "#         Based on my original Query and FINAL_RESULT, generate a short and concise response to\n",
    "#         answer my query.\n",
    "        \n",
    "#         Your response:\n",
    "#     \"\"\").content\n",
    "\n",
    "#     return nx_to_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysing the imported Graph via the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from arango import ArangoClient\n",
    "import re\n",
    "\n",
    "def analyze_networkx_graph(G, query_text):\n",
    "    \"\"\"\n",
    "    Process a natural language query directly against a NetworkX graph\n",
    "    \n",
    "    Args:\n",
    "        G: NetworkX graph object\n",
    "        query_text: Natural language query about the codebase\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with analysis results in a human-readable format\n",
    "    \"\"\"\n",
    "    # Clean the query to extract the actual symbol being sought\n",
    "    clean_query = query_text.lower().strip()\n",
    "    \n",
    "    # Common English words to filter out\n",
    "    common_words = {\"what\", \"is\", \"the\", \"use\", \"of\", \"in\", \"where\", \"how\", \"why\", \"when\", \n",
    "                    \"who\", \"which\", \"does\", \"do\", \"are\", \"can\", \"could\", \"would\", \"should\", \n",
    "                    \"function\", \"method\", \"class\", \"variable\", \"codebase\", \"code\", \"used\", \n",
    "                    \"defined\", \"implemented\", \"called\", \"referenced\"}\n",
    "    \n",
    "    # Find all words that could be symbols\n",
    "    potential_symbols = []\n",
    "    for word in clean_query.split():\n",
    "        word = word.strip(\".,?!()[]{}'\\\"\\n\\t\")\n",
    "        if len(word) > 2 and word.lower() not in common_words:\n",
    "            potential_symbols.append(word)\n",
    "    \n",
    "    # Also look for multi-word symbols with underscores\n",
    "    for i in range(len(clean_query.split()) - 1):\n",
    "        compound = '_'.join(clean_query.split()[i:i+2])\n",
    "        if '_' in compound and compound not in potential_symbols:\n",
    "            potential_symbols.append(compound)\n",
    "    \n",
    "    # Extract exact symbol if passed directly\n",
    "    if query_text.strip() and len(query_text.strip().split()) == 1 and '_' in query_text:\n",
    "        # User likely just passed the symbol name directly\n",
    "        potential_symbols = [query_text.strip()]\n",
    "    \n",
    "    # Find matches in the graph\n",
    "    candidates = []\n",
    "    \n",
    "    # First, look for exact node IDs or node names\n",
    "    for symbol in potential_symbols:\n",
    "        for node_id, node_data in G.nodes(data=True):\n",
    "            # Check if this is a symbol node\n",
    "            if node_data.get('type') == 'symbol':\n",
    "                # Check the node ID\n",
    "                if isinstance(node_id, str) and symbol.lower() in node_id.lower():\n",
    "                    candidates.append((node_id, symbol, 1.0))  # 1.0 = high confidence\n",
    "                \n",
    "                # Check if symbol appears in the context (code snippet)\n",
    "                context = node_data.get('context', '')\n",
    "                if context and symbol.lower() in context.lower():\n",
    "                    # Higher confidence if it appears as a variable assignment\n",
    "                    patterns = [\n",
    "                        f\"self.{symbol}\", \n",
    "                        f\"{symbol} =\", \n",
    "                        f\"def {symbol}\", \n",
    "                        f\"class {symbol}\"\n",
    "                    ]\n",
    "                    score = 0.8  # Base score\n",
    "                    for pattern in patterns:\n",
    "                        if pattern.lower() in context.lower():\n",
    "                            score = 0.9  # Higher confidence\n",
    "                            break\n",
    "                    candidates.append((node_id, symbol, score))\n",
    "    \n",
    "    # Second, check for symbol references in edge contexts\n",
    "    if not candidates:\n",
    "        for source, target, edge_data in G.edges(data=True):\n",
    "            edge_type = edge_data.get('edge_type')\n",
    "            if edge_type in ['references', 'defines']:\n",
    "                context = edge_data.get('context', '')\n",
    "                for symbol in potential_symbols:\n",
    "                    if context and symbol.lower() in context.lower():\n",
    "                        patterns = [\n",
    "                            f\"self.{symbol}\", \n",
    "                            f\"{symbol} =\", \n",
    "                            f\"def {symbol}\", \n",
    "                            f\"class {symbol}\"\n",
    "                        ]\n",
    "                        score = 0.7  # Base score for edges\n",
    "                        for pattern in patterns:\n",
    "                            if pattern.lower() in context.lower():\n",
    "                                score = 0.8  # Higher confidence\n",
    "                                break\n",
    "                        candidates.append((target, symbol, score))\n",
    "    \n",
    "    # Third, search in snippets\n",
    "    if not candidates:\n",
    "        for node_id, node_data in G.nodes(data=True):\n",
    "            if node_data.get('type') == 'snippet':\n",
    "                code_snippet = node_data.get('code_snippet', '')\n",
    "                for symbol in potential_symbols:\n",
    "                    if code_snippet and symbol.lower() in code_snippet.lower():\n",
    "                        patterns = [\n",
    "                            f\"self.{symbol}\", \n",
    "                            f\"{symbol} =\", \n",
    "                            f\"def {symbol}\", \n",
    "                            f\"class {symbol}\"\n",
    "                        ]\n",
    "                        score = 0.6  # Base score for snippets\n",
    "                        for pattern in patterns:\n",
    "                            if pattern.lower() in code_snippet.lower():\n",
    "                                score = 0.7  # Higher confidence\n",
    "                                break\n",
    "                        # Find the file this snippet belongs to\n",
    "                        file_nodes = []\n",
    "                        for source, target, edge_data in G.in_edges(node_id, data=True):\n",
    "                            if edge_data.get('edge_type') == 'contains_snippet':\n",
    "                                file_nodes.append(source)\n",
    "                        \n",
    "                        # Create a pseudo symbol node ID using the file\n",
    "                        if file_nodes:\n",
    "                            pseudo_id = f\"{file_nodes[0]}::{symbol}\"\n",
    "                            candidates.append((pseudo_id, symbol, score))\n",
    "                        else:\n",
    "                            candidates.append((node_id, symbol, score))\n",
    "    \n",
    "    # No matches found\n",
    "    if not candidates:\n",
    "        return {\"response\": f\"Could not find a matching symbol in the codebase for '{query_text}'. Please try rephrasing your query with a specific function, class, or variable name.\"}\n",
    "    \n",
    "    # Sort candidates by confidence score\n",
    "    candidates.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    # Choose the best candidate\n",
    "    node_id, symbol_name, _ = candidates[0]\n",
    "    \n",
    "    # Find all usages of the identified symbol\n",
    "    symbol_usages = find_symbol_usage_nx(G, node_id, symbol_name)\n",
    "    \n",
    "    # Generate response\n",
    "    if \"error\" in symbol_usages:\n",
    "        # Try a broader search if the specific node wasn't found\n",
    "        broader_usages = search_symbol_in_contexts(G, symbol_name)\n",
    "        if broader_usages:\n",
    "            return {\"response\": broader_usages}\n",
    "        return {\"response\": symbol_usages[\"error\"]}\n",
    "    \n",
    "    # Collect information about the symbol\n",
    "    definition_files = symbol_usages.get(\"defined_in\", [])\n",
    "    usage_info = symbol_usages.get(\"usages\", {})\n",
    "    symbol_type = symbol_usages.get(\"symbol_type\", \"variable\")  # Default to variable\n",
    "    \n",
    "    # Build human-readable response\n",
    "    response = f\"Symbol: '{symbol_name}' (Type: {symbol_type})\\n\\n\"\n",
    "    response += f\"Defined in: {', '.join(definition_files) if definition_files else 'No definition location found'}\\n\\n\"\n",
    "    \n",
    "    if symbol_usages.get(\"docstring\"):\n",
    "        response += f\"Documentation:\\n{symbol_usages['docstring']}\\n\\n\"\n",
    "    \n",
    "    response += \"Used in the following locations:\\n\"\n",
    "    \n",
    "    if not usage_info:\n",
    "        response += \"\\nNo usage information found for this symbol in graph nodes.\\n\"\n",
    "        \n",
    "        # Try a broader search in contexts\n",
    "        broader_usages = search_symbol_in_contexts(G, symbol_name)\n",
    "        if broader_usages:\n",
    "            response += \"\\nHowever, found these mentions in code snippets:\\n\\n\"\n",
    "            response += broader_usages\n",
    "    else:\n",
    "        for file, usages in usage_info.items():\n",
    "            response += f\"\\nFile: {file}\\n\"\n",
    "            for usage in usages:\n",
    "                line = usage.get('line', 'unknown line')\n",
    "                context = usage.get('context', 'No context available')\n",
    "                response += f\"- Line {line}: {context}\\n\"\n",
    "    \n",
    "    # Add related symbols if available\n",
    "    if symbol_usages.get(\"related_symbols\"):\n",
    "        response += \"\\nRelated symbols:\\n\"\n",
    "        for related in symbol_usages[\"related_symbols\"][:5]:  # Limit to top 5\n",
    "            response += f\"- {related}\\n\"\n",
    "    \n",
    "    # Add graph statistics\n",
    "    response += f\"\\nAnalysis performed on graph with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\"\n",
    "    \n",
    "    return {\"response\": response}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Searching fir the symbol in both context, networkx format and file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_symbol_in_contexts(G, symbol_name):\n",
    "    \"\"\"\n",
    "    Search for a symbol in all contexts (code snippets) in the graph\n",
    "    \n",
    "    Args:\n",
    "        G: NetworkX graph\n",
    "        symbol_name: Name of the symbol to search\n",
    "    \n",
    "    Returns:\n",
    "        String with usage information\n",
    "    \"\"\"\n",
    "    response = \"\"\n",
    "    found = False\n",
    "    \n",
    "    # Look in snippet nodes\n",
    "    for node_id, node_data in G.nodes(data=True):\n",
    "        if node_data.get('type') == 'snippet':\n",
    "            code_snippet = node_data.get('code_snippet', '')\n",
    "            if code_snippet and symbol_name.lower() in code_snippet.lower():\n",
    "                found = True\n",
    "                file_name = \"Unknown\"\n",
    "                # Find which file this snippet belongs to\n",
    "                for source, target, edge_data in G.in_edges(node_id, data=True):\n",
    "                    if edge_data.get('edge_type') == 'contains_snippet':\n",
    "                        file_name = source\n",
    "                        break\n",
    "                \n",
    "                start_line = node_data.get('start_line', 'unknown')\n",
    "                response += f\"\\nFile: {file_name} (Lines {start_line}-{node_data.get('end_line', 'unknown')})\\n\"\n",
    "                \n",
    "                # Extract the lines containing the symbol\n",
    "                lines = code_snippet.split('\\n')\n",
    "                for i, line in enumerate(lines):\n",
    "                    if symbol_name.lower() in line.lower():\n",
    "                        line_num = int(start_line) + i if isinstance(start_line, int) else \"?\"\n",
    "                        response += f\"- Line {line_num}: {line.strip()}\\n\"\n",
    "    \n",
    "    # Look in edge contexts\n",
    "    for source, target, edge_data in G.edges(data=True):\n",
    "        context = edge_data.get('context', '')\n",
    "        if context and symbol_name.lower() in context.lower():\n",
    "            found = True\n",
    "            edge_type = edge_data.get('edge_type', 'unknown')\n",
    "            line_num = edge_data.get('line_number', 'unknown')\n",
    "            \n",
    "            # For references or defines edges, source is usually the file\n",
    "            file_name = source if edge_type in ['references', 'defines'] else \"Unknown\"\n",
    "            \n",
    "            response += f\"\\nFile: {file_name} (Line {line_num})\\n\"\n",
    "            response += f\"- {context.strip()}\\n\"\n",
    "    \n",
    "    if found:\n",
    "        return response\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def find_symbol_usage_nx(graph, node_id, symbol_name):\n",
    "    \"\"\"\n",
    "    Find all usages of a specific symbol across the codebase using NetworkX graph\n",
    "    \n",
    "    Args:\n",
    "        graph: The NetworkX graph object\n",
    "        node_id: The node ID of the symbol node in the graph\n",
    "        symbol_name: The name of the symbol for display purposes\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with files and line numbers where the symbol is used\n",
    "    \"\"\"\n",
    "    # Check if this is a valid node\n",
    "    if node_id not in graph:\n",
    "        # This could be a pseudo node ID we created for snippet matches\n",
    "        if isinstance(node_id, str) and '::' in node_id:\n",
    "            file_path = node_id.split('::')[0]\n",
    "            # Return information based on file path and symbol name\n",
    "            return {\n",
    "                \"symbol\": symbol_name,\n",
    "                \"symbol_type\": \"variable\",  # Assuming variable as default\n",
    "                \"defined_in\": [file_path],\n",
    "                \"docstring\": \"\",\n",
    "                \"usages\": search_symbol_in_file(graph, file_path, symbol_name),\n",
    "                \"related_symbols\": []\n",
    "            }\n",
    "        return {\"error\": f\"Symbol '{symbol_name}' not found in the codebase as a specific node\"}\n",
    "    \n",
    "    # Get symbol node data\n",
    "    node_data = graph.nodes[node_id]\n",
    "    \n",
    "    # Extract symbol type and other metadata\n",
    "    symbol_type = node_data.get('symbol_type', 'variable')  # Default to variable\n",
    "    docstring = node_data.get('docstring', '')\n",
    "    \n",
    "    # Find definition locations\n",
    "    definitions = []\n",
    "    for source, target, edge_data in graph.in_edges(node_id, data=True):\n",
    "        if edge_data.get('edge_type') == 'defines':\n",
    "            # Source should be a file node\n",
    "            definitions.append(source)\n",
    "    \n",
    "    # If no definitions found through edges, extract from node ID\n",
    "    if not definitions and isinstance(node_id, str) and '::' in node_id:\n",
    "        file_path = node_id.split('::')[0]\n",
    "        definitions.append(file_path)\n",
    "    \n",
    "    # Find all references to the symbol\n",
    "    usages = {}\n",
    "    for source, target, edge_data in graph.in_edges(node_id, data=True):\n",
    "        edge_type = edge_data.get('edge_type')\n",
    "        \n",
    "        # Consider references\n",
    "        if edge_type == 'references':\n",
    "            # Source should be a file node\n",
    "            file_name = source\n",
    "            \n",
    "            # Extract line info and context\n",
    "            line_info = edge_data.get('line_number', 'unknown line')\n",
    "            context = edge_data.get('context', 'No context available')\n",
    "            \n",
    "            if file_name not in usages:\n",
    "                usages[file_name] = []\n",
    "            \n",
    "            usages[file_name].append({\n",
    "                'line': line_info,\n",
    "                'context': context\n",
    "            })\n",
    "    \n",
    "    # Find related symbols (e.g., symbols in the same file)\n",
    "    related_symbols = []\n",
    "    for def_file in definitions:\n",
    "        for source, target, edge_data in graph.out_edges(def_file, data=True):\n",
    "            if edge_data.get('edge_type') == 'defines' and target != node_id:\n",
    "                # Extract symbol name from target node ID\n",
    "                if isinstance(target, str) and '::' in target:\n",
    "                    related_symbol = target.split('::')[1]\n",
    "                    related_symbols.append(related_symbol)\n",
    "    \n",
    "    return {\n",
    "        \"symbol\": symbol_name,\n",
    "        \"symbol_type\": symbol_type,\n",
    "        \"defined_in\": definitions,\n",
    "        \"docstring\": docstring,\n",
    "        \"usages\": usages,\n",
    "        \"related_symbols\": related_symbols\n",
    "    }\n",
    "\n",
    "def search_symbol_in_file(graph, file_path, symbol_name):\n",
    "    \"\"\"\n",
    "    Search for uses of a symbol within a specific file\n",
    "    \n",
    "    Args:\n",
    "        graph: NetworkX graph\n",
    "        file_path: Path of the file to search in\n",
    "        symbol_name: Name of the symbol to search for\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with usage information\n",
    "    \"\"\"\n",
    "    usages = {}\n",
    "    \n",
    "    # Look for snippet nodes from this file\n",
    "    for node_id, node_data in graph.nodes(data=True):\n",
    "        if node_data.get('type') == 'snippet':\n",
    "            # Check if this snippet belongs to the file\n",
    "            belongs_to_file = False\n",
    "            for source, target, edge_data in graph.in_edges(node_id, data=True):\n",
    "                if edge_data.get('edge_type') == 'contains_snippet' and source == file_path:\n",
    "                    belongs_to_file = True\n",
    "                    break\n",
    "            \n",
    "            if belongs_to_file:\n",
    "                code_snippet = node_data.get('code_snippet', '')\n",
    "                if code_snippet and symbol_name.lower() in code_snippet.lower():\n",
    "                    start_line = node_data.get('start_line', 'unknown')\n",
    "                    \n",
    "                    if file_path not in usages:\n",
    "                        usages[file_path] = []\n",
    "                    \n",
    "                    # Extract the lines containing the symbol\n",
    "                    lines = code_snippet.split('\\n')\n",
    "                    for i, line in enumerate(lines):\n",
    "                        if symbol_name.lower() in line.lower():\n",
    "                            line_num = int(start_line) + i if isinstance(start_line, int) else \"unknown\"\n",
    "                            usages[file_path].append({\n",
    "                                'line': line_num,\n",
    "                                'context': line.strip()\n",
    "                            })\n",
    "    \n",
    "    return usages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text LLM file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_nx_algorithm_to_text(query_text, graph=None, db_connection=None, graph_name=None):\n",
    "    \"\"\"\n",
    "    Universal entry point for processing natural language queries against a code graph\n",
    "    \n",
    "    Args:\n",
    "        query_text: Natural language query about the codebase\n",
    "        graph: NetworkX graph object (if already available)\n",
    "        db_connection: ArangoDB connection (if graph should be loaded from ArangoDB)\n",
    "        graph_name: Name of the graph in ArangoDB (if db_connection is provided)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with analysis results in a human-readable format\n",
    "    \"\"\"\n",
    "    # Case 1: NetworkX graph is directly provided\n",
    "    if graph is not None:\n",
    "        if isinstance(graph, nx.Graph):\n",
    "            return analyze_networkx_graph(graph, query_text)\n",
    "        else:\n",
    "            return {\"response\": \"Provided graph object is not a valid NetworkX graph\"}\n",
    "    \n",
    "    # Case 2: ArangoDB connection is provided but no graph\n",
    "    elif db_connection is not None and graph_name is None:\n",
    "        try:\n",
    "            # Try to list available graphs\n",
    "            try:\n",
    "                available_graphs = db_connection.graphs()\n",
    "                if available_graphs:\n",
    "                    graph_name = available_graphs[0][\"name\"]\n",
    "                else:\n",
    "                    return {\"response\": \"No graphs found in the ArangoDB database\"}\n",
    "            except:\n",
    "                # Different API for networkx-arangodb\n",
    "                try:\n",
    "                    import networkx_arangodb as nxadb\n",
    "                    # Try a different approach for networkx-arangodb\n",
    "                    available_graphs = [g for g in db_connection.graphs()]\n",
    "                    if available_graphs:\n",
    "                        graph_name = available_graphs[0]\n",
    "                    else:\n",
    "                        return {\"response\": \"No graphs found in the ArangoDB database\"}\n",
    "                except:\n",
    "                    return {\"response\": \"Could not retrieve graph list from ArangoDB\"}\n",
    "        except Exception as e:\n",
    "            return {\"response\": f\"Error retrieving graphs from ArangoDB: {str(e)}\"}\n",
    "    \n",
    "    # Case 3: ArangoDB connection and graph name are provided\n",
    "    if db_connection is not None and graph_name is not None:\n",
    "        try:\n",
    "            # Try to load the graph using standard ArangoDB driver\n",
    "            try:\n",
    "                # Check if graph_name is a string\n",
    "                if not isinstance(graph_name, str):\n",
    "                    if hasattr(graph_name, 'name'):\n",
    "                        # It might be a graph object with a name attribute\n",
    "                        graph_name = graph_name.name\n",
    "                    else:\n",
    "                        return {\"response\": \"Graph name must be a string or an object with a name attribute\"}\n",
    "                \n",
    "                # Try to get the graph from ArangoDB\n",
    "                arango_graph = db_connection.graph(graph_name)\n",
    "                # Convert to NetworkX using our custom function\n",
    "                nx_graph = convert_arango_to_networkx(db_connection, graph_name)\n",
    "                return analyze_networkx_graph(nx_graph, query_text)\n",
    "            except Exception as e1:\n",
    "                # If standard approach fails, try networkx-arangodb\n",
    "                try:\n",
    "                    import networkx_arangodb as nxadb\n",
    "                    graph = nxadb.Graph(name=graph_name, db=db_connection)\n",
    "                    nx_graph = graph.to_networkx()\n",
    "                    return analyze_networkx_graph(nx_graph, query_text)\n",
    "                except Exception as e2:\n",
    "                    return {\"response\": f\"Error loading graph from ArangoDB: {str(e1)}\\nAlternative method error: {str(e2)}\"}\n",
    "        except Exception as e:\n",
    "            return {\"response\": f\"Error accessing ArangoDB: {str(e)}\"}\n",
    "    \n",
    "    # Case 4: No graph info provided at all\n",
    "    return {\"response\": \"Please provide either a NetworkX graph or ArangoDB connection details\"}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting it to NETWORKX, can be removed for effiency and maintained in arango"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_arango_to_networkx(db, graph_name):\n",
    "    \"\"\"\n",
    "    Convert an ArangoDB graph to NetworkX format\n",
    "    \n",
    "    Args:\n",
    "        db: ArangoDB database connection\n",
    "        graph_name: Name of the graph in ArangoDB\n",
    "    \n",
    "    Returns:\n",
    "        NetworkX graph object\n",
    "    \"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    try:\n",
    "        # Get graph from ArangoDB\n",
    "        arango_graph = db.graph(graph_name)\n",
    "        \n",
    "        # Get graph properties\n",
    "        try:\n",
    "            graph_properties = arango_graph.properties()\n",
    "            edge_definitions = graph_properties.get('edgeDefinitions', [])\n",
    "            \n",
    "            # Process each vertex collection\n",
    "            vertex_collections = set()\n",
    "            for edge_def in edge_definitions:\n",
    "                vertex_collections.update(edge_def.get('from', []))\n",
    "                vertex_collections.update(edge_def.get('to', []))\n",
    "                \n",
    "            # Add nodes from vertex collections\n",
    "            for collection_name in vertex_collections:\n",
    "                try:\n",
    "                    collection = db.collection(collection_name)\n",
    "                    for doc in collection:\n",
    "                        node_id = doc['_id']\n",
    "                        G.add_node(node_id, **doc)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error with vertex collection {collection_name}: {str(e)}\")\n",
    "            \n",
    "            # Process edge collections\n",
    "            for edge_def in edge_definitions:\n",
    "                edge_collection_name = edge_def.get('collection')\n",
    "                if edge_collection_name:\n",
    "                    try:\n",
    "                        collection = db.collection(edge_collection_name)\n",
    "                        for doc in collection:\n",
    "                            from_id = doc['_from']\n",
    "                            to_id = doc['_to']\n",
    "                            G.add_edge(from_id, to_id, **doc)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error with edge collection {edge_collection_name}: {str(e)}\")\n",
    "                        \n",
    "        except Exception as e:\n",
    "            # Alternative approach: try to infer collections from naming convention\n",
    "            print(f\"Error getting graph properties, trying alternate approach: {str(e)}\")\n",
    "            \n",
    "            # Common naming patterns for ArangoDB collections\n",
    "            node_collection_pattern = re.compile(f\"{re.escape(graph_name)}_node\")\n",
    "            edge_collection_pattern = re.compile(f\"{re.escape(graph_name)}_.*_to_.*\")\n",
    "            \n",
    "            # Try to find matching collections\n",
    "            for collection_name in db.collections():\n",
    "                if node_collection_pattern.match(collection_name):\n",
    "                    try:\n",
    "                        collection = db.collection(collection_name)\n",
    "                        for doc in collection:\n",
    "                            node_id = doc['_id']\n",
    "                            G.add_node(node_id, **doc)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error with inferred vertex collection {collection_name}: {str(e)}\")\n",
    "                        \n",
    "                elif edge_collection_pattern.match(collection_name):\n",
    "                    try:\n",
    "                        collection = db.collection(collection_name)\n",
    "                        for doc in collection:\n",
    "                            from_id = doc['_from']\n",
    "                            to_id = doc['_to']\n",
    "                            G.add_edge(from_id, to_id, **doc)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error with inferred edge collection {collection_name}: {str(e)}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing ArangoDB graph: {str(e)}\")\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T04:45:41.453078Z",
     "iopub.status.busy": "2025-03-01T04:45:41.452762Z",
     "iopub.status.idle": "2025-03-01T04:45:58.787334Z",
     "shell.execute_reply": "2025-03-01T04:45:58.786559Z",
     "shell.execute_reply.started": "2025-03-01T04:45:41.453055Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "result = text_to_nx_algorithm_to_text(\n",
    "    \"What is the use of feature_scaler in the codebase?\",\n",
    "    graph=G  # Your NetworkX graph\n",
    ")\n",
    "print(result['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T04:47:01.096433Z",
     "iopub.status.busy": "2025-03-01T04:47:01.096097Z",
     "iopub.status.idle": "2025-03-01T04:47:02.298827Z",
     "shell.execute_reply": "2025-03-01T04:47:02.298082Z",
     "shell.execute_reply.started": "2025-03-01T04:47:01.096408Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(G_adb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T04:47:03.420389Z",
     "iopub.status.busy": "2025-03-01T04:47:03.420081Z",
     "iopub.status.idle": "2025-03-01T04:47:03.426586Z",
     "shell.execute_reply": "2025-03-01T04:47:03.425816Z",
     "shell.execute_reply.started": "2025-03-01T04:47:03.420367Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "arango_graph.schema"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
