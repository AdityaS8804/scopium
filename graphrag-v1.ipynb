{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T06:27:55.540171Z",
     "iopub.status.busy": "2025-03-04T06:27:55.539628Z",
     "iopub.status.idle": "2025-03-04T06:27:57.074713Z",
     "shell.execute_reply": "2025-03-04T06:27:57.073411Z",
     "shell.execute_reply.started": "2025-03-04T06:27:55.540130Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing packages - **DONT TOUCH IT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T06:27:59.586675Z",
     "iopub.status.busy": "2025-03-04T06:27:59.586283Z",
     "iopub.status.idle": "2025-03-04T06:28:01.781957Z",
     "shell.execute_reply": "2025-03-04T06:28:01.780296Z",
     "shell.execute_reply.started": "2025-03-04T06:27:59.586633Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'flask' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/pallets/flask.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T06:28:04.152873Z",
     "iopub.status.busy": "2025-03-04T06:28:04.152522Z",
     "iopub.status.idle": "2025-03-04T06:28:15.047579Z",
     "shell.execute_reply": "2025-03-04T06:28:15.046302Z",
     "shell.execute_reply.started": "2025-03-04T06:28:04.152845Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nx-arangodb in ./.venv/lib/python3.10/site-packages (1.3.0)\n",
      "Requirement already satisfied: networkx<=3.4,>=3.0 in ./.venv/lib/python3.10/site-packages (from nx-arangodb) (3.4)\n",
      "Requirement already satisfied: phenolrs~=0.5 in ./.venv/lib/python3.10/site-packages (from nx-arangodb) (0.5.9)\n",
      "Requirement already satisfied: python-arango~=8.1 in ./.venv/lib/python3.10/site-packages (from nx-arangodb) (8.1.6)\n",
      "Requirement already satisfied: adbnx-adapter~=5.0.5 in ./.venv/lib/python3.10/site-packages (from nx-arangodb) (5.0.6)\n",
      "Requirement already satisfied: requests>=2.27.1 in ./.venv/lib/python3.10/site-packages (from adbnx-adapter~=5.0.5->nx-arangodb) (2.32.3)\n",
      "Requirement already satisfied: rich>=12.5.1 in ./.venv/lib/python3.10/site-packages (from adbnx-adapter~=5.0.5->nx-arangodb) (13.9.4)\n",
      "Requirement already satisfied: setuptools>=45 in ./.venv/lib/python3.10/site-packages (from adbnx-adapter~=5.0.5->nx-arangodb) (75.6.0)\n",
      "Requirement already satisfied: numpy~=1.26 in ./.venv/lib/python3.10/site-packages (from phenolrs~=0.5->nx-arangodb) (1.26.4)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in ./.venv/lib/python3.10/site-packages (from python-arango~=8.1->nx-arangodb) (2.3.0)\n",
      "Requirement already satisfied: requests_toolbelt in ./.venv/lib/python3.10/site-packages (from python-arango~=8.1->nx-arangodb) (1.0.0)\n",
      "Requirement already satisfied: PyJWT in ./.venv/lib/python3.10/site-packages (from python-arango~=8.1->nx-arangodb) (2.10.1)\n",
      "Requirement already satisfied: importlib_metadata>=4.7.1 in ./.venv/lib/python3.10/site-packages (from python-arango~=8.1->nx-arangodb) (8.6.1)\n",
      "Requirement already satisfied: packaging>=23.1 in ./.venv/lib/python3.10/site-packages (from python-arango~=8.1->nx-arangodb) (24.2)\n",
      "Requirement already satisfied: zipp>=3.20 in ./.venv/lib/python3.10/site-packages (from importlib_metadata>=4.7.1->python-arango~=8.1->nx-arangodb) (3.21.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests>=2.27.1->adbnx-adapter~=5.0.5->nx-arangodb) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests>=2.27.1->adbnx-adapter~=5.0.5->nx-arangodb) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests>=2.27.1->adbnx-adapter~=5.0.5->nx-arangodb) (2025.1.31)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.10/site-packages (from rich>=12.5.1->adbnx-adapter~=5.0.5->nx-arangodb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.10/site-packages (from rich>=12.5.1->adbnx-adapter~=5.0.5->nx-arangodb) (2.19.1)\n",
      "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in ./.venv/lib/python3.10/site-packages (from rich>=12.5.1->adbnx-adapter~=5.0.5->nx-arangodb) (4.12.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=12.5.1->adbnx-adapter~=5.0.5->nx-arangodb) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install nx-arangodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T06:28:15.049958Z",
     "iopub.status.busy": "2025-03-04T06:28:15.049522Z",
     "iopub.status.idle": "2025-03-04T06:28:15.324661Z",
     "shell.execute_reply": "2025-03-04T06:28:15.323352Z",
     "shell.execute_reply.started": "2025-03-04T06:28:15.049913Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nvidia-smi\n",
      "zsh:1: command not found: nvcc\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T06:28:16.653376Z",
     "iopub.status.busy": "2025-03-04T06:28:16.652942Z",
     "iopub.status.idle": "2025-03-04T06:28:37.048897Z",
     "shell.execute_reply": "2025-03-04T06:28:37.047530Z",
     "shell.execute_reply.started": "2025-03-04T06:28:16.653341Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.nvidia.com\n",
      "Collecting nx-cugraph-cu12\n",
      "  Downloading https://pypi.nvidia.com/nx-cugraph-cu12/nx_cugraph_cu12-25.2.0-py3-none-any.whl (160 kB)\n",
      "INFO: pip is looking at multiple versions of nx-cugraph-cu12 to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading https://pypi.nvidia.com/nx-cugraph-cu12/nx_cugraph_cu12-24.12.0-py3-none-any.whl (152 kB)\n",
      "  Downloading https://pypi.nvidia.com/nx-cugraph-cu12/nx_cugraph_cu12-24.10.0-py3-none-any.whl (149 kB)\n",
      "  Downloading https://pypi.nvidia.com/nx-cugraph-cu12/nx_cugraph_cu12-24.8.0-py3-none-any.whl (140 kB)\n",
      "  Downloading https://pypi.nvidia.com/nx-cugraph-cu12/nx_cugraph_cu12-24.6.1-py3-none-any.whl (129 kB)\n",
      "  Downloading https://pypi.nvidia.com/nx-cugraph-cu12/nx_cugraph_cu12-24.6.0-py3-none-any.whl (129 kB)\n",
      "  Downloading https://pypi.nvidia.com/nx-cugraph-cu12/nx_cugraph_cu12-24.4.0-py3-none-any.whl (125 kB)\n",
      "  Downloading https://pypi.nvidia.com/nx-cugraph-cu12/nx_cugraph_cu12-24.2.0-py3-none-any.whl (117 kB)\n",
      "INFO: pip is still looking at multiple versions of nx-cugraph-cu12 to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading https://pypi.nvidia.com/nx-cugraph-cu12/nx_cugraph_cu12-23.12.0-py3-none-any.whl (87 kB)\n",
      "  Downloading https://pypi.nvidia.com/nx-cugraph-cu12/nx_cugraph_cu12-23.10.0-py3-none-any.whl (39 kB)\n",
      "\u001b[31mERROR: Cannot install nx-cugraph-cu12==23.10.0, nx-cugraph-cu12==23.12.0, nx-cugraph-cu12==24.10.0, nx-cugraph-cu12==24.12.0, nx-cugraph-cu12==24.2.0, nx-cugraph-cu12==24.4.0, nx-cugraph-cu12==24.6.0, nx-cugraph-cu12==24.6.1, nx-cugraph-cu12==24.8.0 and nx-cugraph-cu12==25.2.0 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "The conflict is caused by:\n",
      "    nx-cugraph-cu12 25.2.0 depends on cupy-cuda12x>=12.0.0\n",
      "    nx-cugraph-cu12 24.12.0 depends on cupy-cuda12x>=12.0.0\n",
      "    nx-cugraph-cu12 24.10.0 depends on cupy-cuda12x>=12.0.0\n",
      "    nx-cugraph-cu12 24.8.0 depends on cupy-cuda12x>=12.0.0\n",
      "    nx-cugraph-cu12 24.6.1 depends on cupy-cuda12x>=12.0.0\n",
      "    nx-cugraph-cu12 24.6.0 depends on cupy-cuda12x>=12.0.0\n",
      "    nx-cugraph-cu12 24.4.0 depends on cupy-cuda12x>=12.0.0\n",
      "    nx-cugraph-cu12 24.2.0 depends on cupy-cuda12x>=12.0.0\n",
      "    nx-cugraph-cu12 23.12.0 depends on cupy-cuda12x>=12.0.0\n",
      "    nx-cugraph-cu12 23.10.0 depends on cupy-cuda12x>=12.0.0\n",
      "\n",
      "To fix this you could try to:\n",
      "1. loosen the range of package versions you've specified\n",
      "2. remove package versions to allow pip to attempt to solve the dependency conflict\n",
      "\n",
      "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 5] Input/output error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/GitHub/scopium/.venv/lib/python3.10/site-packages/IPython/utils/_process_posix.py:156\u001b[0m, in \u001b[0;36mProcessHandler.system\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;66;03m# res is the index of the pattern that caused the match, so we\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# know whether we've finished (if we matched EOF) or not\u001b[39;00m\n\u001b[0;32m--> 156\u001b[0m     res_idx \u001b[38;5;241m=\u001b[39m \u001b[43mchild\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpect_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatterns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28mprint\u001b[39m(child\u001b[38;5;241m.\u001b[39mbefore[out_size:]\u001b[38;5;241m.\u001b[39mdecode(enc, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m'\u001b[39m), end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/GitHub/scopium/.venv/lib/python3.10/site-packages/pexpect/spawnbase.py:383\u001b[0m, in \u001b[0;36mSpawnBase.expect_list\u001b[0;34m(self, pattern_list, timeout, searchwindowsize, async_, **kw)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpect_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GitHub/scopium/.venv/lib/python3.10/site-packages/pexpect/expect.py:169\u001b[0m, in \u001b[0;36mExpecter.expect_loop\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# Still have time left, so read more data\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m incoming \u001b[38;5;241m=\u001b[39m \u001b[43mspawn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_nonblocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspawn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaxread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspawn\u001b[38;5;241m.\u001b[39mdelayafterread \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/GitHub/scopium/.venv/lib/python3.10/site-packages/pexpect/pty_spawn.py:500\u001b[0m, in \u001b[0;36mspawn.read_nonblocking\u001b[0;34m(self, size, timeout)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;66;03m# Because of the select(0) check above, we know that no data\u001b[39;00m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;66;03m# is available right now. But if a non-zero timeout is given\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;66;03m# (possibly timeout=None), we call select() with a timeout.\u001b[39;00m\n\u001b[0;32m--> 500\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (timeout \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(spawn, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mread_nonblocking(size)\n",
      "File \u001b[0;32m~/GitHub/scopium/.venv/lib/python3.10/site-packages/pexpect/pty_spawn.py:450\u001b[0m, in \u001b[0;36mspawn.read_nonblocking.<locals>.select\u001b[0;34m(timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mselect\u001b[39m(timeout):\n\u001b[0;32m--> 450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mselect_ignore_interrupts\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchild_fd\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/GitHub/scopium/.venv/lib/python3.10/site-packages/pexpect/utils.py:143\u001b[0m, in \u001b[0;36mselect_ignore_interrupts\u001b[0;34m(iwtd, owtd, ewtd, timeout)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mselect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43miwtd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mowtd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mewtd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpip install nx-cugraph-cu12 --extra-index-url https://pypi.nvidia.com # Requires CUDA-capable GPU\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GitHub/scopium/.venv/lib/python3.10/site-packages/ipykernel/zmqshell.py:657\u001b[0m, in \u001b[0;36mZMQInteractiveShell.system_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    655\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_exit_code\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m system(cmd)\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 657\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_exit_code\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvar_expand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GitHub/scopium/.venv/lib/python3.10/site-packages/IPython/utils/_process_posix.py:167\u001b[0m, in \u001b[0;36mProcessHandler.system\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    162\u001b[0m         out_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(child\u001b[38;5;241m.\u001b[39mbefore)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# We need to send ^C to the process.  The ascii code for '^C' is 3\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# (the character is known as ETX for 'End of Text', see\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# curses.ascii.ETX).\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m     \u001b[43mchild\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msendline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mchr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# Read and print any more output the program might produce on its\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# way out.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/GitHub/scopium/.venv/lib/python3.10/site-packages/pexpect/pty_spawn.py:578\u001b[0m, in \u001b[0;36mspawn.sendline\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''Wraps send(), sending string ``s`` to child process, with\u001b[39;00m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;124;03m``os.linesep`` automatically appended. Returns number of bytes\u001b[39;00m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;124;03mwritten.  Only a limited number of bytes may be sent for each\u001b[39;00m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;124;03mline in the default terminal mode, see docstring of :meth:`send`.\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    577\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coerce_send_string(s)\n\u001b[0;32m--> 578\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinesep\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GitHub/scopium/.venv/lib/python3.10/site-packages/pexpect/pty_spawn.py:569\u001b[0m, in \u001b[0;36mspawn.send\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log(s, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msend\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    568\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encoder\u001b[38;5;241m.\u001b[39mencode(s, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 569\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchild_fd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 5] Input/output error"
     ]
    }
   ],
   "source": [
    "!pip install nx-cugraph-cu12 --extra-index-url https://pypi.nvidia.com # Requires CUDA-capable GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T06:31:33.136264Z",
     "iopub.status.busy": "2025-03-04T06:31:33.135815Z",
     "iopub.status.idle": "2025-03-04T06:31:48.649399Z",
     "shell.execute_reply": "2025-03-04T06:31:48.648009Z",
     "shell.execute_reply.started": "2025-03-04T06:31:33.136235Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in ./.venv/lib/python3.10/site-packages (0.3.20)\n",
      "Requirement already satisfied: langchain-community in ./.venv/lib/python3.10/site-packages (0.3.19)\n",
      "Requirement already satisfied: langchain-openai in ./.venv/lib/python3.10/site-packages (0.3.7)\n",
      "Requirement already satisfied: langgraph in ./.venv/lib/python3.10/site-packages (0.3.5)\n",
      "Requirement already satisfied: langchain_mistralai in ./.venv/lib/python3.10/site-packages (0.2.7)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.41 in ./.venv/lib/python3.10/site-packages (from langchain) (0.3.41)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in ./.venv/lib/python3.10/site-packages (from langchain) (0.3.6)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in ./.venv/lib/python3.10/site-packages (from langchain) (0.3.11)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in ./.venv/lib/python3.10/site-packages (from langchain) (2.10.6)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./.venv/lib/python3.10/site-packages (from langchain) (2.0.38)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.venv/lib/python3.10/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.venv/lib/python3.10/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in ./.venv/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./.venv/lib/python3.10/site-packages (from langchain-community) (3.11.13)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in ./.venv/lib/python3.10/site-packages (from langchain-community) (9.0.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in ./.venv/lib/python3.10/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in ./.venv/lib/python3.10/site-packages (from langchain-community) (2.8.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in ./.venv/lib/python3.10/site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in ./.venv/lib/python3.10/site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.58.1 in ./.venv/lib/python3.10/site-packages (from langchain-openai) (1.65.1)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in ./.venv/lib/python3.10/site-packages (from langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.10 in ./.venv/lib/python3.10/site-packages (from langgraph) (2.0.16)\n",
      "Requirement already satisfied: langgraph-prebuilt<0.2,>=0.1.1 in ./.venv/lib/python3.10/site-packages (from langgraph) (0.1.1)\n",
      "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.42 in ./.venv/lib/python3.10/site-packages (from langgraph) (0.1.53)\n",
      "Requirement already satisfied: tokenizers<1,>=0.15.1 in ./.venv/lib/python3.10/site-packages (from langchain_mistralai) (0.21.0)\n",
      "Requirement already satisfied: httpx<1,>=0.25.2 in ./.venv/lib/python3.10/site-packages (from langchain_mistralai) (0.28.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.venv/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./.venv/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.10/site-packages (from httpx<1,>=0.25.2->langchain_mistralai) (4.8.0)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.10/site-packages (from httpx<1,>=0.25.2->langchain_mistralai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.10/site-packages (from httpx<1,>=0.25.2->langchain_mistralai) (1.0.7)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.10/site-packages (from httpx<1,>=0.25.2->langchain_mistralai) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.25.2->langchain_mistralai) (0.14.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.venv/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./.venv/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./.venv/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (4.12.2)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.1.0 in ./.venv/lib/python3.10/site-packages (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph) (1.1.0)\n",
      "Requirement already satisfied: orjson>=3.10.1 in ./.venv/lib/python3.10/site-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./.venv/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in ./.venv/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.10/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./.venv/lib/python3.10/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (0.8.2)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.10/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./.venv/lib/python3.10/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./.venv/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in ./.venv/lib/python3.10/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./.venv/lib/python3.10/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in ./.venv/lib/python3.10/site-packages (from tokenizers<1,>=0.15.1->langchain_mistralai) (0.29.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./.venv/lib/python3.10/site-packages (from anyio->httpx<1,>=0.25.2->langchain_mistralai) (1.2.2)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain_mistralai) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain_mistralai) (2025.2.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.41->langchain) (3.0.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.venv/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade langchain langchain-community langchain-openai langgraph langchain_mistralai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T06:31:48.651596Z",
     "iopub.status.busy": "2025-03-04T06:31:48.651210Z",
     "iopub.status.idle": "2025-03-04T06:31:53.301722Z",
     "shell.execute_reply": "2025-03-04T06:31:53.300318Z",
     "shell.execute_reply.started": "2025-03-04T06:31:48.651547Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: networkx==3.4 in ./.venv/lib/python3.10/site-packages (3.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install networkx==3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tree-sitter in ./.venv/lib/python3.10/site-packages (0.24.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tree-sitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'tree-sitter-cpp' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/tree-sitter/tree-sitter-cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T06:32:31.887003Z",
     "iopub.status.busy": "2025-03-04T06:32:31.886594Z",
     "iopub.status.idle": "2025-03-04T06:32:31.893200Z",
     "shell.execute_reply": "2025-03-04T06:32:31.891936Z",
     "shell.execute_reply.started": "2025-03-04T06:32:31.886967Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import ast\n",
    "from typing import Dict, Set, List, Tuple, Optional,Any\n",
    "import json\n",
    "from arango import ArangoClient\n",
    "import nx_arangodb as nxadb\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.graphs import ArangoGraph\n",
    "from langchain_community.chains.graph_qa.arangodb import ArangoGraphQAChain\n",
    "from langchain_core.tools import tool\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T06:32:37.418216Z",
     "iopub.status.busy": "2025-03-04T06:32:37.417841Z",
     "iopub.status.idle": "2025-03-04T06:32:37.427476Z",
     "shell.execute_reply": "2025-03-04T06:32:37.426221Z",
     "shell.execute_reply.started": "2025-03-04T06:32:37.418187Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import ast\n",
    "# import networkx as nx\n",
    "# from typing import Dict, Set, List\n",
    "# import json\n",
    "# from arango import ArangoClient\n",
    "# os.environ[\"MISTRAL_API_KEY\"]=\"jJAuJZkjVcy2ynUhan375sHNviHiBeJU\"\n",
    "\n",
    "# class CodebaseVisualizer:\n",
    "#     def __init__(self, root_dir: str):\n",
    "#         self.root_dir = root_dir\n",
    "#         self.graph = nx.DiGraph()\n",
    "#         self.file_contents: Dict[str, str] = {}\n",
    "#         self.import_relations: Dict[str, Set[str]] = {}\n",
    "#         self.module_symbols: Dict[str, Dict[str, Dict[str, int]]] = {}  # file -> {symbol -> {type, line_no}}\n",
    "#         self.file_index: Dict[str, int] = {}  # Maps files to indices\n",
    "#         self.current_index = 0\n",
    "#         self.directories: Set[str] = set()\n",
    "\n",
    "#     def _get_next_index(self) -> int:\n",
    "#         \"\"\"Get next available index for file indexing.\"\"\"\n",
    "#         self.current_index += 1\n",
    "#         return self.current_index\n",
    "\n",
    "#     def _chunk_code(self, code: str, lines_per_chunk: int = 20) -> List[Dict]:\n",
    "#         \"\"\"\n",
    "#         Chunk the given code into snippets.\n",
    "#         Returns a list of dictionaries with 'code_snippet', 'start_line', and 'end_line'.\n",
    "#         \"\"\"\n",
    "#         lines = code.splitlines()\n",
    "#         chunks = []\n",
    "#         for i in range(0, len(lines), lines_per_chunk):\n",
    "#             chunk_lines = lines[i:i + lines_per_chunk]\n",
    "#             chunk = {\n",
    "#                 'code_snippet': '\\n'.join(chunk_lines),\n",
    "#                 'start_line': i + 1,\n",
    "#                 'end_line': i + len(chunk_lines)\n",
    "#             }\n",
    "#             chunks.append(chunk)\n",
    "#         return chunks\n",
    "\n",
    "#     def parse_files(self) -> None:\n",
    "#         \"\"\"Parse all Python files in the directory and build relationships.\"\"\"\n",
    "#         # First pass: Index all files and create directory nodes\n",
    "#         for root, dirs, files in os.walk(self.root_dir):\n",
    "#             # Add directory node\n",
    "#             rel_dir = os.path.relpath(root, self.root_dir)\n",
    "#             if rel_dir != '.':\n",
    "#                 self.directories.add(rel_dir)\n",
    "#                 self.graph.add_node(rel_dir, type='directory')\n",
    "\n",
    "#             # Index Python files\n",
    "#             for file in files:\n",
    "#                 if file.endswith('.py'):\n",
    "#                     file_path = os.path.join(root, file)\n",
    "#                     rel_path = os.path.relpath(file_path, self.root_dir)\n",
    "#                     self.file_index[rel_path] = self._get_next_index()\n",
    "                    \n",
    "#                     try:\n",
    "#                         with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#                             content = f.read()\n",
    "#                             self.file_contents[rel_path] = content\n",
    "#                             self._analyze_file(rel_path, content)\n",
    "#                     except Exception as e:\n",
    "#                         print(f\"Error parsing {file_path}: {e}\")\n",
    "\n",
    "#     def _analyze_file(self, file_path: str, content: str) -> None:\n",
    "#         \"\"\"Analyze a single file for imports and symbols with line numbers.\"\"\"\n",
    "#         try:\n",
    "#             tree = ast.parse(content)\n",
    "#             imports = set()\n",
    "#             symbols = {}\n",
    "\n",
    "#             for node in ast.walk(tree):\n",
    "#                 # Track imports\n",
    "#                 if isinstance(node, (ast.Import, ast.ImportFrom)):\n",
    "#                     if isinstance(node, ast.Import):\n",
    "#                         for name in node.names:\n",
    "#                             imports.add((name.name, node.lineno))\n",
    "#                     else:  # ImportFrom\n",
    "#                         module = node.module if node.module else ''\n",
    "#                         imports.add((module, node.lineno))\n",
    "\n",
    "#                 # Track defined symbols with line numbers\n",
    "#                 elif isinstance(node, (ast.FunctionDef, ast.ClassDef)):\n",
    "#                     symbols[node.name] = {\n",
    "#                         'type': 'class' if isinstance(node, ast.ClassDef) else 'function',\n",
    "#                         'line_no': node.lineno\n",
    "#                     }\n",
    "\n",
    "#             self.import_relations[file_path] = imports\n",
    "#             self.module_symbols[file_path] = symbols\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error analyzing {file_path}: {e}\")\n",
    "\n",
    "#     def build_graph(self) -> nx.DiGraph:\n",
    "#         \"\"\"Build the NetworkX graph with enhanced node and edge information.\"\"\"\n",
    "#         # Start with a directed graph for clarity in relationships\n",
    "#         dot_graph = nx.DiGraph()\n",
    "        \n",
    "#         # Add nodes for all files with indices and code snippet nodes\n",
    "#         for file_path, file_idx in self.file_index.items():\n",
    "#             dot_graph.add_node(file_path, \n",
    "#                                type='file',\n",
    "#                                file_index=file_idx,\n",
    "#                                directory=os.path.dirname(file_path))\n",
    "            \n",
    "#             # Create snippet nodes for the entire file\n",
    "#             if file_path in self.file_contents:\n",
    "#                 chunks = self._chunk_code(self.file_contents[file_path])\n",
    "#                 for idx, chunk_info in enumerate(chunks):\n",
    "#                     snippet_node = f\"{file_path}::snippet::{idx}\"\n",
    "#                     dot_graph.add_node(snippet_node,\n",
    "#                                        type='snippet',\n",
    "#                                        code_snippet=chunk_info['code_snippet'],\n",
    "#                                        start_line=chunk_info['start_line'],\n",
    "#                                        end_line=chunk_info['end_line'])\n",
    "#                     # Connect file node to snippet node\n",
    "#                     dot_graph.add_edge(file_path, snippet_node, \n",
    "#                                        edge_type='contains_snippet',\n",
    "#                                        start_line=chunk_info['start_line'],\n",
    "#                                        end_line=chunk_info['end_line'])\n",
    "\n",
    "#             # Add nodes for symbols in this file\n",
    "#             for symbol, details in self.module_symbols.get(file_path, {}).items():\n",
    "#                 symbol_node = f\"{file_path}::{symbol}\"\n",
    "#                 dot_graph.add_node(symbol_node, \n",
    "#                                    type='symbol',\n",
    "#                                    symbol_type=details['type'],\n",
    "#                                    line_number=details['line_no'])\n",
    "#                 dot_graph.add_edge(file_path, symbol_node, \n",
    "#                                    edge_type='defines',\n",
    "#                                    line_number=details['line_no'])\n",
    "\n",
    "#         # Add directory nodes\n",
    "#         for directory in self.directories:\n",
    "#             dot_graph.add_node(directory, type='directory')\n",
    "\n",
    "#         # Add edges for imports with line numbers\n",
    "#         for file_path, imports in self.import_relations.items():\n",
    "#             for imp, line_no in imports:\n",
    "#                 # Look for matching files or symbols\n",
    "#                 for target_file, symbols in self.module_symbols.items():\n",
    "#                     if imp in symbols:\n",
    "#                         dot_graph.add_edge(file_path, \n",
    "#                                            f\"{target_file}::{imp}\",\n",
    "#                                            edge_type='import',\n",
    "#                                            line_number=line_no)\n",
    "#                     elif target_file.replace('.py', '').endswith(imp):\n",
    "#                         dot_graph.add_edge(file_path, \n",
    "#                                            target_file,\n",
    "#                                            edge_type='import',\n",
    "#                                            line_number=line_no)\n",
    "        \n",
    "#         # Save the built graph in self.graph for later export (to ArangoDB, JSON, etc.)\n",
    "#         self.graph = dot_graph\n",
    "#         return dot_graph\n",
    "\n",
    "#     def export_file_index(self, output_path: str = 'file_index.txt') -> None:\n",
    "#         \"\"\"Export the file index mapping to a text file.\"\"\"\n",
    "#         with open(output_path, 'w') as f:\n",
    "#             # Write header\n",
    "#             f.write(\"File Index Mapping\\n\")\n",
    "#             f.write(\"=================\\n\\n\")\n",
    "            \n",
    "#             # Sort by index for better readability\n",
    "#             sorted_items = sorted(self.file_index.items(), key=lambda x: x[1])\n",
    "            \n",
    "#             # Write each file and its index\n",
    "#             for file_path, index in sorted_items:\n",
    "#                 f.write(f\"{index}: {file_path}\\n\")\n",
    "                \n",
    "#                 # If there are symbols in this file, list them with line numbers\n",
    "#                 if file_path in self.module_symbols:\n",
    "#                     f.write(\"  Symbols:\\n\")\n",
    "#                     for symbol, details in self.module_symbols[file_path].items():\n",
    "#                         symbol_type = details['type']\n",
    "#                         line_no = details['line_no']\n",
    "#                         f.write(f\"    - {symbol} ({symbol_type}) at line {line_no}\\n\")\n",
    "#                     f.write(\"\\n\")\n",
    "\n",
    "#     def export_graph_json(self, output_path: str = 'codebase_graph.json') -> None:\n",
    "#         \"\"\"Export the graph structure to JSON.\"\"\"\n",
    "#         graph_data = {\n",
    "#             'nodes': [\n",
    "#                 {\n",
    "#                     'id': node,\n",
    "#                     'type': data['type'],\n",
    "#                     'file_index': data.get('file_index'),\n",
    "#                     'directory': data.get('directory'),\n",
    "#                     'symbol_type': data.get('symbol_type'),\n",
    "#                     'line_number': data.get('line_number'),\n",
    "#                     'code_snippet': data.get('code_snippet'),\n",
    "#                     'start_line': data.get('start_line'),\n",
    "#                     'end_line': data.get('end_line')\n",
    "#                 } \n",
    "#                 for node, data in self.graph.nodes(data=True)\n",
    "#             ],\n",
    "#             'links': [\n",
    "#                 {\n",
    "#                     'source': source,\n",
    "#                     'target': target,\n",
    "#                     'type': data.get('edge_type'),\n",
    "#                     'line_number': data.get('line_number'),\n",
    "#                     'start_line': data.get('start_line'),\n",
    "#                     'end_line': data.get('end_line')\n",
    "#                 } \n",
    "#                 for source, target, data in self.graph.edges(data=True)\n",
    "#             ]\n",
    "#         }\n",
    "        \n",
    "#         with open(output_path, 'w') as f:\n",
    "#             json.dump(graph_data, f, indent=2)\n",
    "#     def export_to_arango(self,\n",
    "#                          db_name: str = 'codebase',\n",
    "#                          username: str = 'root',\n",
    "#                          password: str = 'passwd',\n",
    "#                          host: str = 'http://localhost:8529') -> None:\n",
    "#         \"\"\"\n",
    "#         Export the graph into ArangoDB.\n",
    "#         IMPORTANT: This method first deletes any existing graph and associated data in ArangoDB.\n",
    "#         \"\"\"\n",
    "#         client = ArangoClient(hosts=host)\n",
    "#         db = client.db(username=username, password=password,verify=True)\n",
    "\n",
    "#         # Delete the existing graph and its collections if they exist.\n",
    "#         graph_name = \"FlaskRepv1\"\n",
    "#         '''\n",
    "#         if db.has_graph(graph_name):\n",
    "#             try:\n",
    "#                 # Delete the entire graph and its collections.\n",
    "#                 db.graph(graph_name).delete(delete_collections=True)\n",
    "#                 print(\"Existing ArangoDB graph deleted.\")\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error deleting graph: {e}\")\n",
    "#                 '''\n",
    "        \n",
    "#         G_adb = nxadb.Graph(\n",
    "#             name=graph_name,\n",
    "#             db=db,\n",
    "#             incoming_graph_data=self.graph,\n",
    "#             write_batch_size=50000,\n",
    "#             overwrite_graph=True\n",
    "#         )\n",
    "        \n",
    "#         self.G_adb=G_adb\n",
    "#         return G_adb\n",
    "#         print(\"Graph successfully exported to ArangoDB.\")\n",
    "#     def text_to_nx_algorithm_to_text(self,query):\n",
    "#         \"\"\"This tool is available to invoke a NetworkX Algorithm on\n",
    "#         the ArangoDB Graph. You are responsible for accepting the\n",
    "#         Natural Language Query, establishing which algorithm needs to\n",
    "#         be executed, executing the algorithm, and translating the results back\n",
    "#         to Natural Language, with respect to the original query.\n",
    "    \n",
    "#         If the query (e.g traversals, shortest path, etc.) can be solved using the Arango Query Language, then do not use\n",
    "#         this tool.\n",
    "#         \"\"\"\n",
    "#         llm = ChatMistralAI(\n",
    "#             model=\"mistral-large-latest\",\n",
    "#             temperature=0,\n",
    "#             max_retries=2,\n",
    "#             # other params...\n",
    "#         )\n",
    "#         ######################\n",
    "#         print(\"1) Generating NetworkX code\")\n",
    "    \n",
    "#         text_to_nx = llm.invoke(f\"\"\"\n",
    "#         I have a NetworkX Graph called `G_adb`. It has the following schema: {arango_graph.schema}\n",
    "    \n",
    "#         I have the following graph analysis query: {query}.\n",
    "    \n",
    "#         Generate the Python Code required to answer the query using the `G_adb` object.\n",
    "        \n",
    "#         It should give code so that is gives ALL the necessary contents that use this part of the code in terms of ALL nested imports. Consider the nested directories and sub-directory informations as well.\n",
    "    \n",
    "#         Be very precise on the NetworkX algorithm you select to answer this query. Think step by step.\n",
    "    \n",
    "#         Only assume that networkx is installed, and other base python dependencies.\n",
    "    \n",
    "#         Always set the last variable as `FINAL_RESULT`, which represents the answer to the original query.\n",
    "    \n",
    "#         Only provide python code that I can directly execute via `exec()`. Do not provide any instructions.\n",
    "    \n",
    "#         Make sure that `FINAL_RESULT` stores a short & consice answer. Avoid setting this variable to a long sequence.\n",
    "    \n",
    "#         Your code:\n",
    "#         \"\"\").content\n",
    "    \n",
    "#         text_to_nx_cleaned = re.sub(r\"^```python\\n|```$\", \"\", text_to_nx, flags=re.MULTILINE).strip()\n",
    "        \n",
    "#         print('-'*10)\n",
    "#         print(text_to_nx_cleaned)\n",
    "#         print('-'*10)\n",
    "    \n",
    "#         ######################\n",
    "    \n",
    "#         print(\"\\n2) Executing NetworkX code\")\n",
    "#         global_vars = {\"G_adb\":self.G_adb , \"nx\": nx}\n",
    "#         local_vars = {}\n",
    "    \n",
    "#         try:\n",
    "#             exec(text_to_nx_cleaned, global_vars, local_vars)\n",
    "#             text_to_nx_final = text_to_nx\n",
    "#         except Exception as e:\n",
    "#             print(f\"EXEC ERROR: {e}\")\n",
    "#             return f\"EXEC ERROR: {e}\"\n",
    "    \n",
    "#             # TODO: Consider experimenting with a code corrector!\n",
    "#             attempt = 1\n",
    "#             MAX_ATTEMPTS = 3\n",
    "    \n",
    "#             # while attempt <= MAX_ATTEMPTS\n",
    "#                 # ...\n",
    "    \n",
    "#         print('-'*10)\n",
    "#         FINAL_RESULT = local_vars[\"FINAL_RESULT\"]\n",
    "#         print(f\"FINAL_RESULT: {FINAL_RESULT}\")\n",
    "#         print('-'*10)\n",
    "    \n",
    "#         ######################\n",
    "    \n",
    "#         print(\"3) Formulating final answer\")\n",
    "    \n",
    "#         nx_to_text = llm.invoke(f\"\"\"\n",
    "#             I have a NetworkX Graph called `G_adb`. It has the following schema: {arango_graph.schema}\n",
    "    \n",
    "#             I have the following graph analysis query: {query}.\n",
    "    \n",
    "#             I have executed the following python code to help me answer my query:\n",
    "    \n",
    "#             ---\n",
    "#             {text_to_nx_final}\n",
    "#             ---\n",
    "    \n",
    "#             The `FINAL_RESULT` variable is set to the following: {FINAL_RESULT}.\n",
    "    \n",
    "#             Based on my original Query and FINAL_RESULT, generate a short and concise response to\n",
    "#             answer my query.\n",
    "            \n",
    "#             Your response:\n",
    "#         \"\"\").content\n",
    "    \n",
    "#         return nx_to_text\n",
    "\n",
    "# # Example usage\n",
    "\n",
    "# # Initialize and parse the codebase\n",
    "# visualizer = CodebaseVisualizer(\"flask\")\n",
    "# visualizer.parse_files()\n",
    "# visualizer.build_graph()\n",
    "    \n",
    "# # Export the file index and JSON (for local inspection)\n",
    "# visualizer.export_file_index()\n",
    "# visualizer.export_graph_json()\n",
    "# # Export the enriched graph to ArangoDB (this will delete any existing graph data first)\n",
    "# #visualizer.export_to_arango(db_name=\"FlaskRepv1\",username=\"root\",password=\"cUZ0YaNdcwfUTw6VjRny\",host=\"http://localhost:8529\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T06:32:40.946548Z",
     "iopub.status.busy": "2025-03-04T06:32:40.946222Z",
     "iopub.status.idle": "2025-03-04T06:32:41.131038Z",
     "shell.execute_reply": "2025-03-04T06:32:41.129930Z",
     "shell.execute_reply.started": "2025-03-04T06:32:40.946523Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import networkx as nx\n",
    "from typing import Dict, Set, List, Tuple, Optional\n",
    "import json\n",
    "from arango import ArangoClient\n",
    "import re\n",
    "\n",
    "class CodebaseVisualizer:\n",
    "    def __init__(self, root_dir: str):\n",
    "        self.root_dir = root_dir\n",
    "        self.graph = nx.DiGraph()\n",
    "        self.file_contents: Dict[str, str] = {}\n",
    "        self.import_relations: Dict[str, List[Tuple[str, int]]] = {}  # file -> [(module, line_no)]\n",
    "        self.module_symbols: Dict[str, Dict[str, Dict[str, any]]] = {}  # file -> {symbol -> {type, line_no, context}}\n",
    "        self.symbol_references: Dict[str, List[Tuple[str, int]]] = {}  # symbol -> [(file, line_no)]\n",
    "        self.file_index: Dict[str, int] = {}  # Maps files to indices\n",
    "        self.current_index = 0\n",
    "        self.directories: Set[str] = set()\n",
    "        # Add a new index for all symbols to quickly locate them\n",
    "        self.symbol_index: Dict[str, List[Dict]] = {}  # symbol -> [{file, type, line_no, context}]\n",
    "\n",
    "    def _get_next_index(self) -> int:\n",
    "        \"\"\"Get next available index for file indexing.\"\"\"\n",
    "        self.current_index += 1\n",
    "        return self.current_index\n",
    "\n",
    "    def _chunk_code(self, code: str, lines_per_chunk: int = 20) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Chunk the given code into snippets.\n",
    "        Returns a list of dictionaries with 'code_snippet', 'start_line', and 'end_line'.\n",
    "        \"\"\"\n",
    "        lines = code.splitlines()\n",
    "        chunks = []\n",
    "        for i in range(0, len(lines), lines_per_chunk):\n",
    "            chunk_lines = lines[i:i + lines_per_chunk]\n",
    "            chunk = {\n",
    "                'code_snippet': '\\n'.join(chunk_lines),\n",
    "                'start_line': i + 1,\n",
    "                'end_line': i + len(chunk_lines)\n",
    "            }\n",
    "            chunks.append(chunk)\n",
    "        return chunks\n",
    "\n",
    "    def _get_context_around_line(self, file_path: str, line_no: int, context_lines: int = 3) -> str:\n",
    "        \"\"\"Extract context around a specific line in a file.\"\"\"\n",
    "        if file_path not in self.file_contents:\n",
    "            return \"\"\n",
    "        \n",
    "        lines = self.file_contents[file_path].splitlines()\n",
    "        start = max(0, line_no - context_lines - 1)\n",
    "        end = min(len(lines), line_no + context_lines)\n",
    "        \n",
    "        context = \"\\n\".join(lines[start:end])\n",
    "        return context\n",
    "\n",
    "    def parse_files(self) -> None:\n",
    "        \"\"\"Parse all Python files in the directory and build relationships.\"\"\"\n",
    "        # First pass: Index all files and create directory nodes\n",
    "        for root, dirs, files in os.walk(self.root_dir):\n",
    "            # Add directory node\n",
    "            rel_dir = os.path.relpath(root, self.root_dir)\n",
    "            if rel_dir != '.':\n",
    "                self.directories.add(rel_dir)\n",
    "                self.graph.add_node(rel_dir, type='directory')\n",
    "\n",
    "            # Index Python files\n",
    "            for file in files:\n",
    "                if file.endswith('.py'):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    rel_path = os.path.relpath(file_path, self.root_dir)\n",
    "                    self.file_index[rel_path] = self._get_next_index()\n",
    "                    \n",
    "                    try:\n",
    "                        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                            content = f.read()\n",
    "                            self.file_contents[rel_path] = content\n",
    "                            self._analyze_file(rel_path, content)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error parsing {file_path}: {e}\")\n",
    "        \n",
    "        # Second pass: Find symbol references across files\n",
    "        self._find_symbol_references()\n",
    "        \n",
    "        # Build the symbol index after all analyses\n",
    "        self._build_symbol_index()\n",
    "\n",
    "    def _analyze_file(self, file_path: str, content: str) -> None:\n",
    "        \"\"\"Analyze a single file for imports and symbols with line numbers and context.\"\"\"\n",
    "        try:\n",
    "            tree = ast.parse(content)\n",
    "            imports = []\n",
    "            symbols = {}\n",
    "\n",
    "            for node in ast.walk(tree):\n",
    "                # Track imports\n",
    "                if isinstance(node, (ast.Import, ast.ImportFrom)):\n",
    "                    if isinstance(node, ast.Import):\n",
    "                        for name in node.names:\n",
    "                            imports.append((name.name, node.lineno))\n",
    "                    else:  # ImportFrom\n",
    "                        module = node.module if node.module else ''\n",
    "                        for name in node.names:\n",
    "                            imports.append((f\"{module}.{name.name}\" if module else name.name, node.lineno))\n",
    "\n",
    "                # Track defined symbols with line numbers and context\n",
    "                elif isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.Assign)):\n",
    "                    if isinstance(node, (ast.FunctionDef, ast.ClassDef)):\n",
    "                        symbol_name = node.name\n",
    "                        symbol_type = 'class' if isinstance(node, ast.ClassDef) else 'function'\n",
    "                        line_no = node.lineno\n",
    "                        context = self._extract_node_source(content, node)\n",
    "                        \n",
    "                        symbols[symbol_name] = {\n",
    "                            'type': symbol_type,\n",
    "                            'line_no': line_no,\n",
    "                            'context': context,\n",
    "                            'docstring': ast.get_docstring(node)\n",
    "                        }\n",
    "                    elif isinstance(node, ast.Assign):\n",
    "                        # Handle variable assignments\n",
    "                        for target in node.targets:\n",
    "                            if isinstance(target, ast.Name):\n",
    "                                symbol_name = target.id\n",
    "                                line_no = node.lineno\n",
    "                                context = self._extract_node_source(content, node)\n",
    "                                \n",
    "                                symbols[symbol_name] = {\n",
    "                                    'type': 'variable',\n",
    "                                    'line_no': line_no,\n",
    "                                    'context': context\n",
    "                                }\n",
    "\n",
    "            self.import_relations[file_path] = imports\n",
    "            self.module_symbols[file_path] = symbols\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing {file_path}: {e}\")\n",
    "\n",
    "    def _extract_node_source(self, source: str, node) -> str:\n",
    "        \"\"\"Extract the source code for an AST node.\"\"\"\n",
    "        try:\n",
    "            lines = source.splitlines()\n",
    "            if hasattr(node, 'lineno') and hasattr(node, 'end_lineno'):\n",
    "                start = node.lineno - 1\n",
    "                end = getattr(node, 'end_lineno', start + 1)\n",
    "                return '\\n'.join(lines[start:end])\n",
    "            return \"\"\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "\n",
    "    def _find_symbol_references(self) -> None:\n",
    "        \"\"\"Find references to symbols across all files.\"\"\"\n",
    "        for file_path, content in self.file_contents.items():\n",
    "            try:\n",
    "                tree = ast.parse(content)\n",
    "                self._process_file_for_references(file_path, tree, content)\n",
    "            except Exception as e:\n",
    "                print(f\"Error finding references in {file_path}: {e}\")\n",
    "\n",
    "    def _process_file_for_references(self, file_path: str, tree, source: str) -> None:\n",
    "        \"\"\"Process a file's AST to find references to symbols.\"\"\"\n",
    "        for node in ast.walk(tree):\n",
    "            # Find variable references\n",
    "            if isinstance(node, ast.Name) and isinstance(node.ctx, ast.Load):\n",
    "                symbol_name = node.id\n",
    "                line_no = node.lineno\n",
    "                \n",
    "                # Track reference with context\n",
    "                if symbol_name not in self.symbol_references:\n",
    "                    self.symbol_references[symbol_name] = []\n",
    "                \n",
    "                context = self._get_context_around_line(file_path, line_no)\n",
    "                self.symbol_references[symbol_name].append((file_path, line_no, context))\n",
    "            \n",
    "            # Find attribute references (e.g., obj.method())\n",
    "            elif isinstance(node, ast.Attribute) and isinstance(node.ctx, ast.Load):\n",
    "                attr_name = node.attr\n",
    "                line_no = node.lineno\n",
    "                \n",
    "                if attr_name not in self.symbol_references:\n",
    "                    self.symbol_references[attr_name] = []\n",
    "                \n",
    "                context = self._get_context_around_line(file_path, line_no)\n",
    "                self.symbol_references[attr_name].append((file_path, line_no, context))\n",
    "\n",
    "    def _build_symbol_index(self) -> None:\n",
    "        \"\"\"Build a comprehensive index of all symbols and where they're defined/used.\"\"\"\n",
    "        # Initialize the symbol index\n",
    "        self.symbol_index = {}\n",
    "        \n",
    "        # First, add all symbol definitions\n",
    "        for file_path, symbols in self.module_symbols.items():\n",
    "            for symbol_name, details in symbols.items():\n",
    "                if symbol_name not in self.symbol_index:\n",
    "                    self.symbol_index[symbol_name] = []\n",
    "                \n",
    "                self.symbol_index[symbol_name].append({\n",
    "                    'file': file_path,\n",
    "                    'type': 'definition',\n",
    "                    'symbol_type': details['type'],\n",
    "                    'line_no': details['line_no'],\n",
    "                    'context': details.get('context', ''),\n",
    "                    'docstring': details.get('docstring', '')\n",
    "                })\n",
    "        \n",
    "        # Then, add all references\n",
    "        for symbol_name, references in self.symbol_references.items():\n",
    "            if symbol_name not in self.symbol_index:\n",
    "                self.symbol_index[symbol_name] = []\n",
    "            \n",
    "            for file_path, line_no, context in references:\n",
    "                # Avoid duplicating references if they're already in definitions\n",
    "                if not any(ref['file'] == file_path and ref['line_no'] == line_no and ref['type'] == 'definition' \n",
    "                          for ref in self.symbol_index.get(symbol_name, [])):\n",
    "                    self.symbol_index[symbol_name].append({\n",
    "                        'file': file_path,\n",
    "                        'type': 'reference',\n",
    "                        'line_no': line_no,\n",
    "                        'context': context\n",
    "                    })\n",
    "\n",
    "    def build_graph(self) -> nx.DiGraph:\n",
    "        \"\"\"Build the NetworkX graph with enhanced node and edge information.\"\"\"\n",
    "        # Start with a directed graph for clarity in relationships\n",
    "        dot_graph = nx.DiGraph()\n",
    "        \n",
    "        # Add nodes for all files with indices and code snippet nodes\n",
    "        for file_path, file_idx in self.file_index.items():\n",
    "            dot_graph.add_node(file_path, \n",
    "                               type='file',\n",
    "                               file_index=file_idx,\n",
    "                               directory=os.path.dirname(file_path))\n",
    "            \n",
    "            # Create snippet nodes for the entire file\n",
    "            if file_path in self.file_contents:\n",
    "                chunks = self._chunk_code(self.file_contents[file_path])\n",
    "                for idx, chunk_info in enumerate(chunks):\n",
    "                    snippet_node = f\"{file_path}::snippet::{idx}\"\n",
    "                    dot_graph.add_node(snippet_node,\n",
    "                                       type='snippet',\n",
    "                                       code_snippet=chunk_info['code_snippet'],\n",
    "                                       start_line=chunk_info['start_line'],\n",
    "                                       end_line=chunk_info['end_line'])\n",
    "                    # Connect file node to snippet node\n",
    "                    dot_graph.add_edge(file_path, snippet_node, \n",
    "                                       edge_type='contains_snippet',\n",
    "                                       start_line=chunk_info['start_line'],\n",
    "                                       end_line=chunk_info['end_line'])\n",
    "\n",
    "            # Add nodes for symbols in this file\n",
    "            for symbol, details in self.module_symbols.get(file_path, {}).items():\n",
    "                symbol_node = f\"{file_path}::{symbol}\"\n",
    "                dot_graph.add_node(symbol_node, \n",
    "                                   type='symbol',\n",
    "                                   symbol_type=details['type'],\n",
    "                                   line_number=details['line_no'],\n",
    "                                   context=details.get('context', ''),\n",
    "                                   docstring=details.get('docstring', ''))\n",
    "                dot_graph.add_edge(file_path, symbol_node, \n",
    "                                   edge_type='defines',\n",
    "                                   line_number=details['line_no'])\n",
    "\n",
    "        # Add directory nodes\n",
    "        for directory in self.directories:\n",
    "            dot_graph.add_node(directory, type='directory')\n",
    "\n",
    "        # Add edges for imports with line numbers\n",
    "        for file_path, imports in self.import_relations.items():\n",
    "            for imp, line_no in imports:\n",
    "                # Look for matching files or symbols\n",
    "                for target_file, symbols in self.module_symbols.items():\n",
    "                    if imp in symbols:\n",
    "                        dot_graph.add_edge(file_path, \n",
    "                                           f\"{target_file}::{imp}\",\n",
    "                                           edge_type='import',\n",
    "                                           line_number=line_no)\n",
    "                    elif target_file.replace('.py', '').endswith(imp):\n",
    "                        dot_graph.add_edge(file_path, \n",
    "                                           target_file,\n",
    "                                           edge_type='import',\n",
    "                                           line_number=line_no)\n",
    "        \n",
    "        # Add edges for symbol references with line numbers and context\n",
    "        for symbol, references in self.symbol_references.items():\n",
    "            for file_path, symbols in self.module_symbols.items():\n",
    "                if symbol in symbols:\n",
    "                    symbol_node = f\"{file_path}::{symbol}\"\n",
    "                    \n",
    "                    # Connect symbol to all its references\n",
    "                    for ref_file, ref_line, context in references:\n",
    "                        if ref_file != file_path:  # Only add cross-file references\n",
    "                            dot_graph.add_edge(ref_file, \n",
    "                                              symbol_node,\n",
    "                                              edge_type='references',\n",
    "                                              line_number=ref_line,\n",
    "                                              context=context)\n",
    "        \n",
    "        # Save the built graph in self.graph for later export\n",
    "        self.graph = dot_graph\n",
    "        return dot_graph\n",
    "\n",
    "    def find_symbol_usages(self, symbol_name: str) -> List[Dict]:\n",
    "        \"\"\"Find all usages of a symbol in the codebase with context.\"\"\"\n",
    "        # Use the symbol index for fast lookup\n",
    "        if symbol_name in self.symbol_index:\n",
    "            return self.symbol_index[symbol_name]\n",
    "        return []\n",
    "\n",
    "    def find_symbol_by_partial_name(self, partial_name: str) -> Dict[str, List[Dict]]:\n",
    "        \"\"\"Find symbols that match the partial name (case-insensitive).\"\"\"\n",
    "        results = {}\n",
    "        partial_name_lower = partial_name.lower()\n",
    "        \n",
    "        for symbol_name, occurrences in self.symbol_index.items():\n",
    "            if partial_name_lower in symbol_name.lower():\n",
    "                results[symbol_name] = occurrences\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def get_symbol_definitions(self, symbol_name: str) -> List[Dict]:\n",
    "        \"\"\"Get all definitions of a symbol across the codebase.\"\"\"\n",
    "        if symbol_name in self.symbol_index:\n",
    "            return [item for item in self.symbol_index[symbol_name] if item['type'] == 'definition']\n",
    "        return []\n",
    "\n",
    "    def get_symbol_references(self, symbol_name: str) -> List[Dict]:\n",
    "        \"\"\"Get all references to a symbol across the codebase.\"\"\"\n",
    "        if symbol_name in self.symbol_index:\n",
    "            return [item for item in self.symbol_index[symbol_name] if item['type'] == 'reference']\n",
    "        return []\n",
    "\n",
    "    def infer_symbol_location(self, symbol_query: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Infer the location and details of a symbol based on a query.\n",
    "        This function makes it easier for LLMs to find the right symbol.\n",
    "        \"\"\"\n",
    "        # First, try exact match\n",
    "        if symbol_query in self.symbol_index:\n",
    "            definitions = self.get_symbol_definitions(symbol_query)\n",
    "            if definitions:\n",
    "                return {\n",
    "                    'found': True,\n",
    "                    'exact_match': True,\n",
    "                    'symbol_name': symbol_query,\n",
    "                    'definitions': definitions,\n",
    "                    'references': self.get_symbol_references(symbol_query)\n",
    "                }\n",
    "        \n",
    "        # Try partial match\n",
    "        partial_matches = self.find_symbol_by_partial_name(symbol_query)\n",
    "        if partial_matches:\n",
    "            # Sort matches by relevance (length of partial match compared to full name)\n",
    "            sorted_matches = sorted(\n",
    "                partial_matches.items(), \n",
    "                key=lambda x: (abs(len(x[0]) - len(symbol_query)), x[0])\n",
    "            )\n",
    "            \n",
    "            # Get the most relevant matches\n",
    "            relevant_symbols = {}\n",
    "            for symbol_name, occurrences in sorted_matches[:5]:  # Take top 5 matches\n",
    "                definitions = [item for item in occurrences if item['type'] == 'definition']\n",
    "                references = [item for item in occurrences if item['type'] == 'reference']\n",
    "                \n",
    "                if definitions:  # Only include symbols with definitions\n",
    "                    relevant_symbols[symbol_name] = {\n",
    "                        'definitions': definitions,\n",
    "                        'references': references\n",
    "                    }\n",
    "            \n",
    "            if relevant_symbols:\n",
    "                return {\n",
    "                    'found': True,\n",
    "                    'exact_match': False,\n",
    "                    'partial_matches': relevant_symbols\n",
    "                }\n",
    "        \n",
    "        # No matches found\n",
    "        return {\n",
    "            'found': False,\n",
    "            'message': f\"No symbols matching '{symbol_query}' found in the codebase.\"\n",
    "        }\n",
    "\n",
    "    def export_file_index(self, output_path: str = 'file_index.txt') -> None:\n",
    "        \"\"\"Export the file index mapping to a text file.\"\"\"\n",
    "        with open(output_path, 'w') as f:\n",
    "            # Write header\n",
    "            f.write(\"File Index Mapping\\n\")\n",
    "            f.write(\"=================\\n\\n\")\n",
    "            \n",
    "            # Sort by index for better readability\n",
    "            sorted_items = sorted(self.file_index.items(), key=lambda x: x[1])\n",
    "            \n",
    "            # Write each file and its index\n",
    "            for file_path, index in sorted_items:\n",
    "                f.write(f\"{index}: {file_path}\\n\")\n",
    "                \n",
    "                # If there are symbols in this file, list them with line numbers\n",
    "                if file_path in self.module_symbols:\n",
    "                    f.write(\"  Symbols:\\n\")\n",
    "                    for symbol, details in self.module_symbols[file_path].items():\n",
    "                        symbol_type = details['type']\n",
    "                        line_no = details['line_no']\n",
    "                        f.write(f\"    - {symbol} ({symbol_type}) at line {line_no}\\n\")\n",
    "                    f.write(\"\\n\")\n",
    "\n",
    "    def export_symbol_index(self, output_path: str = 'symbol_index.json') -> None:\n",
    "        \"\"\"Export the symbol index to a JSON file for quick lookups.\"\"\"\n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(self.symbol_index, f, indent=2)\n",
    "\n",
    "    def _analyze_symbol_purpose(self, symbol_name, usages):\n",
    "        \"\"\"Analyze the purpose of a symbol based on its usage patterns.\"\"\"\n",
    "        # Get the definition if available\n",
    "        definitions = [u for u in usages if u['type'] == 'definition']\n",
    "        references = [u for u in usages if u['type'] == 'reference']\n",
    "        \n",
    "        purpose = \"\"\n",
    "        \n",
    "        # Check if we have a definition with docstring\n",
    "        if definitions and definitions[0].get('docstring'):\n",
    "            purpose += f\"{definitions[0]['docstring']}\\n\\n\"\n",
    "        \n",
    "        # If it's a function, try to infer what it does from usage\n",
    "        if definitions and definitions[0].get('symbol_type') == 'function':\n",
    "            # Collect contexts where it's used\n",
    "            contexts = [ref.get('context', '') for ref in references if ref.get('context')]\n",
    "            \n",
    "            # Analyze contexts for patterns\n",
    "            if contexts:\n",
    "                common_patterns = self._find_common_usage_patterns(contexts, symbol_name)\n",
    "                \n",
    "                purpose += \"Based on usage patterns, this function appears to:\\n\"\n",
    "                \n",
    "                if any(\"assert\" in ctx for ctx in contexts):\n",
    "                    purpose += \"- Be used in test assertions to verify behavior\\n\"\n",
    "                \n",
    "                if any(\"if\" in ctx and symbol_name in ctx for ctx in contexts):\n",
    "                    purpose += \"- Be used as a condition in control flow statements\\n\"\n",
    "                \n",
    "                if common_patterns:\n",
    "                    for pattern in common_patterns[:3]:  # Top 3 patterns\n",
    "                        purpose += f\"- {pattern}\\n\"\n",
    "        \n",
    "        # If we couldn't infer much, provide a generic description\n",
    "        if not purpose or len(purpose.strip()) < 10:\n",
    "            if definitions and definitions[0].get('symbol_type') == 'function':\n",
    "                context = definitions[0].get('context', '')\n",
    "                \n",
    "                # Look for parameters to understand what it takes\n",
    "                params = self._extract_function_params(context)\n",
    "                \n",
    "                purpose += f\"This function appears to check or validate something\"\n",
    "                if params:\n",
    "                    purpose += f\" related to {', '.join(params)}\"\n",
    "                purpose += \".\\n\"\n",
    "        \n",
    "        return purpose\n",
    "\n",
    "    def _find_common_usage_patterns(self, contexts, symbol_name):\n",
    "        \"\"\"Find common patterns in the usage contexts.\"\"\"\n",
    "        patterns = []\n",
    "        \n",
    "        # Check if it's used with certain objects/methods frequently\n",
    "        if any(f\".{symbol_name}\" in ctx for ctx in contexts):\n",
    "            patterns.append(\"Be a method called on objects\")\n",
    "        \n",
    "        # Check if it's used for configuration or setup\n",
    "        if any(\"config\" in ctx.lower() or \"setup\" in ctx.lower() for ctx in contexts):\n",
    "            patterns.append(\"Be involved in configuration or setup\")\n",
    "        \n",
    "        # Check if it's used for logging\n",
    "        if any(\"log\" in ctx.lower() for ctx in contexts):\n",
    "            patterns.append(\"Be related to logging functionality\")\n",
    "        \n",
    "        # Check if it's used in exception handling\n",
    "        if any(\"except\" in ctx.lower() or \"try\" in ctx.lower() for ctx in contexts):\n",
    "            patterns.append(\"Be involved in exception handling\")\n",
    "        \n",
    "        return patterns\n",
    "\n",
    "    def _extract_function_params(self, context):\n",
    "        \"\"\"Extract parameter names from a function definition.\"\"\"\n",
    "        params = []\n",
    "        if context:\n",
    "            # Simple regex-based extraction\n",
    "            match = re.search(r'def\\s+\\w+\\s*\\((.*?)\\)', context, re.DOTALL)\n",
    "            if match:\n",
    "                param_string = match.group(1)\n",
    "                # Split by comma and clean up\n",
    "                raw_params = [p.strip() for p in param_string.split(',')]\n",
    "                # Extract just the parameter name (before any : or =)\n",
    "                params = [re.split(r'[=:]', p)[0].strip() for p in raw_params if p]\n",
    "                # Remove self if it's there\n",
    "                if params and params[0] == 'self':\n",
    "                    params = params[1:]\n",
    "        return params\n",
    "\n",
    "    def _general_codebase_analysis(self, query):\n",
    "        \"\"\"Provide a general analysis based on the query.\"\"\"\n",
    "        response = f\"# Analysis for Query: {query}\\n\\n\"\n",
    "        \n",
    "        # Check if the query is asking about structure\n",
    "        if any(term in query.lower() for term in ['structure', 'organization', 'layout']):\n",
    "            response += \"## Codebase Structure\\n\\n\"\n",
    "            # Count files by directory\n",
    "            files_by_dir = {}\n",
    "            for file_path in self.file_index:\n",
    "                directory = os.path.dirname(file_path)\n",
    "                if directory not in files_by_dir:\n",
    "                    files_by_dir[directory] = []\n",
    "                files_by_dir[directory].append(file_path)\n",
    "            \n",
    "            response += f\"The codebase contains {len(self.file_index)} Python files across {len(files_by_dir)} directories.\\n\\n\"\n",
    "            \n",
    "            # Show top-level directories\n",
    "            response += \"Main directories:\\n\"\n",
    "            for directory, files in sorted(files_by_dir.items(), key=lambda x: len(x[1]), reverse=True)[:10]:\n",
    "                dir_name = directory if directory else '(root)'\n",
    "                response += f\"- {dir_name}: {len(files)} files\\n\"\n",
    "        \n",
    "        # Check if the query is asking about specific functionality\n",
    "        functionality_terms = ['handle', 'process', 'create', 'generate', 'calculate']\n",
    "        for term in functionality_terms:\n",
    "            if term in query.lower():\n",
    "                # Search for functions with this term\n",
    "                matching_functions = []\n",
    "                for file_path, symbols in self.module_symbols.items():\n",
    "                    for symbol, details in symbols.items():\n",
    "                        if details['type'] == 'function' and term in symbol.lower():\n",
    "                            matching_functions.append((file_path, symbol, details))\n",
    "                \n",
    "                if matching_functions:\n",
    "                    response += f\"\\n## Functions Related to '{term}'\\n\\n\"\n",
    "                    for file_path, symbol, details in matching_functions[:5]:  # Show top 5\n",
    "                        response += f\"- `{symbol}` in {file_path}:{details['line_no']}\\n\"\n",
    "                    if len(matching_functions) > 5:\n",
    "                        response += f\"... and {len(matching_functions) - 5} more functions\\n\"\n",
    "        \n",
    "        return response\n",
    "\n",
    "    def export_graph_json(self, output_path: str = 'codebase_graph.json') -> None:\n",
    "        \"\"\"Export the graph structure to JSON.\"\"\"\n",
    "        graph_data = {\n",
    "            'nodes': [\n",
    "                {\n",
    "                    'id': node,\n",
    "                    'type': data['type'],\n",
    "                    'file_index': data.get('file_index'),\n",
    "                    'directory': data.get('directory'),\n",
    "                    'symbol_type': data.get('symbol_type'),\n",
    "                    'line_number': data.get('line_number'),\n",
    "                    'code_snippet': data.get('code_snippet'),\n",
    "                    'start_line': data.get('start_line'),\n",
    "                    'end_line': data.get('end_line'),\n",
    "                    'context': data.get('context'),\n",
    "                    'docstring': data.get('docstring')\n",
    "                } \n",
    "                for node, data in self.graph.nodes(data=True)\n",
    "            ],\n",
    "            'links': [\n",
    "                {\n",
    "                    'source': source,\n",
    "                    'target': target,\n",
    "                    'type': data.get('edge_type'),\n",
    "                    'line_number': data.get('line_number'),\n",
    "                    'start_line': data.get('start_line'),\n",
    "                    'end_line': data.get('end_line'),\n",
    "                    'context': data.get('context')\n",
    "                } \n",
    "                for source, target, data in self.graph.edges(data=True)\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(graph_data, f, indent=2)\n",
    "            \n",
    "    def export_inference_data(self, output_path: str = 'symbol_inference.json') -> None:\n",
    "        \"\"\"\n",
    "        Export inference-friendly data about all symbols in the codebase.\n",
    "        This creates a dedicated lookup file optimized for LLMs to find symbols.\n",
    "        \"\"\"\n",
    "        inference_data = {\n",
    "            'symbols': {},\n",
    "            'metadata': {\n",
    "                'total_symbols': len(self.symbol_index),\n",
    "                'total_files': len(self.file_index),\n",
    "                'total_directories': len(self.directories)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Process each symbol with its definitions and references\n",
    "        for symbol_name, occurrences in self.symbol_index.items():\n",
    "            definitions = [item for item in occurrences if item['type'] == 'definition']\n",
    "            references = [item for item in occurrences if item['type'] == 'reference']\n",
    "            \n",
    "            if definitions:  # Only include symbols with at least one definition\n",
    "                # Create a symbol entry with all relevant information\n",
    "                symbol_entry = {\n",
    "                    'definition_files': list(set(d['file'] for d in definitions)),\n",
    "                    'definition_count': len(definitions),\n",
    "                    'reference_count': len(references),\n",
    "                    'definitions': definitions,\n",
    "                    # Include just summarized references to keep file size manageable\n",
    "                    'reference_summary': {\n",
    "                        f['file']: len([r for r in references if r['file'] == f['file']])\n",
    "                        for f in sorted(set(({'file': r['file']} for r in references)), key=lambda x: x['file'])\n",
    "                    },\n",
    "                    'symbol_types': list(set(d.get('symbol_type', 'unknown') for d in definitions)),\n",
    "                    'purpose': self._analyze_symbol_purpose(symbol_name, occurrences) if definitions else \"\"\n",
    "                }\n",
    "                \n",
    "                inference_data['symbols'][symbol_name] = symbol_entry\n",
    "        \n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(inference_data, f, indent=2)\n",
    "\n",
    "    def export_to_arango(self,\n",
    "                         db_name: str = 'codebase',\n",
    "                         username: str = 'root',\n",
    "                         password: str = 'passwd',\n",
    "                         host: str = 'http://localhost:8529') -> None:\n",
    "        \"\"\"Export the graph into ArangoDB.\"\"\"\n",
    "        client = ArangoClient(hosts=host)\n",
    "        db = client.db(username=username, password=password, verify=True)\n",
    "\n",
    "        # Delete the existing graph and its collections if they exist.\n",
    "        graph_name = \"FlaskRepv1\"\n",
    "        \n",
    "        # Import networkx_to_arangodb if it's available\n",
    "        try:\n",
    "            import networkx_to_arangodb as nxadb # type: ignore\n",
    "            G_adb = nxadb.Graph(\n",
    "                name=graph_name,\n",
    "                db=db,\n",
    "                incoming_graph_data=self.graph,\n",
    "                write_batch_size=50000,\n",
    "                overwrite_graph=True\n",
    "            )\n",
    "            \n",
    "            self.G_adb = G_adb\n",
    "            print(\"Graph successfully exported to ArangoDB.\")\n",
    "            return G_adb\n",
    "        except ImportError:\n",
    "            print(\"networkx_to_arangodb module not found. Please install it to export to ArangoDB.\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T06:32:46.868174Z",
     "iopub.status.busy": "2025-03-04T06:32:46.867712Z",
     "iopub.status.idle": "2025-03-04T06:32:46.872269Z",
     "shell.execute_reply": "2025-03-04T06:32:46.871147Z",
     "shell.execute_reply.started": "2025-03-04T06:32:46.868142Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#visualizer.export_to_arango(db_name=\"FlaskRepv1\",username=\"root\",password=\"cUZ0YaNdcwfUTw6VjRny\",host=\"https://d2eeb8083350.arangodb.cloud:8529\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T06:32:47.073946Z",
     "iopub.status.busy": "2025-03-04T06:32:47.073567Z",
     "iopub.status.idle": "2025-03-04T06:32:47.078449Z",
     "shell.execute_reply": "2025-03-04T06:32:47.077323Z",
     "shell.execute_reply.started": "2025-03-04T06:32:47.073914Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# visualizer = CodebaseVisualizer(\"flask\")\n",
    "# visualizer.parse_files()\n",
    "# G = visualizer.build_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T06:32:48.213329Z",
     "iopub.status.busy": "2025-03-04T06:32:48.212975Z",
     "iopub.status.idle": "2025-03-04T06:32:48.219590Z",
     "shell.execute_reply": "2025-03-04T06:32:48.218354Z",
     "shell.execute_reply.started": "2025-03-04T06:32:48.213302Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# class GraphVisualizer:\n",
    "#     def __init__(self, graph: nx.Graph):\n",
    "#         self.graph = graph\n",
    "#         self.pos = None\n",
    "        \n",
    "#     def set_layout(self, layout_type: str = 'spring', **layout_params) -> None:\n",
    "#         \"\"\"\n",
    "#         Set the layout for the graph visualization.\n",
    "        \n",
    "#         Args:\n",
    "#             layout_type: Type of layout ('spring', 'circular', 'kamada_kawai', \n",
    "#                         'random', 'shell', 'spectral')\n",
    "#             layout_params: Additional parameters for the layout algorithm\n",
    "#         \"\"\"\n",
    "#         layout_funcs = {\n",
    "#             'spring': nx.spring_layout,\n",
    "#             'circular': nx.circular_layout,\n",
    "#             'kamada_kawai': nx.kamada_kawai_layout,\n",
    "#             'random': nx.random_layout,\n",
    "#             'shell': nx.shell_layout,\n",
    "#             'spectral': nx.spectral_layout\n",
    "#         }\n",
    "        \n",
    "#         if layout_type not in layout_funcs:\n",
    "#             raise ValueError(f\"Unsupported layout type. Choose from: {list(layout_funcs.keys())}\")\n",
    "            \n",
    "#         self.pos = layout_funcs[layout_type](self.graph, **layout_params)\n",
    "    \n",
    "#     def _get_node_colors(self) -> Dict[str, str]:\n",
    "#         \"\"\"Extract node colors from graph attributes or generate defaults.\"\"\"\n",
    "#         colors = {}\n",
    "#         for node in self.graph.nodes():\n",
    "#             # Check for color in node attributes\n",
    "#             attrs = self.graph.nodes[node]\n",
    "#             if 'fillcolor' in attrs:\n",
    "#                 colors[node] = attrs['fillcolor']\n",
    "#             elif 'color' in attrs:\n",
    "#                 colors[node] = attrs['color']\n",
    "#             else:\n",
    "#                 colors[node] = 'lightblue'  # default color\n",
    "#         return colors\n",
    "    \n",
    "#     def _get_node_sizes(self) -> Dict[str, float]:\n",
    "#         \"\"\"Extract or compute node sizes.\"\"\"\n",
    "#         sizes = {}\n",
    "#         for node in self.graph.nodes():\n",
    "#             attrs = self.graph.nodes[node]\n",
    "#             if 'size' in attrs:\n",
    "#                 sizes[node] = attrs['size']\n",
    "#             else:\n",
    "#                 # Default size based on node degree\n",
    "#                 sizes[node] = 1000 * (1 + self.graph.degree(node) / 10)\n",
    "#         return sizes\n",
    "    \n",
    "#     def _get_edge_colors(self) -> Dict[Tuple[str, str], str]:\n",
    "#         \"\"\"Extract edge colors from graph attributes or generate defaults.\"\"\"\n",
    "#         colors = {}\n",
    "#         for u, v in self.graph.edges():\n",
    "#             edge_data = self.graph.get_edge_data(u, v)\n",
    "#             if 'color' in edge_data:\n",
    "#                 colors[(u, v)] = edge_data['color']\n",
    "#             else:\n",
    "#                 colors[(u, v)] = 'gray'  # default color\n",
    "#         return colors\n",
    "    \n",
    "#     def _get_node_labels(self) -> Dict[str, str]:\n",
    "#         \"\"\"Extract node labels from graph attributes.\"\"\"\n",
    "#         labels = {}\n",
    "#         for node in self.graph.nodes():\n",
    "#             attrs = self.graph.nodes[node]\n",
    "#             if 'label' in attrs:\n",
    "#                 labels[node] = attrs['label']\n",
    "#             else:\n",
    "#                 labels[node] = str(node)\n",
    "#         return labels\n",
    "    \n",
    "#     def _get_edge_labels(self) -> Dict[Tuple[str, str], str]:\n",
    "#         \"\"\"Extract edge labels from graph attributes.\"\"\"\n",
    "#         labels = {}\n",
    "#         for u, v in self.graph.edges():\n",
    "#             edge_data = self.graph.get_edge_data(u, v)\n",
    "#             if 'label' in edge_data:\n",
    "#                 labels[(u, v)] = edge_data['label']\n",
    "#         return labels\n",
    "\n",
    "#     def visualize(self, \n",
    "#                  figsize: Tuple[int, int] = (12, 8),\n",
    "#                  node_size: Optional[Dict[str, float]] = None,\n",
    "#                  node_color: Optional[Dict[str, str]] = None,\n",
    "#                  edge_color: Optional[Dict[Tuple[str, str], str]] = None,\n",
    "#                  with_labels: bool = True,\n",
    "#                  font_size: int = 8,\n",
    "#                  title: Optional[str] = None,\n",
    "#                  show_edge_labels: bool = True,\n",
    "#                  alpha: float = 0.7,\n",
    "#                  save_path: Optional[str] = None) -> None:\n",
    "#         \"\"\"\n",
    "#         Visualize the graph with customizable options.\n",
    "        \n",
    "#         Args:\n",
    "#             figsize: Size of the figure (width, height)\n",
    "#             node_size: Dictionary mapping nodes to their sizes\n",
    "#             node_color: Dictionary mapping nodes to their colors\n",
    "#             edge_color: Dictionary mapping edges to their colors\n",
    "#             with_labels: Whether to show node labels\n",
    "#             font_size: Size of the font for labels\n",
    "#             title: Title of the graph\n",
    "#             show_edge_labels: Whether to show edge labels\n",
    "#             alpha: Transparency of nodes\n",
    "#             save_path: Path to save the visualization (if None, displays instead)\n",
    "#         \"\"\"\n",
    "#         if self.pos is None:\n",
    "#             self.set_layout('spring')\n",
    "            \n",
    "#         plt.figure(figsize=figsize)\n",
    "        \n",
    "#         # Get or use provided node attributes\n",
    "#         node_colors = node_color if node_color is not None else self._get_node_colors()\n",
    "#         node_sizes = node_size if node_size is not None else self._get_node_sizes()\n",
    "#         edge_colors = edge_color if edge_color is not None else self._get_edge_colors()\n",
    "        \n",
    "#         # Draw nodes\n",
    "#         nx.draw_networkx_nodes(self.graph, self.pos,\n",
    "#                              node_color=[node_colors[node] for node in self.graph.nodes()],\n",
    "#                              node_size=[node_sizes[node] for node in self.graph.nodes()],\n",
    "#                              alpha=alpha)\n",
    "        \n",
    "#         # Draw edges\n",
    "#         for (u, v) in self.graph.edges():\n",
    "#             nx.draw_networkx_edges(self.graph, self.pos,\n",
    "#                                  edgelist=[(u, v)],\n",
    "#                                  edge_color=edge_colors.get((u, v), 'gray'),\n",
    "#                                  alpha=0.5)\n",
    "        \n",
    "#         # Add labels if requested\n",
    "#         if with_labels:\n",
    "#             labels = self._get_node_labels()\n",
    "#             nx.draw_networkx_labels(self.graph, self.pos, labels,\n",
    "#                                   font_size=font_size)\n",
    "        \n",
    "#         # Add edge labels if requested\n",
    "#         if show_edge_labels:\n",
    "#             edge_labels = self._get_edge_labels()\n",
    "#             if edge_labels:\n",
    "#                 nx.draw_networkx_edge_labels(self.graph, self.pos,\n",
    "#                                            edge_labels=edge_labels,\n",
    "#                                            font_size=font_size-2)\n",
    "        \n",
    "#         if title:\n",
    "#             plt.title(title)\n",
    "        \n",
    "#         plt.axis('off')\n",
    "        \n",
    "#         if save_path:\n",
    "#             plt.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "#             plt.close()\n",
    "#         else:\n",
    "#             plt.show()\n",
    "\n",
    "# # Example usage:\n",
    "# '''\n",
    "# # Create a sample graph\n",
    "# G = nx.Graph()\n",
    "# G.add_nodes_from([\n",
    "#     (1, {'fillcolor': 'lightblue', 'label': 'Node 1'}),\n",
    "#     (2, {'fillcolor': 'lightgreen', 'label': 'Node 2'}),\n",
    "#     (3, {'fillcolor': 'lightred', 'label': 'Node 3'})\n",
    "# ])\n",
    "# G.add_edges_from([\n",
    "#     (1, 2, {'color': 'blue', 'label': 'Edge 1-2'}),\n",
    "#     (2, 3, {'color': 'red', 'label': 'Edge 2-3'})\n",
    "# ])\n",
    "# '''\n",
    "# # Create visualizer and display graph\n",
    "# visualizer = GraphVisualizer(G)\n",
    "# visualizer.set_layout('spring', k=2)  # k controls the spacing between nodes\n",
    "# visualizer.visualize(\n",
    "#     figsize=(10, 8),\n",
    "#     font_size=10,\n",
    "#     title=\"Sample Graph Visualization\",\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T06:32:50.079458Z",
     "iopub.status.busy": "2025-03-04T06:32:50.078973Z",
     "iopub.status.idle": "2025-03-04T06:32:50.103278Z",
     "shell.execute_reply": "2025-03-04T06:32:50.101877Z",
     "shell.execute_reply.started": "2025-03-04T06:32:50.079408Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pyvis.network import Network\n",
    "\n",
    "def visualize_codebase_graph(codebase_path, output_html=\"codebase_graph.html\", limit_nodes=None):\n",
    "    \"\"\"\n",
    "    Visualize the codebase graph using pyvis.\n",
    "    \n",
    "    Args:\n",
    "        codebase_path: Path to the codebase directory\n",
    "        output_html: Output HTML file for the visualization/\n",
    "        limit_nodes: Optional limit on the number of nodes to display (for large codebases)\n",
    "    \"\"\"\n",
    "    # Initialize and build the graph\n",
    "    visualizer = CodebaseVisualizer(codebase_path)\n",
    "    visualizer.parse_files()\n",
    "    graph = visualizer.build_graph()\n",
    "    \n",
    "    # Export graph to JSON (optional)\n",
    "    visualizer.export_graph_json('codebase_graph.json')\n",
    "    \n",
    "    # Create a pyvis network\n",
    "    net = Network(height=\"900px\", width=\"100%\", bgcolor=\"#222222\", font_color=\"white\")\n",
    "    \n",
    "    # Configure physics\n",
    "    net.barnes_hut(gravity=-5000, central_gravity=0.3, spring_length=200)\n",
    "    \n",
    "    # Define node groups and colors\n",
    "    node_colors = {\n",
    "        'file': '#4287f5',\n",
    "        'directory': '#42f5a7',\n",
    "        'symbol': '#f542cb',\n",
    "        'snippet': '#f5a742'\n",
    "    }\n",
    "    \n",
    "    # If we need to limit nodes for performance\n",
    "    if limit_nodes and len(graph.nodes()) > limit_nodes:\n",
    "        # Focus on file and directory nodes, and limit symbol nodes\n",
    "        important_nodes = [node for node, data in graph.nodes(data=True) \n",
    "                          if data.get('type') in ['file', 'directory']]\n",
    "        \n",
    "        # Add some symbol nodes to reach the limit\n",
    "        symbols = [node for node, data in graph.nodes(data=True) \n",
    "                  if data.get('type') == 'symbol']\n",
    "        \n",
    "        # Take a subset of symbols based on connectivity\n",
    "        symbol_importance = sorted(\n",
    "            [(node, graph.degree(node)) for node in symbols],\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        important_symbols = [node for node, _ in symbol_importance[:limit_nodes - len(important_nodes)]]\n",
    "        selected_nodes = important_nodes + important_symbols\n",
    "        \n",
    "        # Create a subgraph\n",
    "        graph = graph.subgraph(selected_nodes)\n",
    "    \n",
    "    # Add nodes with appropriate styles\n",
    "    for node, node_data in graph.nodes(data=True):\n",
    "        node_type = node_data.get('type', 'unknown')\n",
    "        label = os.path.basename(node) if '/' in node else node\n",
    "        \n",
    "        # Truncate very long labels\n",
    "        if len(label) > 30:\n",
    "            label = label[:27] + \"...\"\n",
    "        \n",
    "        # Create hover title with more details\n",
    "        title = f\"<div style='max-width:300px;'>\"\n",
    "        title += f\"<b>{node}</b><br>\"\n",
    "        title += f\"Type: {node_type}<br>\"\n",
    "        \n",
    "        if node_type == 'file':\n",
    "            title += f\"Directory: {node_data.get('directory', 'N/A')}<br>\"\n",
    "            \n",
    "        elif node_type == 'symbol':\n",
    "            title += f\"Symbol type: {node_data.get('symbol_type', 'N/A')}<br>\"\n",
    "            title += f\"Line: {node_data.get('line_number', 'N/A')}<br>\"\n",
    "            \n",
    "            if node_data.get('docstring'):\n",
    "                docstring = node_data['docstring']\n",
    "                if len(docstring) > 200:\n",
    "                    docstring = docstring[:197] + \"...\"\n",
    "                title += f\"Docstring: {docstring}<br>\"\n",
    "                \n",
    "        elif node_type == 'snippet':\n",
    "            title += f\"Lines: {node_data.get('start_line', 'N/A')}-{node_data.get('end_line', 'N/A')}<br>\"\n",
    "            \n",
    "            if node_data.get('code_snippet'):\n",
    "                snippet = node_data['code_snippet'].replace('\\n', '<br>')\n",
    "                if len(snippet) > 300:\n",
    "                    snippet = snippet[:297] + \"...\"\n",
    "                title += f\"Code:<br><pre>{snippet}</pre>\"\n",
    "                \n",
    "        title += \"</div>\"\n",
    "        \n",
    "        # Determine node size based on type and connections\n",
    "        size = 15  # Default size\n",
    "        if node_type == 'directory':\n",
    "            size = 25\n",
    "        elif node_type == 'file':\n",
    "            size = 20\n",
    "        elif node_type == 'symbol' and node_data.get('symbol_type') == 'class':\n",
    "            size = 18\n",
    "        \n",
    "        # Add node with appropriate styling\n",
    "        net.add_node(\n",
    "            node, \n",
    "            label=label, \n",
    "            title=title,\n",
    "            color=node_colors.get(node_type, '#999999'),\n",
    "            size=size,\n",
    "            shape='dot' if node_type != 'directory' else 'diamond'\n",
    "        )\n",
    "    \n",
    "    # Add edges with appropriate styles\n",
    "    for source, target, edge_data in graph.edges(data=True):\n",
    "        edge_type = edge_data.get('edge_type', 'unknown')\n",
    "        \n",
    "        # Style edges differently based on type\n",
    "        if edge_type == 'import':\n",
    "            color = '#f5f542'\n",
    "            width = 2\n",
    "            dashes = False\n",
    "        elif edge_type == 'references':\n",
    "            color = '#f54242'\n",
    "            width = 1\n",
    "            dashes = [5, 5]\n",
    "        elif edge_type == 'defines':\n",
    "            color = '#42f55a'\n",
    "            width = 3\n",
    "            dashes = False\n",
    "        elif edge_type == 'contains_snippet':\n",
    "            color = '#42c8f5'\n",
    "            width = 1\n",
    "            dashes = [2, 2]\n",
    "        else:\n",
    "            color = '#999999'\n",
    "            width = 1\n",
    "            dashes = False\n",
    "        \n",
    "        # Create hover title with edge details\n",
    "        title = f\"<div><b>{edge_type}</b><br>\"\n",
    "        \n",
    "        if edge_data.get('line_number'):\n",
    "            title += f\"Line: {edge_data['line_number']}<br>\"\n",
    "            \n",
    "        if edge_data.get('context'):\n",
    "            context = edge_data['context']\n",
    "            if len(context) > 200:\n",
    "                context = context[:197] + \"...\"\n",
    "            title += f\"Context: {context}\"\n",
    "            \n",
    "        title += \"</div>\"\n",
    "        \n",
    "        # Add edge with styling\n",
    "        net.add_edge(\n",
    "            source, \n",
    "            target, \n",
    "            title=title,\n",
    "            color=color,\n",
    "            width=width,\n",
    "            dashes=dashes\n",
    "        )\n",
    "    \n",
    "    # Enable physics, navigation and interaction options\n",
    "    net.toggle_physics(True)\n",
    "    net.show_buttons(filter_=['physics'])\n",
    "    \n",
    "    # Save the visualization\n",
    "    net.save_graph(output_html)\n",
    "    print(f\"Graph visualization saved to {output_html}\")\n",
    "    \n",
    "    return output_html\n",
    "\n",
    "def visualize_symbol_subgraph(visualizer, symbol_name, output_html=\"symbol_graph.html\"):\n",
    "    \"\"\"\n",
    "    Create a focused visualization of a symbol and its relationships.\n",
    "    \n",
    "    Args:\n",
    "        visualizer: Initialized CodebaseVisualizer instance\n",
    "        symbol_name: Name of the symbol to visualize\n",
    "        output_html: Output HTML file for the visualization\n",
    "    \"\"\"\n",
    "    # Get the full graph\n",
    "    full_graph = visualizer.graph\n",
    "    \n",
    "    # Find all nodes related to this symbol\n",
    "    symbol_nodes = [node for node in full_graph.nodes() if f\"::{symbol_name}\" in node]\n",
    "    \n",
    "    if not symbol_nodes:\n",
    "        print(f\"Symbol '{symbol_name}' not found in the graph.\")\n",
    "        return None\n",
    "    \n",
    "    # Get nodes that are connected to symbol nodes (1-hop neighborhood)\n",
    "    related_nodes = set(symbol_nodes)\n",
    "    for node in symbol_nodes:\n",
    "        # Add predecessors (nodes that reference this symbol)\n",
    "        related_nodes.update(full_graph.predecessors(node))\n",
    "        # Add successors (nodes that this symbol references)\n",
    "        related_nodes.update(full_graph.successors(node))\n",
    "    \n",
    "    # Create a subgraph\n",
    "    subgraph = full_graph.subgraph(related_nodes)\n",
    "    \n",
    "    # Create a pyvis network\n",
    "    net = Network(height=\"750px\", width=\"100%\", bgcolor=\"#222222\", font_color=\"white\")\n",
    "    net.barnes_hut(gravity=-2000, central_gravity=0.3, spring_length=150)\n",
    "    \n",
    "    # Node colors by type\n",
    "    node_colors = {\n",
    "        'file': '#4287f5',\n",
    "        'directory': '#42f5a7',\n",
    "        'symbol': '#f542cb',\n",
    "        'snippet': '#f5a742'\n",
    "    }\n",
    "    \n",
    "    # Add nodes with appropriate styles\n",
    "    for node, node_data in subgraph.nodes(data=True):\n",
    "        node_type = node_data.get('type', 'unknown')\n",
    "        label = os.path.basename(node) if '/' in node else node\n",
    "        \n",
    "        # Make the focus symbol nodes larger and highlighted\n",
    "        if node in symbol_nodes:\n",
    "            size = 30\n",
    "            color = '#ff0000'  # Bright red for focus\n",
    "        else:\n",
    "            size = 15\n",
    "            color = node_colors.get(node_type, '#999999')\n",
    "        \n",
    "        # Add hover information\n",
    "        title = f\"<div style='max-width:300px;'>\"\n",
    "        title += f\"<b>{node}</b><br>\"\n",
    "        title += f\"Type: {node_type}<br>\"\n",
    "        \n",
    "        if node_type == 'symbol':\n",
    "            title += f\"Symbol type: {node_data.get('symbol_type', 'N/A')}<br>\"\n",
    "            if node_data.get('docstring'):\n",
    "                title += f\"Docstring: {node_data.get('docstring', '')}<br>\"\n",
    "                \n",
    "        title += \"</div>\"\n",
    "        \n",
    "        # Add node with styling\n",
    "        net.add_node(\n",
    "            node,\n",
    "            label=label,\n",
    "            title=title,\n",
    "            color=color,\n",
    "            size=size\n",
    "        )\n",
    "    \n",
    "    # Add edges with styles\n",
    "    for source, target, edge_data in subgraph.edges(data=True):\n",
    "        edge_type = edge_data.get('edge_type', 'unknown')\n",
    "        \n",
    "        # Style edges by type\n",
    "        if edge_type == 'import':\n",
    "            color = '#f5f542'  # Yellow\n",
    "        elif edge_type == 'references':\n",
    "            color = '#f54242'  # Red\n",
    "        elif edge_type == 'defines':\n",
    "            color = '#42f55a'  # Green\n",
    "        else:\n",
    "            color = '#999999'  # Gray\n",
    "        \n",
    "        # Add edge with details in hover\n",
    "        title = f\"{edge_type}\"\n",
    "        if edge_data.get('line_number'):\n",
    "            title += f\" (line {edge_data['line_number']})\"\n",
    "            \n",
    "        net.add_edge(source, target, title=title, color=color)\n",
    "    \n",
    "    # Enable physics and navigation\n",
    "    net.toggle_physics(True)\n",
    "    net.show_buttons(filter_=['physics'])\n",
    "    \n",
    "    # Save the visualization\n",
    "    net.save_graph(output_html)\n",
    "    print(f\"Symbol graph visualization saved to {output_html}\")\n",
    "    \n",
    "    return output_html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T11:54:20.555980Z",
     "iopub.status.busy": "2025-03-01T11:54:20.555647Z",
     "iopub.status.idle": "2025-03-01T11:54:22.195505Z",
     "shell.execute_reply": "2025-03-01T11:54:22.194779Z",
     "shell.execute_reply.started": "2025-03-01T11:54:20.555958Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph visualization saved to codebase_graph.html\n",
      "Found in: src/flask/logging.py at line 31\n",
      "Documentation: Check if there is a handler in the logging chain that will handle the\n",
      "given logger's :meth:`effective level <~logging.Logger.getEffectiveLevel>`.\n"
     ]
    }
   ],
   "source": [
    "# Visualize the entire codebase graph (with node limit for performance)\n",
    "visualize_codebase_graph(\"flask\", limit_nodes=200)\n",
    "\n",
    "# Visualize a specific symbol (like \"has_level_handler\")\n",
    "visualizer = CodebaseVisualizer(\"flask\")\n",
    "visualizer.parse_files()\n",
    "G = visualizer.build_graph()\n",
    "\n",
    "result = visualizer.infer_symbol_location(\"has_level_handler\")\n",
    "if result['found'] and result['exact_match']:\n",
    "    definition = result['definitions'][0]\n",
    "    print(f\"Found in: {definition['file']} at line {definition['line_no']}\")\n",
    "    print(f\"Documentation: {definition.get('docstring', 'No docstring')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T11:54:26.520322Z",
     "iopub.status.busy": "2025-03-01T11:54:26.519967Z",
     "iopub.status.idle": "2025-03-01T11:54:26.947993Z",
     "shell.execute_reply": "2025-03-01T11:54:26.947235Z",
     "shell.execute_reply.started": "2025-03-01T11:54:26.520291Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<StandardDatabase _system>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "db = ArangoClient(hosts=\"https://d2eeb8083350.arangodb.cloud:8529\").db(username=\"root\", password=\"cUZ0YaNdcwfUTw6VjRny\", verify=True)\n",
    "\n",
    "print(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T06:29:42.589733Z",
     "iopub.status.busy": "2025-03-04T06:29:42.589269Z",
     "iopub.status.idle": "2025-03-04T06:29:42.599518Z",
     "shell.execute_reply": "2025-03-04T06:29:42.597248Z",
     "shell.execute_reply.started": "2025-03-04T06:29:42.589696Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[12:01:40 +0530] [INFO]: Overwriting graph 'FlaskRepv1'\n",
      "[12:01:41 +0530] [INFO]: Graph 'FlaskRepv1' exists.\n",
      "[12:01:41 +0530] [INFO]: Default node type set to 'FlaskRepv1_node'\n",
      "[2025/03/08 12:01:43 +0530] [2586] [INFO] - adbnx_adapter: Instantiated ADBNX_Adapter with database '_system'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025/03/08 12:02:28 +0530] [2586] [INFO] - adbnx_adapter: Created ArangoDB 'FlaskRepv1' Graph\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph named 'FlaskRepv1' with 2974 nodes and 8652 edges\n"
     ]
    }
   ],
   "source": [
    "\n",
    "G_adb = nxadb.Graph(\n",
    "    name=\"FlaskRepv1\",\n",
    "    db=db,\n",
    "    incoming_graph_data=G,\n",
    "    write_batch_size=50000, # feel free to modify\n",
    "    overwrite_graph=True\n",
    ")\n",
    "\n",
    "print(G_adb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run from loaded database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T06:36:49.703703Z",
     "iopub.status.busy": "2025-03-04T06:36:49.703340Z",
     "iopub.status.idle": "2025-03-04T06:36:49.748066Z",
     "shell.execute_reply": "2025-03-04T06:36:49.746918Z",
     "shell.execute_reply.started": "2025-03-04T06:36:49.703672Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<StandardDatabase _system>\n"
     ]
    }
   ],
   "source": [
    "db = ArangoClient(hosts=\"https://d2eeb8083350.arangodb.cloud:8529\").db(username=\"root\", password=\"cUZ0YaNdcwfUTw6VjRny\", verify=True)\n",
    "\n",
    "print(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T06:37:00.648492Z",
     "iopub.status.busy": "2025-03-04T06:37:00.648116Z",
     "iopub.status.idle": "2025-03-04T06:37:00.795106Z",
     "shell.execute_reply": "2025-03-04T06:37:00.794025Z",
     "shell.execute_reply.started": "2025-03-04T06:37:00.648462Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[12:02:37 +0530] [INFO]: Graph 'FlaskRepv1' exists.\n",
      "[12:02:38 +0530] [INFO]: Default node type set to 'FlaskRepv1_node'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nx_arangodb.classes.graph.Graph'>\n"
     ]
    }
   ],
   "source": [
    "G_adb = nxadb.Graph(\n",
    "    name=\"FlaskRepv1\",\n",
    "    db=db,\n",
    "    #incoming_graph_data=G,\n",
    "    #write_batch_size=50000 # feel free to modify\n",
    ")\n",
    "\n",
    "print(type(G_adb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T06:37:03.596462Z",
     "iopub.status.busy": "2025-03-04T06:37:03.596069Z",
     "iopub.status.idle": "2025-03-04T06:37:03.725552Z",
     "shell.execute_reply": "2025-03-04T06:37:03.724584Z",
     "shell.execute_reply.started": "2025-03-04T06:37:03.596432Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "arango_graph = ArangoGraph(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Graph Schema': [{'graph_name': 'CodebaseGraph', 'edge_definitions': [{'edge_collection': 'CodebaseGraph_node_to_CodebaseGraph_node', 'from_vertex_collections': ['CodebaseGraph_node'], 'to_vertex_collections': ['CodebaseGraph_node']}]}, {'graph_name': 'FlaskRepv1_node_to_FlaskRespv1_node', 'edge_definitions': [{'edge_collection': 'FlaskRepv1_node_to_FlaskRespv1_node_node_to_FlaskRepv1_node_to_FlaskRespv1_node_node', 'from_vertex_collections': ['FlaskRepv1_node_to_FlaskRespv1_node_node'], 'to_vertex_collections': ['FlaskRepv1_node_to_FlaskRespv1_node_node']}]}, {'graph_name': 'FlaskRepv1_node', 'edge_definitions': [{'edge_collection': 'FlaskRepv1_node_node_to_FlaskRepv1_node_node', 'from_vertex_collections': ['FlaskRepv1_node_node'], 'to_vertex_collections': ['FlaskRepv1_node_node']}]}, {'graph_name': 'code_graph', 'edge_definitions': [{'edge_collection': 'code_edges', 'from_vertex_collections': ['code_nodes'], 'to_vertex_collections': ['code_nodes']}]}, {'graph_name': 'FlaskRespv1', 'edge_definitions': [{'edge_collection': 'FlaskRespv1_node_to_FlaskRespv1_node', 'from_vertex_collections': ['FlaskRespv1_node'], 'to_vertex_collections': ['FlaskRespv1_node']}]}, {'graph_name': 'FlaskRepv2', 'edge_definitions': [{'edge_collection': 'FlaskRepv2_node_to_FlaskRepv2_node', 'from_vertex_collections': ['FlaskRepv2_node'], 'to_vertex_collections': ['FlaskRepv2_node']}]}, {'graph_name': 'Flaskv2', 'edge_definitions': [{'edge_collection': 'Flaskv2_node_to_Flaskv2_node', 'from_vertex_collections': ['Flaskv2_node'], 'to_vertex_collections': ['Flaskv2_node']}]}, {'graph_name': 'FlaskRepv1', 'edge_definitions': [{'edge_collection': 'FlaskRepv1_node_to_FlaskRepv1_node', 'from_vertex_collections': ['FlaskRepv1_node'], 'to_vertex_collections': ['FlaskRepv1_node']}]}], 'Collection Schema': [{'collection_name': 'Flaskv2_node', 'collection_type': 'document', 'document_properties': [{'name': '_key', 'type': 'str'}, {'name': '_id', 'type': 'str'}, {'name': '_rev', 'type': 'str'}, {'name': 'ast_type', 'type': 'str'}, {'name': 'file_path', 'type': 'str'}, {'name': 'rel_path', 'type': 'str'}, {'name': 'module_name', 'type': 'str'}, {'name': 'dir_depth', 'type': 'int'}, {'name': 'imported_by_count', 'type': 'int'}, {'name': 'name', 'type': 'str'}], 'example_document': {'_key': '0', '_id': 'Flaskv2_node/0', '_rev': '_jVNoMc2---', 'ast_type': 'File', 'file_path': 'flask/src/flask/testing.py', 'rel_path': '../src/flask/testing.py', 'module_name': '...src.flask.testing', 'dir_depth': 3, 'imported_by_count': 0, 'name': 'testing.py'}}, {'collection_name': 'Flaskv2_node_to_Flaskv2_node', 'collection_type': 'edge', 'edge_properties': [{'name': '_key', 'type': 'str'}, {'name': '_id', 'type': 'str'}, {'name': '_from', 'type': 'str'}, {'name': '_to', 'type': 'str'}, {'name': '_rev', 'type': 'str'}, {'name': 'relation', 'type': 'str'}], 'example_edge': {'_key': '0', '_id': 'Flaskv2_node_to_Flaskv2_node/0', '_from': 'Flaskv2_node/0', '_to': 'Flaskv2_node/1', '_rev': '_jVNoNBi--h', 'relation': 'contains'}}, {'collection_name': 'FlaskRepv1_node_to_FlaskRepv1_node', 'collection_type': 'edge', 'edge_properties': [{'name': '_key', 'type': 'str'}, {'name': '_id', 'type': 'str'}, {'name': '_from', 'type': 'str'}, {'name': '_to', 'type': 'str'}, {'name': '_rev', 'type': 'str'}, {'name': 'edge_type', 'type': 'str'}, {'name': 'start_line', 'type': 'int'}, {'name': 'end_line', 'type': 'int'}], 'example_edge': {'_key': '0', '_id': 'FlaskRepv1_node_to_FlaskRepv1_node/0', '_from': 'FlaskRepv1_node/0', '_to': 'FlaskRepv1_node/1', '_rev': '_jVPygAO---', 'edge_type': 'contains_snippet', 'start_line': 1, 'end_line': 20}}, {'collection_name': 'FlaskRepv1_node', 'collection_type': 'document', 'document_properties': [{'name': '_key', 'type': 'str'}, {'name': '_id', 'type': 'str'}, {'name': '_rev', 'type': 'str'}, {'name': 'type', 'type': 'str'}, {'name': 'file_index', 'type': 'int'}, {'name': 'directory', 'type': 'str'}], 'example_document': {'_key': '0', '_id': 'FlaskRepv1_node/0', '_rev': '_jVPyJKi---', 'type': 'file', 'file_index': 1, 'directory': 'tests'}}]}\n"
     ]
    }
   ],
   "source": [
    "print( arango_graph.schema )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvis.network import Network\n",
    "\n",
    "def visualize_codebase_graph(codebase_path, output_html=\"codebase_graph.html\", limit_nodes=None):\n",
    "    \"\"\"\n",
    "    Visualize the codebase graph using pyvis.\n",
    "    \n",
    "    Args:\n",
    "        codebase_path: Path to the codebase directory\n",
    "        output_html: Output HTML file for the visualization/\n",
    "        limit_nodes: Optional limit on the number of nodes to display (for large codebases)\n",
    "    \"\"\"\n",
    "    # Initialize and build the graph\n",
    "    visualizer = CodebaseVisualizer(codebase_path)\n",
    "    visualizer.parse_files()\n",
    "    graph = visualizer.build_graph()\n",
    "    \n",
    "    # Export graph to JSON (optional)\n",
    "    visualizer.export_graph_json('codebase_graph.json')\n",
    "    \n",
    "    # Create a pyvis network\n",
    "    net = Network(height=\"900px\", width=\"100%\", bgcolor=\"#222222\", font_color=\"white\")\n",
    "    \n",
    "    # Configure physics\n",
    "    net.barnes_hut(gravity=-5000, central_gravity=0.3, spring_length=200)\n",
    "    \n",
    "    # Define node groups and colors\n",
    "    node_colors = {\n",
    "        'file': '#4287f5',\n",
    "        'directory': '#42f5a7',\n",
    "        'symbol': '#f542cb',\n",
    "        'snippet': '#f5a742'\n",
    "    }\n",
    "    \n",
    "    # If we need to limit nodes for performance\n",
    "    if limit_nodes and len(graph.nodes()) > limit_nodes:\n",
    "        # Focus on file and directory nodes, and limit symbol nodes\n",
    "        important_nodes = [node for node, data in graph.nodes(data=True) \n",
    "                          if data.get('type') in ['file', 'directory']]\n",
    "        \n",
    "        # Add some symbol nodes to reach the limit\n",
    "        symbols = [node for node, data in graph.nodes(data=True) \n",
    "                  if data.get('type') == 'symbol']\n",
    "        \n",
    "        # Take a subset of symbols based on connectivity\n",
    "        symbol_importance = sorted(\n",
    "            [(node, graph.degree(node)) for node in symbols],\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        important_symbols = [node for node, _ in symbol_importance[:limit_nodes - len(important_nodes)]]\n",
    "        selected_nodes = important_nodes + important_symbols\n",
    "        \n",
    "        # Create a subgraph\n",
    "        graph = graph.subgraph(selected_nodes)\n",
    "    \n",
    "    # Add nodes with appropriate styles\n",
    "    for node, node_data in graph.nodes(data=True):\n",
    "        node_type = node_data.get('type', 'unknown')\n",
    "        label = os.path.basename(node) if '/' in node else node\n",
    "        \n",
    "        # Truncate very long labels\n",
    "        if len(label) > 30:\n",
    "            label = label[:27] + \"...\"\n",
    "        \n",
    "        # Create hover title with more details\n",
    "        title = f\"<div style='max-width:300px;'>\"\n",
    "        title += f\"<b>{node}</b><br>\"\n",
    "        title += f\"Type: {node_type}<br>\"\n",
    "        \n",
    "        if node_type == 'file':\n",
    "            title += f\"Directory: {node_data.get('directory', 'N/A')}<br>\"\n",
    "            \n",
    "        elif node_type == 'symbol':\n",
    "            title += f\"Symbol type: {node_data.get('symbol_type', 'N/A')}<br>\"\n",
    "            title += f\"Line: {node_data.get('line_number', 'N/A')}<br>\"\n",
    "            \n",
    "            if node_data.get('docstring'):\n",
    "                docstring = node_data['docstring']\n",
    "                if len(docstring) > 200:\n",
    "                    docstring = docstring[:197] + \"...\"\n",
    "                title += f\"Docstring: {docstring}<br>\"\n",
    "                \n",
    "        elif node_type == 'snippet':\n",
    "            title += f\"Lines: {node_data.get('start_line', 'N/A')}-{node_data.get('end_line', 'N/A')}<br>\"\n",
    "            \n",
    "            if node_data.get('code_snippet'):\n",
    "                snippet = node_data['code_snippet'].replace('\\n', '<br>')\n",
    "                if len(snippet) > 300:\n",
    "                    snippet = snippet[:297] + \"...\"\n",
    "                title += f\"Code:<br><pre>{snippet}</pre>\"\n",
    "                \n",
    "        title += \"</div>\"\n",
    "        \n",
    "        # Determine node size based on type and connections\n",
    "        size = 15  # Default size\n",
    "        if node_type == 'directory':\n",
    "            size = 25\n",
    "        elif node_type == 'file':\n",
    "            size = 20\n",
    "        elif node_type == 'symbol' and node_data.get('symbol_type') == 'class':\n",
    "            size = 18\n",
    "        \n",
    "        # Add node with appropriate styling\n",
    "        net.add_node(\n",
    "            node, \n",
    "            label=label, \n",
    "            title=title,\n",
    "            color=node_colors.get(node_type, '#999999'),\n",
    "            size=size,\n",
    "            shape='dot' if node_type != 'directory' else 'diamond'\n",
    "        )\n",
    "    \n",
    "    # Add edges with appropriate styles\n",
    "    for source, target, edge_data in graph.edges(data=True):\n",
    "        edge_type = edge_data.get('edge_type', 'unknown')\n",
    "        \n",
    "        # Style edges differently based on type\n",
    "        if edge_type == 'import':\n",
    "            color = '#f5f542'\n",
    "            width = 2\n",
    "            dashes = False\n",
    "        elif edge_type == 'references':\n",
    "            color = '#f54242'\n",
    "            width = 1\n",
    "            dashes = [5, 5]\n",
    "        elif edge_type == 'defines':\n",
    "            color = '#42f55a'\n",
    "            width = 3\n",
    "            dashes = False\n",
    "        elif edge_type == 'contains_snippet':\n",
    "            color = '#42c8f5'\n",
    "            width = 1\n",
    "            dashes = [2, 2]\n",
    "        else:\n",
    "            color = '#999999'\n",
    "            width = 1\n",
    "            dashes = False\n",
    "        \n",
    "        # Create hover title with edge details\n",
    "        title = f\"<div><b>{edge_type}</b><br>\"\n",
    "        \n",
    "        if edge_data.get('line_number'):\n",
    "            title += f\"Line: {edge_data['line_number']}<br>\"\n",
    "            \n",
    "        if edge_data.get('context'):\n",
    "            context = edge_data['context']\n",
    "            if len(context) > 200:\n",
    "                context = context[:197] + \"...\"\n",
    "            title += f\"Context: {context}\"\n",
    "            \n",
    "        title += \"</div>\"\n",
    "        \n",
    "        # Add edge with styling\n",
    "        net.add_edge(\n",
    "            source, \n",
    "            target, \n",
    "            title=title,\n",
    "            color=color,\n",
    "            width=width,\n",
    "            dashes=dashes\n",
    "        )\n",
    "    \n",
    "    # Enable physics, navigation and interaction options\n",
    "    net.toggle_physics(True)\n",
    "    net.show_buttons(filter_=['physics'])\n",
    "    \n",
    "    # Save the visualization\n",
    "    net.save_graph(output_html)\n",
    "    print(f\"Graph visualization saved to {output_html}\")\n",
    "    \n",
    "    return output_html\n",
    "\n",
    "def visualize_symbol_subgraph(visualizer, symbol_name, output_html=\"symbol_graph.html\"):\n",
    "    \"\"\"\n",
    "    Create a focused visualization of a symbol and its relationships.\n",
    "    \n",
    "    Args:\n",
    "        visualizer: Initialized CodebaseVisualizer instance\n",
    "        symbol_name: Name of the symbol to visualize\n",
    "        output_html: Output HTML file for the visualization\n",
    "    \"\"\"\n",
    "    # Get the full graph\n",
    "    full_graph = visualizer.graph\n",
    "    \n",
    "    # Find all nodes related to this symbol\n",
    "    symbol_nodes = [node for node in full_graph.nodes() if f\"::{symbol_name}\" in node]\n",
    "    \n",
    "    if not symbol_nodes:\n",
    "        print(f\"Symbol '{symbol_name}' not found in the graph.\")\n",
    "        return None\n",
    "    \n",
    "    # Get nodes that are connected to symbol nodes (1-hop neighborhood)\n",
    "    related_nodes = set(symbol_nodes)\n",
    "    for node in symbol_nodes:\n",
    "        # Add predecessors (nodes that reference this symbol)\n",
    "        related_nodes.update(full_graph.predecessors(node))\n",
    "        # Add successors (nodes that this symbol references)\n",
    "        related_nodes.update(full_graph.successors(node))\n",
    "    \n",
    "    # Create a subgraph\n",
    "    subgraph = full_graph.subgraph(related_nodes)\n",
    "    \n",
    "    # Create a pyvis network\n",
    "    net = Network(height=\"750px\", width=\"100%\", bgcolor=\"#222222\", font_color=\"white\")\n",
    "    net.barnes_hut(gravity=-2000, central_gravity=0.3, spring_length=150)\n",
    "    \n",
    "    # Node colors by type\n",
    "    node_colors = {\n",
    "        'file': '#4287f5',\n",
    "        'directory': '#42f5a7',\n",
    "        'symbol': '#f542cb',\n",
    "        'snippet': '#f5a742'\n",
    "    }\n",
    "    \n",
    "    # Add nodes with appropriate styles\n",
    "    for node, node_data in subgraph.nodes(data=True):\n",
    "        node_type = node_data.get('type', 'unknown')\n",
    "        label = os.path.basename(node) if '/' in node else node\n",
    "        \n",
    "        # Make the focus symbol nodes larger and highlighted\n",
    "        if node in symbol_nodes:\n",
    "            size = 30\n",
    "            color = '#ff0000'  # Bright red for focus\n",
    "        else:\n",
    "            size = 15\n",
    "            color = node_colors.get(node_type, '#999999')\n",
    "        \n",
    "        # Add hover information\n",
    "        title = f\"<div style='max-width:300px;'>\"\n",
    "        title += f\"<b>{node}</b><br>\"\n",
    "        title += f\"Type: {node_type}<br>\"\n",
    "        \n",
    "        if node_type == 'symbol':\n",
    "            title += f\"Symbol type: {node_data.get('symbol_type', 'N/A')}<br>\"\n",
    "            if node_data.get('docstring'):\n",
    "                title += f\"Docstring: {node_data.get('docstring', '')}<br>\"\n",
    "                \n",
    "        title += \"</div>\"\n",
    "        \n",
    "        # Add node with styling\n",
    "        net.add_node(\n",
    "            node,\n",
    "            label=label,\n",
    "            title=title,\n",
    "            color=color,\n",
    "            size=size\n",
    "        )\n",
    "    \n",
    "    # Add edges with styles\n",
    "    for source, target, edge_data in subgraph.edges(data=True):\n",
    "        edge_type = edge_data.get('edge_type', 'unknown')\n",
    "        \n",
    "        # Style edges by type\n",
    "        if edge_type == 'import':\n",
    "            color = '#f5f542'  # Yellow\n",
    "        elif edge_type == 'references':\n",
    "            color = '#f54242'  # Red\n",
    "        elif edge_type == 'defines':\n",
    "            color = '#42f55a'  # Green\n",
    "        else:\n",
    "            color = '#999999'  # Gray\n",
    "        \n",
    "        # Add edge with details in hover\n",
    "        title = f\"{edge_type}\"\n",
    "        if edge_data.get('line_number'):\n",
    "            title += f\" (line {edge_data['line_number']})\"\n",
    "            \n",
    "        net.add_edge(source, target, title=title, color=color)\n",
    "    \n",
    "    # Enable physics and navigation\n",
    "    net.toggle_physics(True)\n",
    "    net.show_buttons(filter_=['physics'])\n",
    "    \n",
    "    # Save the visualization\n",
    "    net.save_graph(output_html)\n",
    "    print(f\"Symbol graph visualization saved to {output_html}\")\n",
    "    \n",
    "    return output_html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvis.network import Network\n",
    "\n",
    "def visualize_codebase_graph(codebase_path, output_html=\"codebase_graph.html\", limit_nodes=None):\n",
    "    \"\"\"\n",
    "    Visualize the codebase graph using pyvis.\n",
    "    \n",
    "    Args:\n",
    "        codebase_path: Path to the codebase directory\n",
    "        output_html: Output HTML file for the visualization/\n",
    "        limit_nodes: Optional limit on the number of nodes to display (for large codebases)\n",
    "    \"\"\"\n",
    "    # Initialize and build the graph\n",
    "    visualizer = CodebaseVisualizer(codebase_path)\n",
    "    visualizer.parse_files()\n",
    "    graph = visualizer.build_graph()\n",
    "    \n",
    "    # Export graph to JSON (optional)\n",
    "    visualizer.export_graph_json('codebase_graph.json')\n",
    "    \n",
    "    # Create a pyvis network\n",
    "    net = Network(height=\"900px\", width=\"100%\", bgcolor=\"#222222\", font_color=\"white\")\n",
    "    \n",
    "    # Configure physics\n",
    "    net.barnes_hut(gravity=-5000, central_gravity=0.3, spring_length=200)\n",
    "    \n",
    "    # Define node groups and colors\n",
    "    node_colors = {\n",
    "        'file': '#4287f5',\n",
    "        'directory': '#42f5a7',\n",
    "        'symbol': '#f542cb',\n",
    "        'snippet': '#f5a742'\n",
    "    }\n",
    "    \n",
    "    # If we need to limit nodes for performance\n",
    "    if limit_nodes and len(graph.nodes()) > limit_nodes:\n",
    "        # Focus on file and directory nodes, and limit symbol nodes\n",
    "        important_nodes = [node for node, data in graph.nodes(data=True) \n",
    "                          if data.get('type') in ['file', 'directory']]\n",
    "        \n",
    "        # Add some symbol nodes to reach the limit\n",
    "        symbols = [node for node, data in graph.nodes(data=True) \n",
    "                  if data.get('type') == 'symbol']\n",
    "        \n",
    "        # Take a subset of symbols based on connectivity\n",
    "        symbol_importance = sorted(\n",
    "            [(node, graph.degree(node)) for node in symbols],\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        important_symbols = [node for node, _ in symbol_importance[:limit_nodes - len(important_nodes)]]\n",
    "        selected_nodes = important_nodes + important_symbols\n",
    "        \n",
    "        # Create a subgraph\n",
    "        graph = graph.subgraph(selected_nodes)\n",
    "    \n",
    "    # Add nodes with appropriate styles\n",
    "    for node, node_data in graph.nodes(data=True):\n",
    "        node_type = node_data.get('type', 'unknown')\n",
    "        label = os.path.basename(node) if '/' in node else node\n",
    "        \n",
    "        # Truncate very long labels\n",
    "        if len(label) > 30:\n",
    "            label = label[:27] + \"...\"\n",
    "        \n",
    "        # Create hover title with more details\n",
    "        title = f\"<div style='max-width:300px;'>\"\n",
    "        title += f\"<b>{node}</b><br>\"\n",
    "        title += f\"Type: {node_type}<br>\"\n",
    "        \n",
    "        if node_type == 'file':\n",
    "            title += f\"Directory: {node_data.get('directory', 'N/A')}<br>\"\n",
    "            \n",
    "        elif node_type == 'symbol':\n",
    "            title += f\"Symbol type: {node_data.get('symbol_type', 'N/A')}<br>\"\n",
    "            title += f\"Line: {node_data.get('line_number', 'N/A')}<br>\"\n",
    "            \n",
    "            if node_data.get('docstring'):\n",
    "                docstring = node_data['docstring']\n",
    "                if len(docstring) > 200:\n",
    "                    docstring = docstring[:197] + \"...\"\n",
    "                title += f\"Docstring: {docstring}<br>\"\n",
    "                \n",
    "        elif node_type == 'snippet':\n",
    "            title += f\"Lines: {node_data.get('start_line', 'N/A')}-{node_data.get('end_line', 'N/A')}<br>\"\n",
    "            \n",
    "            if node_data.get('code_snippet'):\n",
    "                snippet = node_data['code_snippet'].replace('\\n', '<br>')\n",
    "                if len(snippet) > 300:\n",
    "                    snippet = snippet[:297] + \"...\"\n",
    "                title += f\"Code:<br><pre>{snippet}</pre>\"\n",
    "                \n",
    "        title += \"</div>\"\n",
    "        \n",
    "        # Determine node size based on type and connections\n",
    "        size = 15  # Default size\n",
    "        if node_type == 'directory':\n",
    "            size = 25\n",
    "        elif node_type == 'file':\n",
    "            size = 20\n",
    "        elif node_type == 'symbol' and node_data.get('symbol_type') == 'class':\n",
    "            size = 18\n",
    "        \n",
    "        # Add node with appropriate styling\n",
    "        net.add_node(\n",
    "            node, \n",
    "            label=label, \n",
    "            title=title,\n",
    "            color=node_colors.get(node_type, '#999999'),\n",
    "            size=size,\n",
    "            shape='dot' if node_type != 'directory' else 'diamond'\n",
    "        )\n",
    "    \n",
    "    # Add edges with appropriate styles\n",
    "    for source, target, edge_data in graph.edges(data=True):\n",
    "        edge_type = edge_data.get('edge_type', 'unknown')\n",
    "        \n",
    "        # Style edges differently based on type\n",
    "        if edge_type == 'import':\n",
    "            color = '#f5f542'\n",
    "            width = 2\n",
    "            dashes = False\n",
    "        elif edge_type == 'references':\n",
    "            color = '#f54242'\n",
    "            width = 1\n",
    "            dashes = [5, 5]\n",
    "        elif edge_type == 'defines':\n",
    "            color = '#42f55a'\n",
    "            width = 3\n",
    "            dashes = False\n",
    "        elif edge_type == 'contains_snippet':\n",
    "            color = '#42c8f5'\n",
    "            width = 1\n",
    "            dashes = [2, 2]\n",
    "        else:\n",
    "            color = '#999999'\n",
    "            width = 1\n",
    "            dashes = False\n",
    "        \n",
    "        # Create hover title with edge details\n",
    "        title = f\"<div><b>{edge_type}</b><br>\"\n",
    "        \n",
    "        if edge_data.get('line_number'):\n",
    "            title += f\"Line: {edge_data['line_number']}<br>\"\n",
    "            \n",
    "        if edge_data.get('context'):\n",
    "            context = edge_data['context']\n",
    "            if len(context) > 200:\n",
    "                context = context[:197] + \"...\"\n",
    "            title += f\"Context: {context}\"\n",
    "            \n",
    "        title += \"</div>\"\n",
    "        \n",
    "        # Add edge with styling\n",
    "        net.add_edge(\n",
    "            source, \n",
    "            target, \n",
    "            title=title,\n",
    "            color=color,\n",
    "            width=width,\n",
    "            dashes=dashes\n",
    "        )\n",
    "    \n",
    "    # Enable physics, navigation and interaction options\n",
    "    net.toggle_physics(True)\n",
    "    net.show_buttons(filter_=['physics'])\n",
    "    \n",
    "    # Save the visualization\n",
    "    net.save_graph(output_html)\n",
    "    print(f\"Graph visualization saved to {output_html}\")\n",
    "    \n",
    "    return output_html\n",
    "\n",
    "def visualize_symbol_subgraph(visualizer, symbol_name, output_html=\"symbol_graph.html\"):\n",
    "    \"\"\"\n",
    "    Create a focused visualization of a symbol and its relationships.\n",
    "    \n",
    "    Args:\n",
    "        visualizer: Initialized CodebaseVisualizer instance\n",
    "        symbol_name: Name of the symbol to visualize\n",
    "        output_html: Output HTML file for the visualization\n",
    "    \"\"\"\n",
    "    # Get the full graph\n",
    "    full_graph = visualizer.graph\n",
    "    \n",
    "    # Find all nodes related to this symbol\n",
    "    symbol_nodes = [node for node in full_graph.nodes() if f\"::{symbol_name}\" in node]\n",
    "    \n",
    "    if not symbol_nodes:\n",
    "        print(f\"Symbol '{symbol_name}' not found in the graph.\")\n",
    "        return None\n",
    "    \n",
    "    # Get nodes that are connected to symbol nodes (1-hop neighborhood)\n",
    "    related_nodes = set(symbol_nodes)\n",
    "    for node in symbol_nodes:\n",
    "        # Add predecessors (nodes that reference this symbol)\n",
    "        related_nodes.update(full_graph.predecessors(node))\n",
    "        # Add successors (nodes that this symbol references)\n",
    "        related_nodes.update(full_graph.successors(node))\n",
    "    \n",
    "    # Create a subgraph\n",
    "    subgraph = full_graph.subgraph(related_nodes)\n",
    "    \n",
    "    # Create a pyvis network\n",
    "    net = Network(height=\"750px\", width=\"100%\", bgcolor=\"#222222\", font_color=\"white\")\n",
    "    net.barnes_hut(gravity=-2000, central_gravity=0.3, spring_length=150)\n",
    "    \n",
    "    # Node colors by type\n",
    "    node_colors = {\n",
    "        'file': '#4287f5',\n",
    "        'directory': '#42f5a7',\n",
    "        'symbol': '#f542cb',\n",
    "        'snippet': '#f5a742'\n",
    "    }\n",
    "    \n",
    "    # Add nodes with appropriate styles\n",
    "    for node, node_data in subgraph.nodes(data=True):\n",
    "        node_type = node_data.get('type', 'unknown')\n",
    "        label = os.path.basename(node) if '/' in node else node\n",
    "        \n",
    "        # Make the focus symbol nodes larger and highlighted\n",
    "        if node in symbol_nodes:\n",
    "            size = 30\n",
    "            color = '#ff0000'  # Bright red for focus\n",
    "        else:\n",
    "            size = 15\n",
    "            color = node_colors.get(node_type, '#999999')\n",
    "        \n",
    "        # Add hover information\n",
    "        title = f\"<div style='max-width:300px;'>\"\n",
    "        title += f\"<b>{node}</b><br>\"\n",
    "        title += f\"Type: {node_type}<br>\"\n",
    "        \n",
    "        if node_type == 'symbol':\n",
    "            title += f\"Symbol type: {node_data.get('symbol_type', 'N/A')}<br>\"\n",
    "            if node_data.get('docstring'):\n",
    "                title += f\"Docstring: {node_data.get('docstring', '')}<br>\"\n",
    "                \n",
    "        title += \"</div>\"\n",
    "        \n",
    "        # Add node with styling\n",
    "        net.add_node(\n",
    "            node,\n",
    "            label=label,\n",
    "            title=title,\n",
    "            color=color,\n",
    "            size=size\n",
    "        )\n",
    "    \n",
    "    # Add edges with styles\n",
    "    for source, target, edge_data in subgraph.edges(data=True):\n",
    "        edge_type = edge_data.get('edge_type', 'unknown')\n",
    "        \n",
    "        # Style edges by type\n",
    "        if edge_type == 'import':\n",
    "            color = '#f5f542'  # Yellow\n",
    "        elif edge_type == 'references':\n",
    "            color = '#f54242'  # Red\n",
    "        elif edge_type == 'defines':\n",
    "            color = '#42f55a'  # Green\n",
    "        else:\n",
    "            color = '#999999'  # Gray\n",
    "        \n",
    "        # Add edge with details in hover\n",
    "        title = f\"{edge_type}\"\n",
    "        if edge_data.get('line_number'):\n",
    "            title += f\" (line {edge_data['line_number']})\"\n",
    "            \n",
    "        net.add_edge(source, target, title=title, color=color)\n",
    "    \n",
    "    # Enable physics and navigation\n",
    "    net.toggle_physics(True)\n",
    "    net.show_buttons(filter_=['physics'])\n",
    "    \n",
    "    # Save the visualization\n",
    "    net.save_graph(output_html)\n",
    "    print(f\"Symbol graph visualization saved to {output_html}\")\n",
    "    \n",
    "    return output_html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T06:37:04.449981Z",
     "iopub.status.busy": "2025-03-04T06:37:04.449636Z",
     "iopub.status.idle": "2025-03-04T06:37:04.454585Z",
     "shell.execute_reply": "2025-03-04T06:37:04.453434Z",
     "shell.execute_reply.started": "2025-03-04T06:37:04.449949Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"MISTRAL_API_KEY\"]=\"jJAuJZkjVcy2ynUhan375sHNviHiBeJU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T06:37:06.357854Z",
     "iopub.status.busy": "2025-03-04T06:37:06.357510Z",
     "iopub.status.idle": "2025-03-04T06:37:07.616190Z",
     "shell.execute_reply": "2025-03-04T06:37:07.615006Z",
     "shell.execute_reply.started": "2025-03-04T06:37:06.357826Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={}, response_metadata={'token_usage': {'prompt_tokens': 4, 'total_tokens': 13, 'completion_tokens': 9}, 'model': 'mistral-large-latest', 'finish_reason': 'stop'}, id='run-887dd047-0991-4203-8f39-9aeae0814f00-0', usage_metadata={'input_tokens': 4, 'output_tokens': 9, 'total_tokens': 13})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_mistralai import ChatMistralAI\n",
    "\n",
    "llm = ChatMistralAI(\n",
    "    model=\"mistral-large-latest\",\n",
    "    temperature=0,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "llm.invoke(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T06:37:09.976501Z",
     "iopub.status.busy": "2025-03-04T06:37:09.976151Z",
     "iopub.status.idle": "2025-03-04T06:37:09.981764Z",
     "shell.execute_reply": "2025-03-04T06:37:09.980571Z",
     "shell.execute_reply.started": "2025-03-04T06:37:09.976473Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# @tool\n",
    "# def text_to_nx_algorithm_to_text(query):\n",
    "#     \"\"\"This tool is available to invoke a NetworkX Algorithm on\n",
    "#     the ArangoDB Graph. You are responsible for accepting the\n",
    "#     Natural Language Query, establishing which algorithm needs to\n",
    "#     be executed, executing the algorithm, and translating the results back\n",
    "#     to Natural Language, with respect to the original query.\n",
    "\n",
    "#     If the query (e.g traversals, shortest path, etc.) can be solved using the Arango Query Language, then do not use\n",
    "#     this tool.\n",
    "#     \"\"\"\n",
    "#     llm = ChatMistralAI(\n",
    "#         model=\"mistral-large-latest\",\n",
    "#         temperature=0,\n",
    "#         max_retries=2,\n",
    "#         # other params...\n",
    "#     )\n",
    "#     ######################\n",
    "#     print(\"1) Generating NetworkX code\")\n",
    "\n",
    "#     text_to_nx = llm.invoke(f\"\"\"\n",
    "#     I have a NetworkX Graph called `G_adb`. It has the following schema: {arango_graph.schema}\n",
    "\n",
    "#     I have the following graph analysis query: {query}.\n",
    "\n",
    "#     Generate the Python Code required to answer the query using the `G_adb` object.\n",
    "    \n",
    "#     It should give code so that is gives ALL the necessary contents that use this part of the code in terms of ALL nested imports. Consider the nested directories and sub-directory informations as well.\n",
    "\n",
    "#     Be very precise on the NetworkX algorithm you select to answer this query. Think step by step.\n",
    "\n",
    "#     Only assume that networkx is installed, and other base python dependencies.\n",
    "\n",
    "#     Always set the last variable as `FINAL_RESULT`, which represents the answer to the original query.\n",
    "\n",
    "#     Only provide python code that I can directly execute via `exec()`. Do not provide any instructions.\n",
    "\n",
    "#     Make sure that `FINAL_RESULT` stores a short & consice answer. Avoid setting this variable to a long sequence.\n",
    "\n",
    "#     Your code:\n",
    "#     \"\"\").content\n",
    "\n",
    "#     text_to_nx_cleaned = re.sub(r\"^```python\\n|```$\", \"\", text_to_nx, flags=re.MULTILINE).strip()\n",
    "    \n",
    "#     print('-'*10)\n",
    "#     print(text_to_nx_cleaned)\n",
    "#     print('-'*10)\n",
    "\n",
    "#     ######################\n",
    "\n",
    "#     print(\"\\n2) Executing NetworkX code\")\n",
    "#     global_vars = {\"G_adb\": G_adb, \"nx\": nx}\n",
    "#     local_vars = {}\n",
    "\n",
    "#     try:\n",
    "#         exec(text_to_nx_cleaned, global_vars, local_vars)\n",
    "#         text_to_nx_final = text_to_nx\n",
    "#     except Exception as e:\n",
    "#         print(f\"EXEC ERROR: {e}\")\n",
    "#         return f\"EXEC ERROR: {e}\"\n",
    "\n",
    "#         # TODO: Consider experimenting with a code corrector!\n",
    "#         attempt = 1\n",
    "#         MAX_ATTEMPTS = 3\n",
    "\n",
    "#         # while attempt <= MAX_ATTEMPTS\n",
    "#             # ...\n",
    "\n",
    "#     print('-'*10)\n",
    "#     FINAL_RESULT = local_vars[\"FINAL_RESULT\"]\n",
    "#     print(f\"FINAL_RESULT: {FINAL_RESULT}\")\n",
    "#     print('-'*10)\n",
    "\n",
    "#     ######################\n",
    "\n",
    "#     print(\"3) Formulating final answer\")\n",
    "\n",
    "#     nx_to_text = llm.invoke(f\"\"\"\n",
    "#         I have a NetworkX Graph called `G_adb`. It has the following schema: {arango_graph.schema}\n",
    "\n",
    "#         I have the following graph analysis query: {query}.\n",
    "\n",
    "#         I have executed the following python code to help me answer my query:\n",
    "\n",
    "#         ---\n",
    "#         {text_to_nx_final}\n",
    "#         ---\n",
    "\n",
    "#         The `FINAL_RESULT` variable is set to the following: {FINAL_RESULT}.\n",
    "\n",
    "#         Based on my original Query and FINAL_RESULT, generate a short and concise response to\n",
    "#         answer my query.\n",
    "        \n",
    "#         Your response:\n",
    "#     \"\"\").content\n",
    "\n",
    "#     return nx_to_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing the imported Graph via the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T06:36:13.720607Z",
     "iopub.status.busy": "2025-03-04T06:36:13.720223Z",
     "iopub.status.idle": "2025-03-04T06:36:13.742476Z",
     "shell.execute_reply": "2025-03-04T06:36:13.741250Z",
     "shell.execute_reply.started": "2025-03-04T06:36:13.720574Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from arango import ArangoClient\n",
    "import re\n",
    "\n",
    "def analyze_networkx_graph(G, query_text):\n",
    "    \"\"\"\n",
    "    Process a natural language query directly against a NetworkX graph\n",
    "    \n",
    "    Args:\n",
    "        G: NetworkX graph object\n",
    "        query_text: Natural language query about the codebase\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with analysis results in a human-readable format\n",
    "    \"\"\"\n",
    "    # Clean the query to extract the actual symbol being sought\n",
    "    clean_query = query_text.lower().strip()\n",
    "    \n",
    "    # Common English words to filter out\n",
    "    common_words = {\"what\", \"is\", \"the\", \"use\", \"of\", \"in\", \"where\", \"how\", \"why\", \"when\", \n",
    "                    \"who\", \"which\", \"does\", \"do\", \"are\", \"can\", \"could\", \"would\", \"should\", \n",
    "                    \"function\", \"method\", \"class\", \"variable\", \"codebase\", \"code\", \"used\", \n",
    "                    \"defined\", \"implemented\", \"called\", \"referenced\"}\n",
    "    \n",
    "    # Find all words that could be symbols\n",
    "    potential_symbols = []\n",
    "    for word in clean_query.split():\n",
    "        word = word.strip(\".,?!()[]{}'\\\"\\n\\t\")\n",
    "        if len(word) > 2 and word.lower() not in common_words:\n",
    "            potential_symbols.append(word)\n",
    "    \n",
    "    # Also look for multi-word symbols with underscores\n",
    "    for i in range(len(clean_query.split()) - 1):\n",
    "        compound = '_'.join(clean_query.split()[i:i+2])\n",
    "        if '_' in compound and compound not in potential_symbols:\n",
    "            potential_symbols.append(compound)\n",
    "    \n",
    "    # Extract exact symbol if passed directly\n",
    "    if query_text.strip() and len(query_text.strip().split()) == 1 and '_' in query_text:\n",
    "        # User likely just passed the symbol name directly\n",
    "        potential_symbols = [query_text.strip()]\n",
    "    \n",
    "    # Find matches in the graph\n",
    "    candidates = []\n",
    "    \n",
    "    # First, look for exact node IDs or node names\n",
    "    for symbol in potential_symbols:\n",
    "        for node_id, node_data in G.nodes(data=True):\n",
    "            # Check if this is a symbol node\n",
    "            if node_data.get('type') == 'symbol':\n",
    "                # Check the node ID\n",
    "                if isinstance(node_id, str) and symbol.lower() in node_id.lower():\n",
    "                    candidates.append((node_id, symbol, 1.0))  # 1.0 = high confidence\n",
    "                \n",
    "                # Check if symbol appears in the context (code snippet)\n",
    "                context = node_data.get('context', '')\n",
    "                if context and symbol.lower() in context.lower():\n",
    "                    # Higher confidence if it appears as a variable assignment\n",
    "                    patterns = [\n",
    "                        f\"self.{symbol}\", \n",
    "                        f\"{symbol} =\", \n",
    "                        f\"def {symbol}\", \n",
    "                        f\"class {symbol}\"\n",
    "                    ]\n",
    "                    score = 0.8  # Base score\n",
    "                    for pattern in patterns:\n",
    "                        if pattern.lower() in context.lower():\n",
    "                            score = 0.9  # Higher confidence\n",
    "                            break\n",
    "                    candidates.append((node_id, symbol, score))\n",
    "    \n",
    "    # Second, check for symbol references in edge contexts\n",
    "    if not candidates:\n",
    "        for source, target, edge_data in G.edges(data=True):\n",
    "            edge_type = edge_data.get('edge_type')\n",
    "            if edge_type in ['references', 'defines']:\n",
    "                context = edge_data.get('context', '')\n",
    "                for symbol in potential_symbols:\n",
    "                    if context and symbol.lower() in context.lower():\n",
    "                        patterns = [\n",
    "                            f\"self.{symbol}\", \n",
    "                            f\"{symbol} =\", \n",
    "                            f\"def {symbol}\", \n",
    "                            f\"class {symbol}\"\n",
    "                        ]\n",
    "                        score = 0.7  # Base score for edges\n",
    "                        for pattern in patterns:\n",
    "                            if pattern.lower() in context.lower():\n",
    "                                score = 0.8  # Higher confidence\n",
    "                                break\n",
    "                        candidates.append((target, symbol, score))\n",
    "    \n",
    "    # Third, search in snippets\n",
    "    if not candidates:\n",
    "        for node_id, node_data in G.nodes(data=True):\n",
    "            if node_data.get('type') == 'snippet':\n",
    "                code_snippet = node_data.get('code_snippet', '')\n",
    "                for symbol in potential_symbols:\n",
    "                    if code_snippet and symbol.lower() in code_snippet.lower():\n",
    "                        patterns = [\n",
    "                            f\"self.{symbol}\", \n",
    "                            f\"{symbol} =\", \n",
    "                            f\"def {symbol}\", \n",
    "                            f\"class {symbol}\"\n",
    "                        ]\n",
    "                        score = 0.6  # Base score for snippets\n",
    "                        for pattern in patterns:\n",
    "                            if pattern.lower() in code_snippet.lower():\n",
    "                                score = 0.7  # Higher confidence\n",
    "                                break\n",
    "                        # Find the file this snippet belongs to\n",
    "                        file_nodes = []\n",
    "                        for source, target, edge_data in G.in_edges(node_id, data=True):\n",
    "                            if edge_data.get('edge_type') == 'contains_snippet':\n",
    "                                file_nodes.append(source)\n",
    "                        \n",
    "                        # Create a pseudo symbol node ID using the file\n",
    "                        if file_nodes:\n",
    "                            pseudo_id = f\"{file_nodes[0]}::{symbol}\"\n",
    "                            candidates.append((pseudo_id, symbol, score))\n",
    "                        else:\n",
    "                            candidates.append((node_id, symbol, score))\n",
    "    \n",
    "    # No matches found\n",
    "    if not candidates:\n",
    "        return {\"response\": f\"Could not find a matching symbol in the codebase for '{query_text}'. Please try rephrasing your query with a specific function, class, or variable name.\"}\n",
    "    \n",
    "    # Sort candidates by confidence score\n",
    "    candidates.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    # Choose the best candidate\n",
    "    node_id, symbol_name, _ = candidates[0]\n",
    "    \n",
    "    # Find all usages of the identified symbol\n",
    "    symbol_usages = find_symbol_usage_nx(G, node_id, symbol_name)\n",
    "    \n",
    "    # Generate response\n",
    "    if \"error\" in symbol_usages:\n",
    "        # Try a broader search if the specific node wasn't found\n",
    "        broader_usages = search_symbol_in_contexts(G, symbol_name)\n",
    "        if broader_usages:\n",
    "            return {\"response\": broader_usages}\n",
    "        return {\"response\": symbol_usages[\"error\"]}\n",
    "    \n",
    "    # Collect information about the symbol\n",
    "    definition_files = symbol_usages.get(\"defined_in\", [])\n",
    "    usage_info = symbol_usages.get(\"usages\", {})\n",
    "    symbol_type = symbol_usages.get(\"symbol_type\", \"variable\")  # Default to variable\n",
    "    \n",
    "    # Build human-readable response\n",
    "    response = f\"Symbol: '{symbol_name}' (Type: {symbol_type})\\n\\n\"\n",
    "    response += f\"Defined in: {', '.join(definition_files) if definition_files else 'No definition location found'}\\n\\n\"\n",
    "    \n",
    "    if symbol_usages.get(\"docstring\"):\n",
    "        response += f\"Documentation:\\n{symbol_usages['docstring']}\\n\\n\"\n",
    "    \n",
    "    response += \"Used in the following locations:\\n\"\n",
    "    \n",
    "    if not usage_info:\n",
    "        response += \"\\nNo usage information found for this symbol in graph nodes.\\n\"\n",
    "        \n",
    "        # Try a broader search in contexts\n",
    "        broader_usages = search_symbol_in_contexts(G, symbol_name)\n",
    "        if broader_usages:\n",
    "            response += \"\\nHowever, found these mentions in code snippets:\\n\\n\"\n",
    "            response += broader_usages\n",
    "    else:\n",
    "        for file, usages in usage_info.items():\n",
    "            response += f\"\\nFile: {file}\\n\"\n",
    "            for usage in usages:\n",
    "                line = usage.get('line', 'unknown line')\n",
    "                context = usage.get('context', 'No context available')\n",
    "                response += f\"- Line {line}: {context}\\n\"\n",
    "    \n",
    "    # Add related symbols if available\n",
    "    if symbol_usages.get(\"related_symbols\"):\n",
    "        response += \"\\nRelated symbols:\\n\"\n",
    "        for related in symbol_usages[\"related_symbols\"][:5]:  # Limit to top 5\n",
    "            response += f\"- {related}\\n\"\n",
    "    \n",
    "    # Add graph statistics\n",
    "    response += f\"\\nAnalysis performed on graph with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\"\n",
    "    \n",
    "    return {\"response\": response}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching for the symbol in both context, networkx format and file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T11:56:17.150825Z",
     "iopub.status.busy": "2025-03-01T11:56:17.150480Z",
     "iopub.status.idle": "2025-03-01T11:56:17.164834Z",
     "shell.execute_reply": "2025-03-01T11:56:17.163909Z",
     "shell.execute_reply.started": "2025-03-01T11:56:17.150798Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def search_symbol_in_contexts(G, symbol_name):\n",
    "    \"\"\"\n",
    "    Search for a symbol in all contexts (code snippets) in the graph\n",
    "    \n",
    "    Args:\n",
    "        G: NetworkX graph\n",
    "        symbol_name: Name of the symbol to search\n",
    "    \n",
    "    Returns:\n",
    "        String with usage information\n",
    "    \"\"\"\n",
    "    response = \"\"\n",
    "    found = False\n",
    "    \n",
    "    # Look in snippet nodes\n",
    "    for node_id, node_data in G.nodes(data=True):\n",
    "        if node_data.get('type') == 'snippet':\n",
    "            code_snippet = node_data.get('code_snippet', '')\n",
    "            if code_snippet and symbol_name.lower() in code_snippet.lower():\n",
    "                found = True\n",
    "                file_name = \"Unknown\"\n",
    "                # Find which file this snippet belongs to\n",
    "                for source, target, edge_data in G.in_edges(node_id, data=True):\n",
    "                    if edge_data.get('edge_type') == 'contains_snippet':\n",
    "                        file_name = source\n",
    "                        break\n",
    "                \n",
    "                start_line = node_data.get('start_line', 'unknown')\n",
    "                response += f\"\\nFile: {file_name} (Lines {start_line}-{node_data.get('end_line', 'unknown')})\\n\"\n",
    "                \n",
    "                # Extract the lines containing the symbol\n",
    "                lines = code_snippet.split('\\n')\n",
    "                for i, line in enumerate(lines):\n",
    "                    if symbol_name.lower() in line.lower():\n",
    "                        line_num = int(start_line) + i if isinstance(start_line, int) else \"?\"\n",
    "                        response += f\"- Line {line_num}: {line.strip()}\\n\"\n",
    "    \n",
    "    # Look in edge contexts\n",
    "    for source, target, edge_data in G.edges(data=True):\n",
    "        context = edge_data.get('context', '')\n",
    "        if context and symbol_name.lower() in context.lower():\n",
    "            found = True\n",
    "            edge_type = edge_data.get('edge_type', 'unknown')\n",
    "            line_num = edge_data.get('line_number', 'unknown')\n",
    "            \n",
    "            # For references or defines edges, source is usually the file\n",
    "            file_name = source if edge_type in ['references', 'defines'] else \"Unknown\"\n",
    "            \n",
    "            response += f\"\\nFile: {file_name} (Line {line_num})\\n\"\n",
    "            response += f\"- {context.strip()}\\n\"\n",
    "    \n",
    "    if found:\n",
    "        return response\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def find_symbol_usage_nx(graph, node_id, symbol_name):\n",
    "    \"\"\"\n",
    "    Find all usages of a specific symbol across the codebase using NetworkX graph\n",
    "    \n",
    "    Args:\n",
    "        graph: The NetworkX graph object\n",
    "        node_id: The node ID of the symbol node in the graph\n",
    "        symbol_name: The name of the symbol for display purposes\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with files and line numbers where the symbol is used\n",
    "    \"\"\"\n",
    "    # Check if this is a valid node\n",
    "    if node_id not in graph:\n",
    "        # This could be a pseudo node ID we created for snippet matches\n",
    "        if isinstance(node_id, str) and '::' in node_id:\n",
    "            file_path = node_id.split('::')[0]\n",
    "            # Return information based on file path and symbol name\n",
    "            return {\n",
    "                \"symbol\": symbol_name,\n",
    "                \"symbol_type\": \"variable\",  # Assuming variable as default\n",
    "                \"defined_in\": [file_path],\n",
    "                \"docstring\": \"\",\n",
    "                \"usages\": search_symbol_in_file(graph, file_path, symbol_name),\n",
    "                \"related_symbols\": []\n",
    "            }\n",
    "        return {\"error\": f\"Symbol '{symbol_name}' not found in the codebase as a specific node\"}\n",
    "    \n",
    "    # Get symbol node data\n",
    "    node_data = graph.nodes[node_id]\n",
    "    \n",
    "    # Extract symbol type and other metadata\n",
    "    symbol_type = node_data.get('symbol_type', 'variable')  # Default to variable\n",
    "    docstring = node_data.get('docstring', '')\n",
    "    \n",
    "    # Find definition locations\n",
    "    definitions = []\n",
    "    for source, target, edge_data in graph.in_edges(node_id, data=True):\n",
    "        if edge_data.get('edge_type') == 'defines':\n",
    "            # Source should be a file node\n",
    "            definitions.append(source)\n",
    "    \n",
    "    # If no definitions found through edges, extract from node ID\n",
    "    if not definitions and isinstance(node_id, str) and '::' in node_id:\n",
    "        file_path = node_id.split('::')[0]\n",
    "        definitions.append(file_path)\n",
    "    \n",
    "    # Find all references to the symbol\n",
    "    usages = {}\n",
    "    for source, target, edge_data in graph.in_edges(node_id, data=True):\n",
    "        edge_type = edge_data.get('edge_type')\n",
    "        \n",
    "        # Consider references\n",
    "        if edge_type == 'references':\n",
    "            # Source should be a file node\n",
    "            file_name = source\n",
    "            \n",
    "            # Extract line info and context\n",
    "            line_info = edge_data.get('line_number', 'unknown line')\n",
    "            context = edge_data.get('context', 'No context available')\n",
    "            \n",
    "            if file_name not in usages:\n",
    "                usages[file_name] = []\n",
    "            \n",
    "            usages[file_name].append({\n",
    "                'line': line_info,\n",
    "                'context': context\n",
    "            })\n",
    "    \n",
    "    # Find related symbols (e.g., symbols in the same file)\n",
    "    related_symbols = []\n",
    "    for def_file in definitions:\n",
    "        for source, target, edge_data in graph.out_edges(def_file, data=True):\n",
    "            if edge_data.get('edge_type') == 'defines' and target != node_id:\n",
    "                # Extract symbol name from target node ID\n",
    "                if isinstance(target, str) and '::' in target:\n",
    "                    related_symbol = target.split('::')[1]\n",
    "                    related_symbols.append(related_symbol)\n",
    "    \n",
    "    return {\n",
    "        \"symbol\": symbol_name,\n",
    "        \"symbol_type\": symbol_type,\n",
    "        \"defined_in\": definitions,\n",
    "        \"docstring\": docstring,\n",
    "        \"usages\": usages,\n",
    "        \"related_symbols\": related_symbols\n",
    "    }\n",
    "\n",
    "def search_symbol_in_file(graph, file_path, symbol_name):\n",
    "    \"\"\"\n",
    "    Search for uses of a symbol within a specific file\n",
    "    \n",
    "    Args:\n",
    "        graph: NetworkX graph\n",
    "        file_path: Path of the file to search in\n",
    "        symbol_name: Name of the symbol to search for\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with usage information\n",
    "    \"\"\"\n",
    "    usages = {}\n",
    "    \n",
    "    # Look for snippet nodes from this file\n",
    "    for node_id, node_data in graph.nodes(data=True):\n",
    "        if node_data.get('type') == 'snippet':\n",
    "            # Check if this snippet belongs to the file\n",
    "            belongs_to_file = False\n",
    "            for source, target, edge_data in graph.in_edges(node_id, data=True):\n",
    "                if edge_data.get('edge_type') == 'contains_snippet' and source == file_path:\n",
    "                    belongs_to_file = True\n",
    "                    break\n",
    "            \n",
    "            if belongs_to_file:\n",
    "                code_snippet = node_data.get('code_snippet', '')\n",
    "                if code_snippet and symbol_name.lower() in code_snippet.lower():\n",
    "                    start_line = node_data.get('start_line', 'unknown')\n",
    "                    \n",
    "                    if file_path not in usages:\n",
    "                        usages[file_path] = []\n",
    "                    \n",
    "                    # Extract the lines containing the symbol\n",
    "                    lines = code_snippet.split('\\n')\n",
    "                    for i, line in enumerate(lines):\n",
    "                        if symbol_name.lower() in line.lower():\n",
    "                            line_num = int(start_line) + i if isinstance(start_line, int) else \"unknown\"\n",
    "                            usages[file_path].append({\n",
    "                                'line': line_num,\n",
    "                                'context': line.strip()\n",
    "                            })\n",
    "    \n",
    "    return usages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The text LLM file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T11:56:24.257102Z",
     "iopub.status.busy": "2025-03-01T11:56:24.256777Z",
     "iopub.status.idle": "2025-03-01T11:56:24.265398Z",
     "shell.execute_reply": "2025-03-01T11:56:24.264465Z",
     "shell.execute_reply.started": "2025-03-01T11:56:24.257079Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def convert_arango_to_networkx(db, graph_name):\n",
    "    \"\"\"\n",
    "    Convert an ArangoDB graph to NetworkX format\n",
    "    \n",
    "    Args:\n",
    "        db: ArangoDB database connection\n",
    "        graph_name: Name of the graph in ArangoDB\n",
    "    \n",
    "    Returns:\n",
    "        NetworkX graph object\n",
    "    \"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    try:\n",
    "        # Get graph from ArangoDB\n",
    "        arango_graph = db.graph(graph_name)\n",
    "        \n",
    "        # Get graph properties\n",
    "        try:\n",
    "            graph_properties = arango_graph.properties()\n",
    "            edge_definitions = graph_properties.get('edgeDefinitions', [])\n",
    "            \n",
    "            # Process each vertex collection\n",
    "            vertex_collections = set()\n",
    "            for edge_def in edge_definitions:\n",
    "                vertex_collections.update(edge_def.get('from', []))\n",
    "                vertex_collections.update(edge_def.get('to', []))\n",
    "                \n",
    "            # Add nodes from vertex collections\n",
    "            for collection_name in vertex_collections:\n",
    "                try:\n",
    "                    collection = db.collection(collection_name)\n",
    "                    for doc in collection:\n",
    "                        node_id = doc['_id']\n",
    "                        G.add_node(node_id, **doc)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error with vertex collection {collection_name}: {str(e)}\")\n",
    "            \n",
    "            # Process edge collections\n",
    "            for edge_def in edge_definitions:\n",
    "                edge_collection_name = edge_def.get('collection')\n",
    "                if edge_collection_name:\n",
    "                    try:\n",
    "                        collection = db.collection(edge_collection_name)\n",
    "                        for doc in collection:\n",
    "                            from_id = doc['_from']\n",
    "                            to_id = doc['_to']\n",
    "                            G.add_edge(from_id, to_id, **doc)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error with edge collection {edge_collection_name}: {str(e)}\")\n",
    "                        \n",
    "        except Exception as e:\n",
    "            # Alternative approach: try to infer collections from naming convention\n",
    "            print(f\"Error getting graph properties, trying alternate approach: {str(e)}\")\n",
    "            \n",
    "            # Common naming patterns for ArangoDB collections\n",
    "            node_collection_pattern = re.compile(f\"{re.escape(graph_name)}_node\")\n",
    "            edge_collection_pattern = re.compile(f\"{re.escape(graph_name)}_.*_to_.*\")\n",
    "            \n",
    "            # Try to find matching collections\n",
    "            for collection_name in db.collections():\n",
    "                if node_collection_pattern.match(collection_name):\n",
    "                    try:\n",
    "                        collection = db.collection(collection_name)\n",
    "                        for doc in collection:\n",
    "                            node_id = doc['_id']\n",
    "                            G.add_node(node_id, **doc)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error with inferred vertex collection {collection_name}: {str(e)}\")\n",
    "                        \n",
    "                elif edge_collection_pattern.match(collection_name):\n",
    "                    try:\n",
    "                        collection = db.collection(collection_name)\n",
    "                        for doc in collection:\n",
    "                            from_id = doc['_from']\n",
    "                            to_id = doc['_to']\n",
    "                            G.add_edge(from_id, to_id, **doc)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error with inferred edge collection {collection_name}: {str(e)}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing ArangoDB graph: {str(e)}\")\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T11:56:20.296881Z",
     "iopub.status.busy": "2025-03-01T11:56:20.296554Z",
     "iopub.status.idle": "2025-03-01T11:56:20.304595Z",
     "shell.execute_reply": "2025-03-01T11:56:20.303580Z",
     "shell.execute_reply.started": "2025-03-01T11:56:20.296856Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def text_to_nx_algorithm_to_text(query_text, graph=None, db_connection=None, graph_name=None):\n",
    "    \"\"\"\n",
    "    Universal entry point for processing natural language queries against a code graph\n",
    "    \n",
    "    Args:\n",
    "        query_text: Natural language query about the codebase\n",
    "        graph: NetworkX graph object (if already available)\n",
    "        db_connection: ArangoDB connection (if graph should be loaded from ArangoDB)\n",
    "        graph_name: Name of the graph in ArangoDB (if db_connection is provided)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with analysis results in a human-readable format\n",
    "    \"\"\"\n",
    "    # Case 1: NetworkX graph is directly provided\n",
    "    if graph is not None:\n",
    "        if isinstance(graph, nx.Graph):\n",
    "            return analyze_networkx_graph(graph, query_text)\n",
    "        else:\n",
    "            return {\"response\": \"Provided graph object is not a valid NetworkX graph\"}\n",
    "    \n",
    "    # Case 2: ArangoDB connection is provided but no graph\n",
    "    elif db_connection is not None and graph_name is None:\n",
    "        try:\n",
    "            # Try to list available graphs\n",
    "            try:\n",
    "                available_graphs = db_connection.graphs()\n",
    "                if available_graphs:\n",
    "                    graph_name = available_graphs[0][\"name\"]\n",
    "                else:\n",
    "                    return {\"response\": \"No graphs found in the ArangoDB database\"}\n",
    "            except:\n",
    "                # Different API for networkx-arangodb\n",
    "                try:\n",
    "                    import networkx_arangodb as nxadb\n",
    "                    # Try a different approach for networkx-arangodb\n",
    "                    available_graphs = [g for g in db_connection.graphs()]\n",
    "                    if available_graphs:\n",
    "                        graph_name = available_graphs[0]\n",
    "                    else:\n",
    "                        return {\"response\": \"No graphs found in the ArangoDB database\"}\n",
    "                except:\n",
    "                    return {\"response\": \"Could not retrieve graph list from ArangoDB\"}\n",
    "        except Exception as e:\n",
    "            return {\"response\": f\"Error retrieving graphs from ArangoDB: {str(e)}\"}\n",
    "        \n",
    "    \n",
    "    # Case 3: ArangoDB connection and graph name are provided\n",
    "    if db_connection is not None and graph_name is not None:\n",
    "        try:\n",
    "            # Try to load the graph using standard ArangoDB driver\n",
    "            try:\n",
    "                # Check if graph_name is a string\n",
    "                if not isinstance(graph_name, str):\n",
    "                    if hasattr(graph_name, 'name'):\n",
    "                        # It might be a graph object with a name attribute\n",
    "                        graph_name = graph_name.name\n",
    "                    else:\n",
    "                        return {\"response\": \"Graph name must be a string or an object with a name attribute\"}\n",
    "                \n",
    "                # Try to get the graph from ArangoDB\n",
    "                arango_graph = db_connection.graph(graph_name)\n",
    "                # Convert to NetworkX using our custom function\n",
    "                nx_graph = convert_arango_to_networkx(db_connection, graph_name)\n",
    "                \n",
    "                return analyze_networkx_graph(nx_graph, query_text)\n",
    "            except Exception as e1:\n",
    "                # If standard approach fails, try networkx-arangodb\n",
    "                try:\n",
    "                    import networkx_arangodb as nxadb\n",
    "                    graph = nxadb.Graph(name=graph_name, db=db_connection)\n",
    "                    nx_graph = graph.to_networkx()\n",
    "                    print(type(nx_graph))\n",
    "                    return analyze_networkx_graph(nx_graph, query_text)\n",
    "                except Exception as e2:\n",
    "                    return {\"response\": f\"Error loading graph from ArangoDB: {str(e1)}\\nAlternative method error: {str(e2)}\"}\n",
    "        except Exception as e:\n",
    "            return {\"response\": f\"Error accessing ArangoDB: {str(e)}\"}\n",
    "    \n",
    "    # Case 4: No graph info provided at all\n",
    "    return {\"response\": \"Please provide either a NetworkX graph or ArangoDB connection details\"}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting it to NETWORKX, can be removed for effiency and maintained in arango"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nx_arangodb.classes.graph.Graph"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(G_adb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_Graph__db', '_Graph__graph_exists_in_db', '_Graph__is_smart', '_Graph__name', '_Graph__set_arangodb_backend_config', '_Graph__set_db', '_Graph__set_edge_collections_attributes', '_Graph__set_graph', '_Graph__smart_field', '_Graph__use_arango_views', '__annotations__', '__class__', '__contains__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__networkx_backend__', '__networkx_cache__', '__networkx_plugin__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_adj', '_db_name', '_edge_collections_attributes', '_hosts', '_load_nx_graph', '_loaded_incoming_graph_data', '_node', '_password', '_set_factory_methods', '_username', 'adb_graph', 'add_edge', 'add_edges_from', 'add_node', 'add_node_override', 'add_nodes_from', 'add_nodes_from_override', 'add_weighted_edges_from', 'adj', 'adjacency', 'adjlist_inner_dict_factory', 'adjlist_outer_dict_factory', 'chat', 'clear', 'clear_edges', 'clear_edges_override', 'clear_nxcg_cache', 'clear_override', 'copy', 'copy_override', 'db', 'default_node_type', 'degree', 'edge_attr_dict_factory', 'edge_attributes', 'edge_subgraph', 'edge_type_func', 'edge_type_key', 'edges', 'get_edge_data', 'graph', 'graph_attr_dict_factory', 'graph_exists_in_db', 'has_edge', 'has_node', 'is_directed', 'is_multigraph', 'is_smart', 'name', 'nbunch_iter', 'nbunch_iter_override', 'neighbors', 'node_attr_dict_factory', 'node_dict_factory', 'nodes', 'number_of_edges', 'number_of_edges_override', 'number_of_nodes', 'nxcg_graph', 'order', 'query', 'read_batch_size', 'read_parallelism', 'remove_edge', 'remove_edges_from', 'remove_node', 'remove_nodes_from', 'size', 'smart_field', 'subgraph', 'subgraph_override', 'symmetrize_edges', 'to_directed', 'to_directed_class', 'to_networkx_class', 'to_undirected', 'to_undirected_class', 'update', 'use_nxcg_cache']\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# Create an empty NetworkX DiGraph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Check what methods and attributes are available on your G_adb object\n",
    "# Uncomment this line to inspect what's available:\n",
    "print(dir(G_adb))\n",
    "\n",
    "# Try accessing nodes and edges directly\n",
    "# Option 1: If G_adb has nodes() and edges() methods\n",
    "try:\n",
    "    # Add nodes\n",
    "    for node_id, node_data in G_adb.nodes(data=True):\n",
    "        G.add_node(node_id, **node_data)\n",
    "    \n",
    "    # Add edges\n",
    "    for source, target, edge_data in G_adb.edges(data=True):\n",
    "        G.add_edge(source, target, **edge_data)\n",
    "except AttributeError:\n",
    "    # Option 2: If G_adb has direct node and edge collections\n",
    "    try:\n",
    "        # Check if G_adb.nodes and G_adb.edges are iterable\n",
    "        for node_id, node_data in G_adb.nodes.items():\n",
    "            G.add_node(node_id, **node_data)\n",
    "        \n",
    "        for edge_id, edge_data in G_adb.edges.items():\n",
    "            # Assuming edge_data has 'from' and 'to' attributes\n",
    "            G.add_edge(edge_data['from'], edge_data['to'], **edge_data)\n",
    "    except AttributeError:\n",
    "        print(\"Could not determine how to access nodes and edges. Please inspect G_adb structure.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T11:59:05.742863Z",
     "iopub.status.busy": "2025-03-01T11:59:05.742479Z",
     "iopub.status.idle": "2025-03-01T11:59:05.770985Z",
     "shell.execute_reply": "2025-03-01T11:59:05.770030Z",
     "shell.execute_reply.started": "2025-03-01T11:59:05.742833Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symbol: 'has_level_handler()' (Type: function)\n",
      "\n",
      "Defined in: No definition location found\n",
      "\n",
      "Used in the following locations:\n",
      "\n",
      "No usage information found for this symbol in graph nodes.\n",
      "\n",
      "However, found these mentions in code snippets:\n",
      "\n",
      "\n",
      "File: Unknown (Lines 61-80)\n",
      "- Line 70: def test_has_level_handler():\n",
      "\n",
      "File: FlaskRepv1_node/213 (Line 67)\n",
      "- assert wsgi_errors_stream._get_current_object() is sys.stderr\n",
      "\n",
      "    with app.test_request_context(errors_stream=stream):\n",
      "        assert wsgi_errors_stream._get_current_object() is stream\n",
      "\n",
      "\n",
      "def test_has_level_handler():\n",
      "\n",
      "Analysis performed on graph with 2975 nodes and 8651 edges\n"
     ]
    }
   ],
   "source": [
    "result = text_to_nx_algorithm_to_text(\n",
    "    \"has_level_handler()\",\n",
    "    graph=G,  # Your NetworkX graph\n",
    "    #graph_name=\"FlaskRepv1\",\n",
    "    db_connection=db\n",
    ")\n",
    "print(result['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T04:47:01.096433Z",
     "iopub.status.busy": "2025-03-01T04:47:01.096097Z",
     "iopub.status.idle": "2025-03-01T04:47:02.298827Z",
     "shell.execute_reply": "2025-03-01T04:47:02.298082Z",
     "shell.execute_reply.started": "2025-03-01T04:47:01.096408Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(G_adb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T06:37:40.954596Z",
     "iopub.status.busy": "2025-03-04T06:37:40.954199Z",
     "iopub.status.idle": "2025-03-04T06:37:40.963268Z",
     "shell.execute_reply": "2025-03-04T06:37:40.961965Z",
     "shell.execute_reply.started": "2025-03-04T06:37:40.954567Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "arango_graph.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'keploy'...\n",
      "remote: Enumerating objects: 407, done.\u001b[K\n",
      "remote: Counting objects: 100% (407/407), done.\u001b[K\n",
      "remote: Compressing objects: 100% (374/374), done.\u001b[K\n",
      "remote: Total 407 (delta 40), reused 201 (delta 13), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (407/407), 561.66 KiB | 357.00 KiB/s, done.\n",
      "Resolving deltas: 100% (40/40), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone --depth=1 https://github.com/keploy/keploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_2/xx5z8xdj6j98wh4vt2b59jz80000gp/T/ipykernel_46967/1979416937.py:115: UserWarning: Could not build python parser library: type object 'tree_sitter.Language' has no attribute 'build_library'\n",
      "  warnings.warn(f\"Could not build {language} parser library: {e}\")\n",
      "/var/folders/_2/xx5z8xdj6j98wh4vt2b59jz80000gp/T/ipykernel_46967/1979416937.py:115: UserWarning: Could not build cpp parser library: type object 'tree_sitter.Language' has no attribute 'build_library'\n",
      "  warnings.warn(f\"Could not build {language} parser library: {e}\")\n",
      "/var/folders/_2/xx5z8xdj6j98wh4vt2b59jz80000gp/T/ipykernel_46967/1979416937.py:115: UserWarning: Could not build java parser library: type object 'tree_sitter.Language' has no attribute 'build_library'\n",
      "  warnings.warn(f\"Could not build {language} parser library: {e}\")\n",
      "/var/folders/_2/xx5z8xdj6j98wh4vt2b59jz80000gp/T/ipykernel_46967/1979416937.py:115: UserWarning: Could not build go parser library: type object 'tree_sitter.Language' has no attribute 'build_library'\n",
      "  warnings.warn(f\"Could not build {language} parser library: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph exported to codebase_graph.json\n",
      "Dependency graph saved to codebase_dependencies_go.html\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import ast\n",
    "import json\n",
    "import networkx as nx\n",
    "import warnings\n",
    "import tempfile\n",
    "\n",
    "# Attempt to import tree-sitter safely\n",
    "try:\n",
    "    import tree_sitter\n",
    "    from tree_sitter import Language, Parser\n",
    "    TREE_SITTER_AVAILABLE = True\n",
    "except ImportError:\n",
    "    warnings.warn(\"Tree-sitter not installed. Falling back to regex-based parsing.\")\n",
    "    TREE_SITTER_AVAILABLE = False\n",
    "\n",
    "# Attempt to import pyvis safely\n",
    "try:\n",
    "    from pyvis.network import Network\n",
    "    PYVIS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    warnings.warn(\"Pyvis not installed. Visualization will be limited.\")\n",
    "    PYVIS_AVAILABLE = False\n",
    "\n",
    "class CodebaseVisualizer:\n",
    "    def __init__(self, codebase_path, language=None):\n",
    "        \"\"\"\n",
    "        Initialize the CodebaseVisualizer with multi-language support\n",
    "        \n",
    "        Args:\n",
    "            codebase_path: Path to the codebase directory\n",
    "            language: Optional language specification (if None, will auto-detect)\n",
    "        \"\"\"\n",
    "        self.codebase_path = os.path.abspath(codebase_path)\n",
    "        self.graph = nx.DiGraph()\n",
    "        self.parsed_files = {}\n",
    "        self.code_entities = {}\n",
    "        \n",
    "        # Language detection and mapping\n",
    "        self.language_extensions = {\n",
    "            'python': ['.py'],\n",
    "            'cpp': ['.cpp', '.cxx', '.cc', '.c', '.h', '.hpp'],\n",
    "            'java': ['.java'],\n",
    "            'go': ['.go'],\n",
    "            'javascript': ['.js', '.jsx', '.ts', '.tsx']\n",
    "        }\n",
    "        \n",
    "        # Temporary build directory for tree-sitter libraries\n",
    "        self.build_dir = tempfile.mkdtemp(prefix='tree_sitter_build_')\n",
    "        \n",
    "        # Path to local tree-sitter language repositories\n",
    "        self.tree_sitter_paths = {\n",
    "            'python': os.path.join(os.getcwd(), 'tree-sitter-languages', 'tree-sitter-python'),\n",
    "            'cpp': os.path.join(os.getcwd(), 'tree-sitter-languages', 'tree-sitter-cpp'),\n",
    "            'java': os.path.join(os.getcwd(), 'tree-sitter-languages', 'tree-sitter-java'),\n",
    "            'go': os.path.join(os.getcwd(), 'tree-sitter-languages', 'tree-sitter-go')\n",
    "        }\n",
    "        \n",
    "        # Auto-detect language if not specified\n",
    "        self.language = language or self._detect_language()\n",
    "        \n",
    "        # Tree-sitter language loaders\n",
    "        self.language_parsers = {}\n",
    "        if TREE_SITTER_AVAILABLE:\n",
    "            self._initialize_tree_sitter_parsers()\n",
    "\n",
    "    def _initialize_tree_sitter_parsers(self):\n",
    "        \"\"\"\n",
    "        Initialize tree-sitter language parsers\n",
    "        \"\"\"\n",
    "        for lang, path in self.tree_sitter_paths.items():\n",
    "            try:\n",
    "                # Check if language repository exists\n",
    "                if os.path.exists(path):\n",
    "                    # Ensure the library is built\n",
    "                    so_path = self._build_tree_sitter_library(lang, path)\n",
    "                    if so_path:\n",
    "                        self.language_parsers[lang] = Language(so_path, lang)\n",
    "            except Exception as e:\n",
    "                warnings.warn(f\"Could not load {lang} parser: {e}\")\n",
    "\n",
    "    def _build_tree_sitter_library(self, language: str, language_path: str):\n",
    "        \"\"\"\n",
    "        Build tree-sitter language library with robust error handling\n",
    "        \n",
    "        Args:\n",
    "            language: Language name\n",
    "            language_path: Path to language repository\n",
    "        \n",
    "        Returns:\n",
    "            Path to compiled library or None\n",
    "        \"\"\"\n",
    "        if not TREE_SITTER_AVAILABLE:\n",
    "            return None\n",
    "\n",
    "        # Ensure the build directory exists\n",
    "        os.makedirs(self.build_dir, exist_ok=True)\n",
    "\n",
    "        # Library path\n",
    "        lib_path = os.path.join(self.build_dir, f'{language}-language.so')\n",
    "\n",
    "        # If library already exists, return it\n",
    "        if os.path.exists(lib_path):\n",
    "            return lib_path\n",
    "\n",
    "        try:\n",
    "            # Attempt to build the library\n",
    "            Language.build_library(\n",
    "                lib_path, \n",
    "                [language_path]\n",
    "            )\n",
    "            return lib_path\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"Could not build {language} parser library: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _detect_language(self) -> str:\n",
    "        \"\"\"\n",
    "        Detect the primary language of the project based on file extensions\n",
    "        \n",
    "        Returns:\n",
    "            str: Detected language\n",
    "        \"\"\"\n",
    "        extension_counts = {}\n",
    "        \n",
    "        for root, _, files in os.walk(self.codebase_path):\n",
    "            for file in files:\n",
    "                for lang, exts in self.language_extensions.items():\n",
    "                    if any(file.endswith(ext) for ext in exts):\n",
    "                        extension_counts[lang] = extension_counts.get(lang, 0) + 1\n",
    "        \n",
    "        # Return the language with most files, default to Python\n",
    "        return max(extension_counts, key=extension_counts.get, default='python')\n",
    "\n",
    "    def parse_files(self):\n",
    "        \"\"\"\n",
    "        Parse files in the codebase directory based on detected language\n",
    "        \"\"\"\n",
    "        for root, _, files in os.walk(self.codebase_path):\n",
    "            for file in files:\n",
    "                # Check if file matches language extensions\n",
    "                if any(file.endswith(ext) for ext in self.language_extensions.get(self.language, [])):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    self._parse_file(file_path)\n",
    "\n",
    "    def _parse_file(self, file_path: str):\n",
    "        \"\"\"\n",
    "        Parse a file based on the detected language\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to the file to parse\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "                \n",
    "            # Language-specific parsing\n",
    "            parse_methods = {\n",
    "                'python': self._parse_python_file,\n",
    "                'cpp': self._parse_cpp_file,\n",
    "                'java': self._parse_java_file,\n",
    "                'go': self._parse_go_file\n",
    "            }\n",
    "            \n",
    "            # Call the appropriate parsing method\n",
    "            parse_method = parse_methods.get(self.language)\n",
    "            if parse_method:\n",
    "                parse_method(file_path, content)\n",
    "            else:\n",
    "                warnings.warn(f\"No parser available for {self.language}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"Error parsing {file_path}: {e}\")\n",
    "\n",
    "    def _parse_python_file(self, file_path: str, content: str):\n",
    "        \"\"\"Parse Python files using AST\"\"\"\n",
    "        try:\n",
    "            tree = ast.parse(content)\n",
    "            \n",
    "            # Initialize file entry\n",
    "            self.parsed_files[file_path] = {\n",
    "                'imports': [],\n",
    "                'functions': [],\n",
    "                'classes': []\n",
    "            }\n",
    "            \n",
    "            for node in ast.walk(tree):\n",
    "                # Import detection\n",
    "                if isinstance(node, (ast.Import, ast.ImportFrom)):\n",
    "                    if isinstance(node, ast.Import):\n",
    "                        for alias in node.names:\n",
    "                            self.parsed_files[file_path]['imports'].append(alias.name)\n",
    "                    else:\n",
    "                        module = node.module or ''\n",
    "                        for alias in node.names:\n",
    "                            import_name = f\"{module}.{alias.name}\" if module else alias.name\n",
    "                            self.parsed_files[file_path]['imports'].append(import_name)\n",
    "                \n",
    "                # Function detection\n",
    "                if isinstance(node, ast.FunctionDef):\n",
    "                    func_name = f\"{os.path.basename(file_path)}::{node.name}\"\n",
    "                    self.parsed_files[file_path]['functions'].append(node.name)\n",
    "                    self.code_entities[func_name] = {\n",
    "                        'type': 'symbol',\n",
    "                        'symbol_type': 'function',\n",
    "                        'file': file_path,\n",
    "                        'line_number': node.lineno,\n",
    "                        'docstring': ast.get_docstring(node)\n",
    "                    }\n",
    "                \n",
    "                # Class detection\n",
    "                if isinstance(node, ast.ClassDef):\n",
    "                    class_name = f\"{os.path.basename(file_path)}::{node.name}\"\n",
    "                    self.parsed_files[file_path]['classes'].append(node.name)\n",
    "                    self.code_entities[class_name] = {\n",
    "                        'type': 'symbol',\n",
    "                        'symbol_type': 'class',\n",
    "                        'file': file_path,\n",
    "                        'line_number': node.lineno,\n",
    "                        'docstring': ast.get_docstring(node)\n",
    "                    }\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"Error parsing Python file {file_path}: {e}\")\n",
    "\n",
    "    def _parse_cpp_file(self, file_path: str, content: str):\n",
    "        \"\"\"\n",
    "        Parse C++ files using regex and optionally tree-sitter\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to the CPP file\n",
    "            content: File content\n",
    "        \"\"\"\n",
    "        # Fallback regex-based parsing\n",
    "        function_pattern = re.compile(r'([\\w:]+)\\s+(\\w+)\\s*\\((.*?)\\)\\s*(?:const)?\\s*{', re.DOTALL)\n",
    "        class_pattern = re.compile(r'class\\s+(\\w+)[\\s:]*(?:public|private|protected)?\\s*\\{', re.DOTALL)\n",
    "        include_pattern = re.compile(r'#include\\s*[<\"](.+?)[>\"]')\n",
    "        \n",
    "        # Initialize file entry\n",
    "        self.parsed_files[file_path] = {\n",
    "            'imports': [],\n",
    "            'functions': [],\n",
    "            'classes': []\n",
    "        }\n",
    "        \n",
    "        # Detect includes/imports\n",
    "        for include in include_pattern.findall(content):\n",
    "            self.parsed_files[file_path]['imports'].append(include)\n",
    "        \n",
    "        # Detect functions\n",
    "        for match in function_pattern.finditer(content):\n",
    "            func_name = f\"{os.path.basename(file_path)}::{match.group(2)}\"\n",
    "            self.parsed_files[file_path]['functions'].append(match.group(2))\n",
    "            self.code_entities[func_name] = {\n",
    "                'type': 'symbol',\n",
    "                'symbol_type': 'function',\n",
    "                'file': file_path,\n",
    "                'line_number': content[:match.start()].count('\\n') + 1,\n",
    "                'context': match.group(0)\n",
    "            }\n",
    "        \n",
    "        # Detect classes\n",
    "        for match in class_pattern.finditer(content):\n",
    "            class_name = f\"{os.path.basename(file_path)}::{match.group(1)}\"\n",
    "            self.parsed_files[file_path]['classes'].append(match.group(1))\n",
    "            self.code_entities[class_name] = {\n",
    "                'type': 'symbol',\n",
    "                'symbol_type': 'class',\n",
    "                'file': file_path,\n",
    "                'line_number': content[:match.start()].count('\\n') + 1,\n",
    "                'context': match.group(0)\n",
    "            }\n",
    "        \n",
    "        # Optional: Use tree-sitter for more robust parsing if available\n",
    "        if self.language_parsers['cpp']:\n",
    "            parser = Parser()\n",
    "            parser.set_language(self.language_parsers['cpp'])\n",
    "            tree = parser.parse(content.encode())\n",
    "            # Additional parsing logic with tree-sitter can be added here\n",
    "\n",
    "    def _parse_java_file(self, file_path: str, content: str):\n",
    "        \"\"\"\n",
    "        Parse Java files\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to the Java file\n",
    "            content: File content\n",
    "        \"\"\"\n",
    "        # Regex patterns for Java\n",
    "        package_pattern = re.compile(r'package\\s+([^;]+);')\n",
    "        import_pattern = re.compile(r'import\\s+([^;]+);')\n",
    "        class_pattern = re.compile(r'(?:public|private|protected)?\\s*(?:abstract)?\\s*class\\s+(\\w+)', re.DOTALL)\n",
    "        method_pattern = re.compile(r'(?:public|private|protected)?\\s*(?:static)?\\s*[\\w<>]+\\s+(\\w+)\\s*\\((.*?)\\)', re.DOTALL)\n",
    "        \n",
    "        # Initialize file entry\n",
    "        self.parsed_files[file_path] = {\n",
    "            'package': None,\n",
    "            'imports': [],\n",
    "            'classes': [],\n",
    "            'methods': []\n",
    "        }\n",
    "        \n",
    "        # Detect package\n",
    "        package_match = package_pattern.search(content)\n",
    "        if package_match:\n",
    "            self.parsed_files[file_path]['package'] = package_match.group(1)\n",
    "        \n",
    "        # Detect imports\n",
    "        for import_match in import_pattern.finditer(content):\n",
    "            self.parsed_files[file_path]['imports'].append(import_match.group(1))\n",
    "        \n",
    "        # Detect classes\n",
    "        for match in class_pattern.finditer(content):\n",
    "            class_name = f\"{os.path.basename(file_path)}::{match.group(1)}\"\n",
    "            self.parsed_files[file_path]['classes'].append(match.group(1))\n",
    "            self.code_entities[class_name] = {\n",
    "                'type': 'symbol',\n",
    "                'symbol_type': 'class',\n",
    "                'file': file_path,\n",
    "                'line_number': content[:match.start()].count('\\n') + 1,\n",
    "                'context': match.group(0)\n",
    "            }\n",
    "        \n",
    "        # Detect methods\n",
    "        for match in method_pattern.finditer(content):\n",
    "            method_name = f\"{os.path.basename(file_path)}::{match.group(1)}\"\n",
    "            self.parsed_files[file_path]['methods'].append(match.group(1))\n",
    "            self.code_entities[method_name] = {\n",
    "                'type': 'symbol',\n",
    "                'symbol_type': 'method',\n",
    "                'file': file_path,\n",
    "                'line_number': content[:match.start()].count('\\n') + 1,\n",
    "                'context': match.group(0)\n",
    "            }\n",
    "\n",
    "    def _parse_go_file(self, file_path: str, content: str):\n",
    "        \"\"\"\n",
    "        Parse Go files\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to the Go file\n",
    "            content: File content\n",
    "        \"\"\"\n",
    "        # Regex patterns for Go\n",
    "        package_pattern = re.compile(r'package\\s+(\\w+)')\n",
    "        import_pattern = re.compile(r'import\\s*(?:\\(([^)]+)\\)|\"([^\"]+)\")', re.DOTALL)\n",
    "        func_pattern = re.compile(r'func\\s*(?:\\(\\w+\\s*\\*?\\w+\\)\\s*)?(\\w+)\\s*\\((.*?)\\)', re.DOTALL)\n",
    "        struct_pattern = re.compile(r'type\\s+(\\w+)\\s+struct\\s*{', re.DOTALL)\n",
    "        \n",
    "        # Initialize file entry\n",
    "        self.parsed_files[file_path] = {\n",
    "            'package': None,\n",
    "            'imports': [],\n",
    "            'functions': [],\n",
    "            'structs': []\n",
    "        }\n",
    "        \n",
    "        # Detect package\n",
    "        package_match = package_pattern.search(content)\n",
    "        if package_match:\n",
    "            self.parsed_files[file_path]['package'] = package_match.group(1)\n",
    "        \n",
    "        # Detect imports\n",
    "        import_matches = import_pattern.findall(content)\n",
    "        for match in import_matches:\n",
    "            # Handle both single and multiple imports\n",
    "            if match[0]:  # Multiple imports in parentheses\n",
    "                for imp in match[0].split('\\n'):\n",
    "                    imp = imp.strip().strip('\"')\n",
    "                    if imp:\n",
    "                        self.parsed_files[file_path]['imports'].append(imp)\n",
    "            elif match[1]:  # Single import\n",
    "                self.parsed_files[file_path]['imports'].append(match[1])\n",
    "        \n",
    "        # Detect functions\n",
    "        for match in func_pattern.finditer(content):\n",
    "            func_name = f\"{os.path.basename(file_path)}::{match.group(1)}\"\n",
    "            self.parsed_files[file_path]['functions'].append(match.group(1))\n",
    "            self.code_entities[func_name] = {\n",
    "                'type': 'symbol',\n",
    "                'symbol_type': 'function',\n",
    "                'file': file_path,\n",
    "                'line_number': content[:match.start()].count('\\n') + 1,\n",
    "                'context': match.group(0)\n",
    "            }\n",
    "        \n",
    "        # Detect structs (Go's equivalent of classes)\n",
    "        for match in struct_pattern.finditer(content):\n",
    "            struct_name = f\"{os.path.basename(file_path)}::{match.group(1)}\"\n",
    "            self.parsed_files[file_path]['structs'].append(match.group(1))\n",
    "            self.code_entities[struct_name] = {\n",
    "                'type': 'symbol',\n",
    "                'symbol_type': 'struct',\n",
    "                'file': file_path,\n",
    "                'line_number': content[:match.start()].count('\\n') + 1,\n",
    "                'context': match.group(0)\n",
    "            }\n",
    "            \n",
    "    def visualize_dependencies(self, output_path='codebase_dependencies_go.html'):\n",
    "        \"\"\"\n",
    "        Create an interactive HTML visualization of code dependencies\n",
    "        \n",
    "        Args:\n",
    "            output_path (str): Path to save the HTML visualization\n",
    "        \n",
    "        Returns:\n",
    "            str: Path to the generated visualization\n",
    "        \"\"\"\n",
    "        if not PYVIS_AVAILABLE:\n",
    "            warnings.warn(\"Pyvis not installed. Cannot create visualization.\")\n",
    "            return None\n",
    "\n",
    "        # Create network with options\n",
    "        net = Network(height=\"750px\", width=\"100%\", bgcolor=\"#222222\", font_color=\"white\")\n",
    "        \n",
    "        # More stable physics settings\n",
    "        net.set_options('''\n",
    "        var options = {\n",
    "            \"physics\": {\n",
    "                \"forceAtlas2Based\": {\n",
    "                    \"gravitationalConstant\": -50,\n",
    "                    \"centralGravity\": 0.01,\n",
    "                    \"springLength\": 150,\n",
    "                    \"springConstant\": 0.08,\n",
    "                    \"damping\": 0.8\n",
    "                },\n",
    "                \"minVelocity\": 0.75,\n",
    "                \"solver\": \"forceAtlas2Based\"\n",
    "            }\n",
    "        }\n",
    "        ''')\n",
    "        \n",
    "        # Color mapping for different symbol types\n",
    "        color_map = {\n",
    "            'function': \"#4287f5\",   # Blue\n",
    "            'method': \"#4287f5\",     # Blue\n",
    "            'class': \"#42f5a7\",      # Green\n",
    "            'struct': \"#42f5a7\",     # Green\n",
    "            'import': \"#f542cb\",     # Pink\n",
    "            'file': \"#f5a742\",       # Orange\n",
    "            'dependency': \"#999999\"  # Gray\n",
    "        }\n",
    "        \n",
    "        # Add nodes with stable positioning\n",
    "        for node in self.graph.nodes:\n",
    "            node_data = self.graph.nodes[node]\n",
    "            node_type = node_data.get('type', 'unknown')\n",
    "            symbol_type = node_data.get('symbol_type', node_type)\n",
    "            \n",
    "            # Determine color and size\n",
    "            color = color_map.get(symbol_type, '#999999')\n",
    "            size = 15  # Default size\n",
    "            \n",
    "            if node_type == 'file':\n",
    "                size = 20\n",
    "            elif node_type == 'symbol':\n",
    "                size = 15 if symbol_type in ['function', 'method'] else 18\n",
    "            \n",
    "            # Create hover title\n",
    "            title = f\"<div style='max-width:300px;'>\"\n",
    "            title += f\"<b>{node}</b><br>\"\n",
    "            title += f\"Type: {symbol_type}<br>\"\n",
    "            \n",
    "            if 'file' in node_data:\n",
    "                title += f\"File: {node_data['file']}<br>\"\n",
    "            \n",
    "            if 'line_number' in node_data:\n",
    "                title += f\"Line: {node_data['line_number']}<br>\"\n",
    "            \n",
    "            title += \"</div>\"\n",
    "            \n",
    "            # Use fixed physics to reduce movement\n",
    "            net.add_node(\n",
    "                node, \n",
    "                label=os.path.basename(node), \n",
    "                title=title,\n",
    "                color=color,\n",
    "                size=size\n",
    "            )\n",
    "        \n",
    "        # Add edges\n",
    "        for source, target, edge_data in self.graph.edges(data=True):\n",
    "            edge_type = edge_data.get('edge_type', 'unknown')\n",
    "            \n",
    "            # Style edges\n",
    "            if edge_type == 'import':\n",
    "                color = '#f5f542'  # Yellow\n",
    "                width = 2\n",
    "            elif edge_type == 'contains':\n",
    "                color = '#42f55a'  # Green\n",
    "                width = 1\n",
    "            else:\n",
    "                color = '#999999'  # Gray\n",
    "                width = 1\n",
    "            \n",
    "            net.add_edge(\n",
    "                source, \n",
    "                target, \n",
    "                title=edge_type,\n",
    "                color=color,\n",
    "                width=width\n",
    "            )\n",
    "        \n",
    "        # Save the graph\n",
    "        net.save_graph(output_path)\n",
    "        print(f\"Dependency graph saved to {output_path}\")\n",
    "        return output_path\n",
    "\n",
    "    def build_graph(self):\n",
    "        \"\"\"\n",
    "        Build a networkx graph representing code relationships\n",
    "        \n",
    "        Returns:\n",
    "            networkx.DiGraph: Graph of code entities and their relationships\n",
    "        \"\"\"\n",
    "        # Create graph\n",
    "        graph = nx.DiGraph()\n",
    "        \n",
    "        # Add file nodes\n",
    "        for file_path in self.parsed_files:\n",
    "            file_node = file_path\n",
    "            graph.add_node(file_node, type='file', directory=os.path.dirname(file_path))\n",
    "        \n",
    "        # Add symbol nodes and edges\n",
    "        for symbol, entity in self.code_entities.items():\n",
    "            # Add symbol node\n",
    "            graph.add_node(\n",
    "                symbol, \n",
    "                type='symbol', \n",
    "                symbol_type=entity['symbol_type'],\n",
    "                file=entity['file'], \n",
    "                line_number=entity['line_number'],\n",
    "                context=entity.get('context', ''),\n",
    "                docstring=entity.get('docstring', '')\n",
    "            )\n",
    "            \n",
    "            # Connect symbol to its file\n",
    "            graph.add_edge(entity['file'], symbol, edge_type='contains')\n",
    "        \n",
    "        # Add import/dependency relationships\n",
    "        for file_path, file_info in self.parsed_files.items():\n",
    "            # Get imports based on language\n",
    "            imports = file_info.get('imports', [])\n",
    "            \n",
    "            for import_name in imports:\n",
    "                # Try to find matching symbols or use import name as node\n",
    "                matching_symbols = [\n",
    "                    symbol for symbol, entity in self.code_entities.items() \n",
    "                    if import_name in symbol or import_name in symbol.split('::')[-1]\n",
    "                ]\n",
    "                \n",
    "                if matching_symbols:\n",
    "                    for symbol in matching_symbols:\n",
    "                        graph.add_edge(file_path, symbol, edge_type='import')\n",
    "                else:\n",
    "                    # Add import as a separate node\n",
    "                    graph.add_node(import_name, type='dependency')\n",
    "                    graph.add_edge(file_path, import_name, edge_type='import')\n",
    "        \n",
    "        self.graph = graph\n",
    "        return graph\n",
    "\n",
    "    def export_graph_json(self, output_path='codebase_graph.json'):\n",
    "        \"\"\"\n",
    "        Export the graph to a JSON file\n",
    "        \n",
    "        Args:\n",
    "            output_path: Path to save the JSON graph\n",
    "        \"\"\"\n",
    "        # Convert graph to JSON-serializable format\n",
    "        graph_data = {\n",
    "            'nodes': [\n",
    "                {**self.graph.nodes[node], 'id': node} \n",
    "                for node in self.graph.nodes\n",
    "            ],\n",
    "            'edges': [\n",
    "                {'source': u, 'target': v, **self.graph.edges[u, v]} \n",
    "                for (u, v) in self.graph.edges\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(graph_data, f, indent=2)\n",
    "        \n",
    "        print(f\"Graph exported to {output_path}\")\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Replace with your actual codebase path\n",
    "        codebase_path = \"/Users/Viku/GitHub/scopium/keploy\"\n",
    "        \n",
    "        # Initialize the visualizer\n",
    "        visualizer = CodebaseVisualizer(codebase_path)\n",
    "        \n",
    "        # Parse files\n",
    "        visualizer.parse_files()\n",
    "        \n",
    "        # Build graph\n",
    "        graph = visualizer.build_graph()\n",
    "        \n",
    "        # Export graph to JSON\n",
    "        visualizer.export_graph_json()\n",
    "        \n",
    "        # Create visualization\n",
    "        visualizer.visualize_dependencies()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mistralai version: 1.5.1\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "This client is deprecated. To migrate to the new client, please refer to this guide: https://github.com/mistralai/client-python/blob/main/MIGRATION.md. If you need to use this client anyway, pin your version to 0.4.2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 555\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m    554\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m--> 555\u001b[0m query_engine \u001b[38;5;241m=\u001b[39m \u001b[43mCodebaseLLMQuery\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdb_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcodebase\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m    \u001b[49m\u001b[43musername\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mroot\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcUZ0YaNdcwfUTw6VjRny\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://d2eeb8083350.arangodb.cloud:8529\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmistral_api_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjJAuJZkjVcy2ynUhan375sHNviHiBeJU\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    561\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[49], line 44\u001b[0m, in \u001b[0;36mCodebaseLLMQuery.__init__\u001b[0;34m(self, db_name, username, password, host, mistral_api_key, model)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMistral API key not provided and not found in environment\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Initialize Mistral client with 0.4.2 compatible method\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmistral_client \u001b[38;5;241m=\u001b[39m \u001b[43mMistralClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmistral_api_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Graph name from your code\u001b[39;00m\n",
      "File \u001b[0;32m~/GitHub/scopium/.venv/lib/python3.10/site-packages/mistralai/client.py:14\u001b[0m, in \u001b[0;36mMistralClient.__init__\u001b[0;34m(self, api_key, endpoint, max_retries, timeout)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m      9\u001b[0m     api_key: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     timeout: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m120\u001b[39m,\n\u001b[1;32m     13\u001b[0m ):\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(MIGRATION_MESSAGE)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: This client is deprecated. To migrate to the new client, please refer to this guide: https://github.com/mistralai/client-python/blob/main/MIGRATION.md. If you need to use this client anyway, pin your version to 0.4.2."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
