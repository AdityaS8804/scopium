{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T04:36:35.485421Z",
     "iopub.status.busy": "2025-03-01T04:36:35.485084Z",
     "iopub.status.idle": "2025-03-01T04:37:23.851375Z",
     "shell.execute_reply": "2025-03-01T04:37:23.850454Z",
     "shell.execute_reply.started": "2025-03-01T04:36:35.485395Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/pallets/flask.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install networkx arango langgraph langchain_openai langchain_community matplotlib scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T04:37:33.888301Z",
     "iopub.status.busy": "2025-03-01T04:37:33.887937Z",
     "iopub.status.idle": "2025-03-01T04:37:38.974728Z",
     "shell.execute_reply": "2025-03-01T04:37:38.974038Z",
     "shell.execute_reply.started": "2025-03-01T04:37:33.888276Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import ast\n",
    "from typing import Dict, Set, List, Tuple, Optional,Any\n",
    "import json\n",
    "from arango import ArangoClient\n",
    "import nx_arangodb as nxadb\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.graphs import ArangoGraph\n",
    "from langchain_community.chains.graph_qa.arangodb import ArangoGraphQAChain\n",
    "from langchain_core.tools import tool\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T04:38:30.721707Z",
     "iopub.status.busy": "2025-03-01T04:38:30.721365Z",
     "iopub.status.idle": "2025-03-01T04:38:31.163453Z",
     "shell.execute_reply": "2025-03-01T04:38:31.162755Z",
     "shell.execute_reply.started": "2025-03-01T04:38:30.721684Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import ast\n",
    "# import networkx as nx\n",
    "# from typing import Dict, Set, List\n",
    "# import json\n",
    "# from arango import ArangoClient\n",
    "# os.environ[\"MISTRAL_API_KEY\"]=\"jJAuJZkjVcy2ynUhan375sHNviHiBeJU\"\n",
    "\n",
    "# class CodebaseVisualizer:\n",
    "#     def __init__(self, root_dir: str):\n",
    "#         self.root_dir = root_dir\n",
    "#         self.graph = nx.DiGraph()\n",
    "#         self.file_contents: Dict[str, str] = {}\n",
    "#         self.import_relations: Dict[str, Set[str]] = {}\n",
    "#         self.module_symbols: Dict[str, Dict[str, Dict[str, int]]] = {}  # file -> {symbol -> {type, line_no}}\n",
    "#         self.file_index: Dict[str, int] = {}  # Maps files to indices\n",
    "#         self.current_index = 0\n",
    "#         self.directories: Set[str] = set()\n",
    "\n",
    "#     def _get_next_index(self) -> int:\n",
    "#         \"\"\"Get next available index for file indexing.\"\"\"\n",
    "#         self.current_index += 1\n",
    "#         return self.current_index\n",
    "\n",
    "#     def _chunk_code(self, code: str, lines_per_chunk: int = 20) -> List[Dict]:\n",
    "#         \"\"\"\n",
    "#         Chunk the given code into snippets.\n",
    "#         Returns a list of dictionaries with 'code_snippet', 'start_line', and 'end_line'.\n",
    "#         \"\"\"\n",
    "#         lines = code.splitlines()\n",
    "#         chunks = []\n",
    "#         for i in range(0, len(lines), lines_per_chunk):\n",
    "#             chunk_lines = lines[i:i + lines_per_chunk]\n",
    "#             chunk = {\n",
    "#                 'code_snippet': '\\n'.join(chunk_lines),\n",
    "#                 'start_line': i + 1,\n",
    "#                 'end_line': i + len(chunk_lines)\n",
    "#             }\n",
    "#             chunks.append(chunk)\n",
    "#         return chunks\n",
    "\n",
    "#     def parse_files(self) -> None:\n",
    "#         \"\"\"Parse all Python files in the directory and build relationships.\"\"\"\n",
    "#         # First pass: Index all files and create directory nodes\n",
    "#         for root, dirs, files in os.walk(self.root_dir):\n",
    "#             # Add directory node\n",
    "#             rel_dir = os.path.relpath(root, self.root_dir)\n",
    "#             if rel_dir != '.':\n",
    "#                 self.directories.add(rel_dir)\n",
    "#                 self.graph.add_node(rel_dir, type='directory')\n",
    "\n",
    "#             # Index Python files\n",
    "#             for file in files:\n",
    "#                 if file.endswith('.py'):\n",
    "#                     file_path = os.path.join(root, file)\n",
    "#                     rel_path = os.path.relpath(file_path, self.root_dir)\n",
    "#                     self.file_index[rel_path] = self._get_next_index()\n",
    "                    \n",
    "#                     try:\n",
    "#                         with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#                             content = f.read()\n",
    "#                             self.file_contents[rel_path] = content\n",
    "#                             self._analyze_file(rel_path, content)\n",
    "#                     except Exception as e:\n",
    "#                         print(f\"Error parsing {file_path}: {e}\")\n",
    "\n",
    "#     def _analyze_file(self, file_path: str, content: str) -> None:\n",
    "#         \"\"\"Analyze a single file for imports and symbols with line numbers.\"\"\"\n",
    "#         try:\n",
    "#             tree = ast.parse(content)\n",
    "#             imports = set()\n",
    "#             symbols = {}\n",
    "\n",
    "#             for node in ast.walk(tree):\n",
    "#                 # Track imports\n",
    "#                 if isinstance(node, (ast.Import, ast.ImportFrom)):\n",
    "#                     if isinstance(node, ast.Import):\n",
    "#                         for name in node.names:\n",
    "#                             imports.add((name.name, node.lineno))\n",
    "#                     else:  # ImportFrom\n",
    "#                         module = node.module if node.module else ''\n",
    "#                         imports.add((module, node.lineno))\n",
    "\n",
    "#                 # Track defined symbols with line numbers\n",
    "#                 elif isinstance(node, (ast.FunctionDef, ast.ClassDef)):\n",
    "#                     symbols[node.name] = {\n",
    "#                         'type': 'class' if isinstance(node, ast.ClassDef) else 'function',\n",
    "#                         'line_no': node.lineno\n",
    "#                     }\n",
    "\n",
    "#             self.import_relations[file_path] = imports\n",
    "#             self.module_symbols[file_path] = symbols\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error analyzing {file_path}: {e}\")\n",
    "\n",
    "#     def build_graph(self) -> nx.DiGraph:\n",
    "#         \"\"\"Build the NetworkX graph with enhanced node and edge information.\"\"\"\n",
    "#         # Start with a directed graph for clarity in relationships\n",
    "#         dot_graph = nx.DiGraph()\n",
    "        \n",
    "#         # Add nodes for all files with indices and code snippet nodes\n",
    "#         for file_path, file_idx in self.file_index.items():\n",
    "#             dot_graph.add_node(file_path, \n",
    "#                                type='file',\n",
    "#                                file_index=file_idx,\n",
    "#                                directory=os.path.dirname(file_path))\n",
    "            \n",
    "#             # Create snippet nodes for the entire file\n",
    "#             if file_path in self.file_contents:\n",
    "#                 chunks = self._chunk_code(self.file_contents[file_path])\n",
    "#                 for idx, chunk_info in enumerate(chunks):\n",
    "#                     snippet_node = f\"{file_path}::snippet::{idx}\"\n",
    "#                     dot_graph.add_node(snippet_node,\n",
    "#                                        type='snippet',\n",
    "#                                        code_snippet=chunk_info['code_snippet'],\n",
    "#                                        start_line=chunk_info['start_line'],\n",
    "#                                        end_line=chunk_info['end_line'])\n",
    "#                     # Connect file node to snippet node\n",
    "#                     dot_graph.add_edge(file_path, snippet_node, \n",
    "#                                        edge_type='contains_snippet',\n",
    "#                                        start_line=chunk_info['start_line'],\n",
    "#                                        end_line=chunk_info['end_line'])\n",
    "\n",
    "#             # Add nodes for symbols in this file\n",
    "#             for symbol, details in self.module_symbols.get(file_path, {}).items():\n",
    "#                 symbol_node = f\"{file_path}::{symbol}\"\n",
    "#                 dot_graph.add_node(symbol_node, \n",
    "#                                    type='symbol',\n",
    "#                                    symbol_type=details['type'],\n",
    "#                                    line_number=details['line_no'])\n",
    "#                 dot_graph.add_edge(file_path, symbol_node, \n",
    "#                                    edge_type='defines',\n",
    "#                                    line_number=details['line_no'])\n",
    "\n",
    "#         # Add directory nodes\n",
    "#         for directory in self.directories:\n",
    "#             dot_graph.add_node(directory, type='directory')\n",
    "\n",
    "#         # Add edges for imports with line numbers\n",
    "#         for file_path, imports in self.import_relations.items():\n",
    "#             for imp, line_no in imports:\n",
    "#                 # Look for matching files or symbols\n",
    "#                 for target_file, symbols in self.module_symbols.items():\n",
    "#                     if imp in symbols:\n",
    "#                         dot_graph.add_edge(file_path, \n",
    "#                                            f\"{target_file}::{imp}\",\n",
    "#                                            edge_type='import',\n",
    "#                                            line_number=line_no)\n",
    "#                     elif target_file.replace('.py', '').endswith(imp):\n",
    "#                         dot_graph.add_edge(file_path, \n",
    "#                                            target_file,\n",
    "#                                            edge_type='import',\n",
    "#                                            line_number=line_no)\n",
    "        \n",
    "#         # Save the built graph in self.graph for later export (to ArangoDB, JSON, etc.)\n",
    "#         self.graph = dot_graph\n",
    "#         return dot_graph\n",
    "\n",
    "#     def export_file_index(self, output_path: str = 'file_index.txt') -> None:\n",
    "#         \"\"\"Export the file index mapping to a text file.\"\"\"\n",
    "#         with open(output_path, 'w') as f:\n",
    "#             # Write header\n",
    "#             f.write(\"File Index Mapping\\n\")\n",
    "#             f.write(\"=================\\n\\n\")\n",
    "            \n",
    "#             # Sort by index for better readability\n",
    "#             sorted_items = sorted(self.file_index.items(), key=lambda x: x[1])\n",
    "            \n",
    "#             # Write each file and its index\n",
    "#             for file_path, index in sorted_items:\n",
    "#                 f.write(f\"{index}: {file_path}\\n\")\n",
    "                \n",
    "#                 # If there are symbols in this file, list them with line numbers\n",
    "#                 if file_path in self.module_symbols:\n",
    "#                     f.write(\"  Symbols:\\n\")\n",
    "#                     for symbol, details in self.module_symbols[file_path].items():\n",
    "#                         symbol_type = details['type']\n",
    "#                         line_no = details['line_no']\n",
    "#                         f.write(f\"    - {symbol} ({symbol_type}) at line {line_no}\\n\")\n",
    "#                     f.write(\"\\n\")\n",
    "\n",
    "#     def export_graph_json(self, output_path: str = 'codebase_graph.json') -> None:\n",
    "#         \"\"\"Export the graph structure to JSON.\"\"\"\n",
    "#         graph_data = {\n",
    "#             'nodes': [\n",
    "#                 {\n",
    "#                     'id': node,\n",
    "#                     'type': data['type'],\n",
    "#                     'file_index': data.get('file_index'),\n",
    "#                     'directory': data.get('directory'),\n",
    "#                     'symbol_type': data.get('symbol_type'),\n",
    "#                     'line_number': data.get('line_number'),\n",
    "#                     'code_snippet': data.get('code_snippet'),\n",
    "#                     'start_line': data.get('start_line'),\n",
    "#                     'end_line': data.get('end_line')\n",
    "#                 } \n",
    "#                 for node, data in self.graph.nodes(data=True)\n",
    "#             ],\n",
    "#             'links': [\n",
    "#                 {\n",
    "#                     'source': source,\n",
    "#                     'target': target,\n",
    "#                     'type': data.get('edge_type'),\n",
    "#                     'line_number': data.get('line_number'),\n",
    "#                     'start_line': data.get('start_line'),\n",
    "#                     'end_line': data.get('end_line')\n",
    "#                 } \n",
    "#                 for source, target, data in self.graph.edges(data=True)\n",
    "#             ]\n",
    "#         }\n",
    "        \n",
    "#         with open(output_path, 'w') as f:\n",
    "#             json.dump(graph_data, f, indent=2)\n",
    "#     def export_to_arango(self,\n",
    "#                          db_name: str = 'codebase',\n",
    "#                          username: str = 'root',\n",
    "#                          password: str = 'passwd',\n",
    "#                          host: str = 'http://localhost:8529') -> None:\n",
    "#         \"\"\"\n",
    "#         Export the graph into ArangoDB.\n",
    "#         IMPORTANT: This method first deletes any existing graph and associated data in ArangoDB.\n",
    "#         \"\"\"\n",
    "#         client = ArangoClient(hosts=host)\n",
    "#         db = client.db(username=username, password=password,verify=True)\n",
    "\n",
    "#         # Delete the existing graph and its collections if they exist.\n",
    "#         graph_name = \"FlaskRepv1\"\n",
    "#         '''\n",
    "#         if db.has_graph(graph_name):\n",
    "#             try:\n",
    "#                 # Delete the entire graph and its collections.\n",
    "#                 db.graph(graph_name).delete(delete_collections=True)\n",
    "#                 print(\"Existing ArangoDB graph deleted.\")\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error deleting graph: {e}\")\n",
    "#                 '''\n",
    "        \n",
    "#         G_adb = nxadb.Graph(\n",
    "#             name=graph_name,\n",
    "#             db=db,\n",
    "#             incoming_graph_data=self.graph,\n",
    "#             write_batch_size=50000,\n",
    "#             overwrite_graph=True\n",
    "#         )\n",
    "        \n",
    "#         self.G_adb=G_adb\n",
    "#         return G_adb\n",
    "#         print(\"Graph successfully exported to ArangoDB.\")\n",
    "#     def text_to_nx_algorithm_to_text(self,query):\n",
    "#         \"\"\"This tool is available to invoke a NetworkX Algorithm on\n",
    "#         the ArangoDB Graph. You are responsible for accepting the\n",
    "#         Natural Language Query, establishing which algorithm needs to\n",
    "#         be executed, executing the algorithm, and translating the results back\n",
    "#         to Natural Language, with respect to the original query.\n",
    "    \n",
    "#         If the query (e.g traversals, shortest path, etc.) can be solved using the Arango Query Language, then do not use\n",
    "#         this tool.\n",
    "#         \"\"\"\n",
    "#         llm = ChatMistralAI(\n",
    "#             model=\"mistral-large-latest\",\n",
    "#             temperature=0,\n",
    "#             max_retries=2,\n",
    "#             # other params...\n",
    "#         )\n",
    "#         ######################\n",
    "#         print(\"1) Generating NetworkX code\")\n",
    "    \n",
    "#         text_to_nx = llm.invoke(f\"\"\"\n",
    "#         I have a NetworkX Graph called `G_adb`. It has the following schema: {arango_graph.schema}\n",
    "    \n",
    "#         I have the following graph analysis query: {query}.\n",
    "    \n",
    "#         Generate the Python Code required to answer the query using the `G_adb` object.\n",
    "        \n",
    "#         It should give code so that is gives ALL the necessary contents that use this part of the code in terms of ALL nested imports. Consider the nested directories and sub-directory informations as well.\n",
    "    \n",
    "#         Be very precise on the NetworkX algorithm you select to answer this query. Think step by step.\n",
    "    \n",
    "#         Only assume that networkx is installed, and other base python dependencies.\n",
    "    \n",
    "#         Always set the last variable as `FINAL_RESULT`, which represents the answer to the original query.\n",
    "    \n",
    "#         Only provide python code that I can directly execute via `exec()`. Do not provide any instructions.\n",
    "    \n",
    "#         Make sure that `FINAL_RESULT` stores a short & consice answer. Avoid setting this variable to a long sequence.\n",
    "    \n",
    "#         Your code:\n",
    "#         \"\"\").content\n",
    "    \n",
    "#         text_to_nx_cleaned = re.sub(r\"^```python\\n|```$\", \"\", text_to_nx, flags=re.MULTILINE).strip()\n",
    "        \n",
    "#         print('-'*10)\n",
    "#         print(text_to_nx_cleaned)\n",
    "#         print('-'*10)\n",
    "    \n",
    "#         ######################\n",
    "    \n",
    "#         print(\"\\n2) Executing NetworkX code\")\n",
    "#         global_vars = {\"G_adb\":self.G_adb , \"nx\": nx}\n",
    "#         local_vars = {}\n",
    "    \n",
    "#         try:\n",
    "#             exec(text_to_nx_cleaned, global_vars, local_vars)\n",
    "#             text_to_nx_final = text_to_nx\n",
    "#         except Exception as e:\n",
    "#             print(f\"EXEC ERROR: {e}\")\n",
    "#             return f\"EXEC ERROR: {e}\"\n",
    "    \n",
    "#             # TODO: Consider experimenting with a code corrector!\n",
    "#             attempt = 1\n",
    "#             MAX_ATTEMPTS = 3\n",
    "    \n",
    "#             # while attempt <= MAX_ATTEMPTS\n",
    "#                 # ...\n",
    "    \n",
    "#         print('-'*10)\n",
    "#         FINAL_RESULT = local_vars[\"FINAL_RESULT\"]\n",
    "#         print(f\"FINAL_RESULT: {FINAL_RESULT}\")\n",
    "#         print('-'*10)\n",
    "    \n",
    "#         ######################\n",
    "    \n",
    "#         print(\"3) Formulating final answer\")\n",
    "    \n",
    "#         nx_to_text = llm.invoke(f\"\"\"\n",
    "#             I have a NetworkX Graph called `G_adb`. It has the following schema: {arango_graph.schema}\n",
    "    \n",
    "#             I have the following graph analysis query: {query}.\n",
    "    \n",
    "#             I have executed the following python code to help me answer my query:\n",
    "    \n",
    "#             ---\n",
    "#             {text_to_nx_final}\n",
    "#             ---\n",
    "    \n",
    "#             The `FINAL_RESULT` variable is set to the following: {FINAL_RESULT}.\n",
    "    \n",
    "#             Based on my original Query and FINAL_RESULT, generate a short and concise response to\n",
    "#             answer my query.\n",
    "            \n",
    "#             Your response:\n",
    "#         \"\"\").content\n",
    "    \n",
    "#         return nx_to_text\n",
    "\n",
    "# # Example usage\n",
    "\n",
    "# # Initialize and parse the codebase\n",
    "# visualizer = CodebaseVisualizer(\"flask\")\n",
    "# visualizer.parse_files()\n",
    "# visualizer.build_graph()\n",
    "    \n",
    "# # Export the file index and JSON (for local inspection)\n",
    "# visualizer.export_file_index()\n",
    "# visualizer.export_graph_json()\n",
    "# # Export the enriched graph to ArangoDB (this will delete any existing graph data first)\n",
    "# #visualizer.export_to_arango(db_name=\"FlaskRepv1\",username=\"root\",password=\"cUZ0YaNdcwfUTw6VjRny\",host=\"http://localhost:8529\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T04:38:34.048801Z",
     "iopub.status.busy": "2025-03-01T04:38:34.048501Z",
     "iopub.status.idle": "2025-03-01T04:38:59.455318Z",
     "shell.execute_reply": "2025-03-01T04:38:59.454208Z",
     "shell.execute_reply.started": "2025-03-01T04:38:34.048779Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#visualizer.export_to_arango(db_name=\"FlaskRepv1\",username=\"root\",password=\"cUZ0YaNdcwfUTw6VjRny\",host=\"https://d2eeb8083350.arangodb.cloud:8529\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T04:39:43.873809Z",
     "iopub.status.busy": "2025-03-01T04:39:43.873462Z",
     "iopub.status.idle": "2025-03-01T04:39:44.082162Z",
     "shell.execute_reply": "2025-03-01T04:39:44.081407Z",
     "shell.execute_reply.started": "2025-03-01T04:39:43.873780Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "visualizer = CodebaseVisualizer(\"flask\")\n",
    "visualizer.parse_files()\n",
    "G = visualizer.build_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T04:39:46.921588Z",
     "iopub.status.busy": "2025-03-01T04:39:46.921252Z",
     "iopub.status.idle": "2025-03-01T04:40:44.405541Z",
     "shell.execute_reply": "2025-03-01T04:40:44.404664Z",
     "shell.execute_reply.started": "2025-03-01T04:39:46.921552Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class GraphVisualizer:\n",
    "    def __init__(self, graph: nx.Graph):\n",
    "        self.graph = graph\n",
    "        self.pos = None\n",
    "        \n",
    "    def set_layout(self, layout_type: str = 'spring', **layout_params) -> None:\n",
    "        \"\"\"\n",
    "        Set the layout for the graph visualization.\n",
    "        \n",
    "        Args:\n",
    "            layout_type: Type of layout ('spring', 'circular', 'kamada_kawai', \n",
    "                        'random', 'shell', 'spectral')\n",
    "            layout_params: Additional parameters for the layout algorithm\n",
    "        \"\"\"\n",
    "        layout_funcs = {\n",
    "            'spring': nx.spring_layout,\n",
    "            'circular': nx.circular_layout,\n",
    "            'kamada_kawai': nx.kamada_kawai_layout,\n",
    "            'random': nx.random_layout,\n",
    "            'shell': nx.shell_layout,\n",
    "            'spectral': nx.spectral_layout\n",
    "        }\n",
    "        \n",
    "        if layout_type not in layout_funcs:\n",
    "            raise ValueError(f\"Unsupported layout type. Choose from: {list(layout_funcs.keys())}\")\n",
    "            \n",
    "        self.pos = layout_funcs[layout_type](self.graph, **layout_params)\n",
    "    \n",
    "    def _get_node_colors(self) -> Dict[str, str]:\n",
    "        \"\"\"Extract node colors from graph attributes or generate defaults.\"\"\"\n",
    "        colors = {}\n",
    "        for node in self.graph.nodes():\n",
    "            # Check for color in node attributes\n",
    "            attrs = self.graph.nodes[node]\n",
    "            if 'fillcolor' in attrs:\n",
    "                colors[node] = attrs['fillcolor']\n",
    "            elif 'color' in attrs:\n",
    "                colors[node] = attrs['color']\n",
    "            else:\n",
    "                colors[node] = 'lightblue'  # default color\n",
    "        return colors\n",
    "    \n",
    "    def _get_node_sizes(self) -> Dict[str, float]:\n",
    "        \"\"\"Extract or compute node sizes.\"\"\"\n",
    "        sizes = {}\n",
    "        for node in self.graph.nodes():\n",
    "            attrs = self.graph.nodes[node]\n",
    "            if 'size' in attrs:\n",
    "                sizes[node] = attrs['size']\n",
    "            else:\n",
    "                # Default size based on node degree\n",
    "                sizes[node] = 1000 * (1 + self.graph.degree(node) / 10)\n",
    "        return sizes\n",
    "    \n",
    "    def _get_edge_colors(self) -> Dict[Tuple[str, str], str]:\n",
    "        \"\"\"Extract edge colors from graph attributes or generate defaults.\"\"\"\n",
    "        colors = {}\n",
    "        for u, v in self.graph.edges():\n",
    "            edge_data = self.graph.get_edge_data(u, v)\n",
    "            if 'color' in edge_data:\n",
    "                colors[(u, v)] = edge_data['color']\n",
    "            else:\n",
    "                colors[(u, v)] = 'gray'  # default color\n",
    "        return colors\n",
    "    \n",
    "    def _get_node_labels(self) -> Dict[str, str]:\n",
    "        \"\"\"Extract node labels from graph attributes.\"\"\"\n",
    "        labels = {}\n",
    "        for node in self.graph.nodes():\n",
    "            attrs = self.graph.nodes[node]\n",
    "            if 'label' in attrs:\n",
    "                labels[node] = attrs['label']\n",
    "            else:\n",
    "                labels[node] = str(node)\n",
    "        return labels\n",
    "    \n",
    "    def _get_edge_labels(self) -> Dict[Tuple[str, str], str]:\n",
    "        \"\"\"Extract edge labels from graph attributes.\"\"\"\n",
    "        labels = {}\n",
    "        for u, v in self.graph.edges():\n",
    "            edge_data = self.graph.get_edge_data(u, v)\n",
    "            if 'label' in edge_data:\n",
    "                labels[(u, v)] = edge_data['label']\n",
    "        return labels\n",
    "\n",
    "    def visualize(self, \n",
    "                 figsize: Tuple[int, int] = (12, 8),\n",
    "                 node_size: Optional[Dict[str, float]] = None,\n",
    "                 node_color: Optional[Dict[str, str]] = None,\n",
    "                 edge_color: Optional[Dict[Tuple[str, str], str]] = None,\n",
    "                 with_labels: bool = True,\n",
    "                 font_size: int = 8,\n",
    "                 title: Optional[str] = None,\n",
    "                 show_edge_labels: bool = True,\n",
    "                 alpha: float = 0.7,\n",
    "                 save_path: Optional[str] = None) -> None:\n",
    "        \"\"\"\n",
    "        Visualize the graph with customizable options.\n",
    "        \n",
    "        Args:\n",
    "            figsize: Size of the figure (width, height)\n",
    "            node_size: Dictionary mapping nodes to their sizes\n",
    "            node_color: Dictionary mapping nodes to their colors\n",
    "            edge_color: Dictionary mapping edges to their colors\n",
    "            with_labels: Whether to show node labels\n",
    "            font_size: Size of the font for labels\n",
    "            title: Title of the graph\n",
    "            show_edge_labels: Whether to show edge labels\n",
    "            alpha: Transparency of nodes\n",
    "            save_path: Path to save the visualization (if None, displays instead)\n",
    "        \"\"\"\n",
    "        if self.pos is None:\n",
    "            self.set_layout('spring')\n",
    "            \n",
    "        plt.figure(figsize=figsize)\n",
    "        \n",
    "        # Get or use provided node attributes\n",
    "        node_colors = node_color if node_color is not None else self._get_node_colors()\n",
    "        node_sizes = node_size if node_size is not None else self._get_node_sizes()\n",
    "        edge_colors = edge_color if edge_color is not None else self._get_edge_colors()\n",
    "        \n",
    "        # Draw nodes\n",
    "        nx.draw_networkx_nodes(self.graph, self.pos,\n",
    "                             node_color=[node_colors[node] for node in self.graph.nodes()],\n",
    "                             node_size=[node_sizes[node] for node in self.graph.nodes()],\n",
    "                             alpha=alpha)\n",
    "        \n",
    "        # Draw edges\n",
    "        for (u, v) in self.graph.edges():\n",
    "            nx.draw_networkx_edges(self.graph, self.pos,\n",
    "                                 edgelist=[(u, v)],\n",
    "                                 edge_color=edge_colors.get((u, v), 'gray'),\n",
    "                                 alpha=0.5)\n",
    "        \n",
    "        # Add labels if requested\n",
    "        if with_labels:\n",
    "            labels = self._get_node_labels()\n",
    "            nx.draw_networkx_labels(self.graph, self.pos, labels,\n",
    "                                  font_size=font_size)\n",
    "        \n",
    "        # Add edge labels if requested\n",
    "        if show_edge_labels:\n",
    "            edge_labels = self._get_edge_labels()\n",
    "            if edge_labels:\n",
    "                nx.draw_networkx_edge_labels(self.graph, self.pos,\n",
    "                                           edge_labels=edge_labels,\n",
    "                                           font_size=font_size-2)\n",
    "        \n",
    "        if title:\n",
    "            plt.title(title)\n",
    "        \n",
    "        plt.axis('off')\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "# Example usage:\n",
    "'''\n",
    "# Create a sample graph\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from([\n",
    "    (1, {'fillcolor': 'lightblue', 'label': 'Node 1'}),\n",
    "    (2, {'fillcolor': 'lightgreen', 'label': 'Node 2'}),\n",
    "    (3, {'fillcolor': 'lightred', 'label': 'Node 3'})\n",
    "])\n",
    "G.add_edges_from([\n",
    "    (1, 2, {'color': 'blue', 'label': 'Edge 1-2'}),\n",
    "    (2, 3, {'color': 'red', 'label': 'Edge 2-3'})\n",
    "])\n",
    "'''\n",
    "# Create visualizer and display graph\n",
    "visualizer = GraphVisualizer(G)\n",
    "visualizer.set_layout('spring', k=2)  # k controls the spacing between nodes\n",
    "visualizer.visualize(\n",
    "    figsize=(10, 8),\n",
    "    font_size=10,\n",
    "    title=\"Sample Graph Visualization\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T04:43:05.876305Z",
     "iopub.status.busy": "2025-03-01T04:43:05.875977Z",
     "iopub.status.idle": "2025-03-01T04:43:06.776671Z",
     "shell.execute_reply": "2025-03-01T04:43:06.775886Z",
     "shell.execute_reply.started": "2025-03-01T04:43:05.876281Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "db = ArangoClient(hosts=\"https://d2eeb8083350.arangodb.cloud:8529\").db(username=\"root\", password=\"cUZ0YaNdcwfUTw6VjRny\", verify=True)\n",
    "\n",
    "print(db)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T04:45:32.004484Z",
     "iopub.status.busy": "2025-03-01T04:45:32.004188Z",
     "iopub.status.idle": "2025-03-01T04:45:35.899436Z",
     "shell.execute_reply": "2025-03-01T04:45:35.898520Z",
     "shell.execute_reply.started": "2025-03-01T04:45:32.004463Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "G_adb = nxadb.Graph(\n",
    "    name=\"FlaskRepv1\",\n",
    "    db=db,\n",
    "    incoming_graph_data=G,\n",
    "    write_batch_size=50000 # feel free to modify\n",
    ")\n",
    "\n",
    "print(G_adb)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run from loaded database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T12:32:04.625116Z",
     "iopub.status.busy": "2025-02-23T12:32:04.624787Z",
     "iopub.status.idle": "2025-02-23T12:32:05.367964Z",
     "shell.execute_reply": "2025-02-23T12:32:05.367236Z",
     "shell.execute_reply.started": "2025-02-23T12:32:04.625091Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "db = ArangoClient(hosts=\"https://d2eeb8083350.arangodb.cloud:8529\").db(username=\"root\", password=\"cUZ0YaNdcwfUTw6VjRny\", verify=True)\n",
    "\n",
    "print(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "G_adb = nxadb.Graph(\n",
    "    name=\"FlaskRepv1\",\n",
    "    db=db,\n",
    "    #incoming_graph_data=G,\n",
    "    #write_batch_size=50000 # feel free to modify\n",
    ")\n",
    "\n",
    "print(G_adb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T04:43:20.247877Z",
     "iopub.status.busy": "2025-03-01T04:43:20.247588Z",
     "iopub.status.idle": "2025-03-01T04:43:23.818006Z",
     "shell.execute_reply": "2025-03-01T04:43:23.817097Z",
     "shell.execute_reply.started": "2025-03-01T04:43:20.247857Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "arango_graph = ArangoGraph(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T04:43:25.534742Z",
     "iopub.status.busy": "2025-03-01T04:43:25.534423Z",
     "iopub.status.idle": "2025-03-01T04:43:25.538505Z",
     "shell.execute_reply": "2025-03-01T04:43:25.537663Z",
     "shell.execute_reply.started": "2025-03-01T04:43:25.534715Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"MISTRAL_API_KEY\"]=\"jJAuJZkjVcy2ynUhan375sHNviHiBeJU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T04:43:27.851350Z",
     "iopub.status.busy": "2025-03-01T04:43:27.851042Z",
     "iopub.status.idle": "2025-03-01T04:43:29.376117Z",
     "shell.execute_reply": "2025-03-01T04:43:29.375233Z",
     "shell.execute_reply.started": "2025-03-01T04:43:27.851326Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain_mistralai import ChatMistralAI\n",
    "\n",
    "llm = ChatMistralAI(\n",
    "    model=\"mistral-large-latest\",\n",
    "    temperature=0,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "llm.invoke(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T04:43:31.747340Z",
     "iopub.status.busy": "2025-03-01T04:43:31.747045Z",
     "iopub.status.idle": "2025-03-01T04:43:31.757820Z",
     "shell.execute_reply": "2025-03-01T04:43:31.756946Z",
     "shell.execute_reply.started": "2025-03-01T04:43:31.747317Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "@tool\n",
    "def text_to_nx_algorithm_to_text(query):\n",
    "    \"\"\"This tool is available to invoke a NetworkX Algorithm on\n",
    "    the ArangoDB Graph. You are responsible for accepting the\n",
    "    Natural Language Query, establishing which algorithm needs to\n",
    "    be executed, executing the algorithm, and translating the results back\n",
    "    to Natural Language, with respect to the original query.\n",
    "\n",
    "    If the query (e.g traversals, shortest path, etc.) can be solved using the Arango Query Language, then do not use\n",
    "    this tool.\n",
    "    \"\"\"\n",
    "    llm = ChatMistralAI(\n",
    "        model=\"mistral-large-latest\",\n",
    "        temperature=0,\n",
    "        max_retries=2,\n",
    "        # other params...\n",
    "    )\n",
    "    ######################\n",
    "    print(\"1) Generating NetworkX code\")\n",
    "\n",
    "    text_to_nx = llm.invoke(f\"\"\"\n",
    "    I have a NetworkX Graph called `G_adb`. It has the following schema: {arango_graph.schema}\n",
    "\n",
    "    I have the following graph analysis query: {query}.\n",
    "\n",
    "    Generate the Python Code required to answer the query using the `G_adb` object.\n",
    "    \n",
    "    It should give code so that is gives ALL the necessary contents that use this part of the code in terms of ALL nested imports. Consider the nested directories and sub-directory informations as well.\n",
    "\n",
    "    Be very precise on the NetworkX algorithm you select to answer this query. Think step by step.\n",
    "\n",
    "    Only assume that networkx is installed, and other base python dependencies.\n",
    "\n",
    "    Always set the last variable as `FINAL_RESULT`, which represents the answer to the original query.\n",
    "\n",
    "    Only provide python code that I can directly execute via `exec()`. Do not provide any instructions.\n",
    "\n",
    "    Make sure that `FINAL_RESULT` stores a short & consice answer. Avoid setting this variable to a long sequence.\n",
    "\n",
    "    Your code:\n",
    "    \"\"\").content\n",
    "\n",
    "    text_to_nx_cleaned = re.sub(r\"^```python\\n|```$\", \"\", text_to_nx, flags=re.MULTILINE).strip()\n",
    "    \n",
    "    print('-'*10)\n",
    "    print(text_to_nx_cleaned)\n",
    "    print('-'*10)\n",
    "\n",
    "    ######################\n",
    "\n",
    "    print(\"\\n2) Executing NetworkX code\")\n",
    "    global_vars = {\"G_adb\": G_adb, \"nx\": nx}\n",
    "    local_vars = {}\n",
    "\n",
    "    try:\n",
    "        exec(text_to_nx_cleaned, global_vars, local_vars)\n",
    "        text_to_nx_final = text_to_nx\n",
    "    except Exception as e:\n",
    "        print(f\"EXEC ERROR: {e}\")\n",
    "        return f\"EXEC ERROR: {e}\"\n",
    "\n",
    "        # TODO: Consider experimenting with a code corrector!\n",
    "        attempt = 1\n",
    "        MAX_ATTEMPTS = 3\n",
    "\n",
    "        # while attempt <= MAX_ATTEMPTS\n",
    "            # ...\n",
    "\n",
    "    print('-'*10)\n",
    "    FINAL_RESULT = local_vars[\"FINAL_RESULT\"]\n",
    "    print(f\"FINAL_RESULT: {FINAL_RESULT}\")\n",
    "    print('-'*10)\n",
    "\n",
    "    ######################\n",
    "\n",
    "    print(\"3) Formulating final answer\")\n",
    "\n",
    "    nx_to_text = llm.invoke(f\"\"\"\n",
    "        I have a NetworkX Graph called `G_adb`. It has the following schema: {arango_graph.schema}\n",
    "\n",
    "        I have the following graph analysis query: {query}.\n",
    "\n",
    "        I have executed the following python code to help me answer my query:\n",
    "\n",
    "        ---\n",
    "        {text_to_nx_final}\n",
    "        ---\n",
    "\n",
    "        The `FINAL_RESULT` variable is set to the following: {FINAL_RESULT}.\n",
    "\n",
    "        Based on my original Query and FINAL_RESULT, generate a short and concise response to\n",
    "        answer my query.\n",
    "        \n",
    "        Your response:\n",
    "    \"\"\").content\n",
    "\n",
    "    return nx_to_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T04:45:41.453078Z",
     "iopub.status.busy": "2025-03-01T04:45:41.452762Z",
     "iopub.status.idle": "2025-03-01T04:45:58.787334Z",
     "shell.execute_reply": "2025-03-01T04:45:58.786559Z",
     "shell.execute_reply.started": "2025-03-01T04:45:41.453055Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "text_to_nx_algorithm_to_text(\"What is the use of has_level_handler in the codebase. Give appropriate context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T04:47:01.096433Z",
     "iopub.status.busy": "2025-03-01T04:47:01.096097Z",
     "iopub.status.idle": "2025-03-01T04:47:02.298827Z",
     "shell.execute_reply": "2025-03-01T04:47:02.298082Z",
     "shell.execute_reply.started": "2025-03-01T04:47:01.096408Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(G_adb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T04:47:03.420389Z",
     "iopub.status.busy": "2025-03-01T04:47:03.420081Z",
     "iopub.status.idle": "2025-03-01T04:47:03.426586Z",
     "shell.execute_reply": "2025-03-01T04:47:03.425816Z",
     "shell.execute_reply.started": "2025-03-01T04:47:03.420367Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "arango_graph.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import networkx as nx\n",
    "from typing import Dict, Set, List, Tuple, Optional\n",
    "import json\n",
    "from arango import ArangoClient\n",
    "import re\n",
    "\n",
    "class CodebaseVisualizer:\n",
    "    def __init__(self, root_dir: str):\n",
    "        self.root_dir = root_dir\n",
    "        self.graph = nx.DiGraph()\n",
    "        self.file_contents: Dict[str, str] = {}\n",
    "        self.import_relations: Dict[str, List[Tuple[str, int]]] = {}  # file -> [(module, line_no)]\n",
    "        self.module_symbols: Dict[str, Dict[str, Dict[str, any]]] = {}  # file -> {symbol -> {type, line_no, context}}\n",
    "        self.symbol_references: Dict[str, List[Tuple[str, int]]] = {}  # symbol -> [(file, line_no)]\n",
    "        self.file_index: Dict[str, int] = {}  # Maps files to indices\n",
    "        self.current_index = 0\n",
    "        self.directories: Set[str] = set()\n",
    "\n",
    "    def _get_next_index(self) -> int:\n",
    "        \"\"\"Get next available index for file indexing.\"\"\"\n",
    "        self.current_index += 1\n",
    "        return self.current_index\n",
    "\n",
    "    def _chunk_code(self, code: str, lines_per_chunk: int = 20) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Chunk the given code into snippets.\n",
    "        Returns a list of dictionaries with 'code_snippet', 'start_line', and 'end_line'.\n",
    "        \"\"\"\n",
    "        lines = code.splitlines()\n",
    "        chunks = []\n",
    "        for i in range(0, len(lines), lines_per_chunk):\n",
    "            chunk_lines = lines[i:i + lines_per_chunk]\n",
    "            chunk = {\n",
    "                'code_snippet': '\\n'.join(chunk_lines),\n",
    "                'start_line': i + 1,\n",
    "                'end_line': i + len(chunk_lines)\n",
    "            }\n",
    "            chunks.append(chunk)\n",
    "        return chunks\n",
    "\n",
    "    def _get_context_around_line(self, file_path: str, line_no: int, context_lines: int = 3) -> str:\n",
    "        \"\"\"Extract context around a specific line in a file.\"\"\"\n",
    "        if file_path not in self.file_contents:\n",
    "            return \"\"\n",
    "        \n",
    "        lines = self.file_contents[file_path].splitlines()\n",
    "        start = max(0, line_no - context_lines - 1)\n",
    "        end = min(len(lines), line_no + context_lines)\n",
    "        \n",
    "        context = \"\\n\".join(lines[start:end])\n",
    "        return context\n",
    "\n",
    "    def parse_files(self) -> None:\n",
    "        \"\"\"Parse all Python files in the directory and build relationships.\"\"\"\n",
    "        # First pass: Index all files and create directory nodes\n",
    "        for root, dirs, files in os.walk(self.root_dir):\n",
    "            # Add directory node\n",
    "            rel_dir = os.path.relpath(root, self.root_dir)\n",
    "            if rel_dir != '.':\n",
    "                self.directories.add(rel_dir)\n",
    "                self.graph.add_node(rel_dir, type='directory')\n",
    "\n",
    "            # Index Python files\n",
    "            for file in files:\n",
    "                if file.endswith('.py'):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    rel_path = os.path.relpath(file_path, self.root_dir)\n",
    "                    self.file_index[rel_path] = self._get_next_index()\n",
    "                    \n",
    "                    try:\n",
    "                        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                            content = f.read()\n",
    "                            self.file_contents[rel_path] = content\n",
    "                            self._analyze_file(rel_path, content)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error parsing {file_path}: {e}\")\n",
    "        \n",
    "        # Second pass: Find symbol references across files\n",
    "        self._find_symbol_references()\n",
    "\n",
    "    def _analyze_file(self, file_path: str, content: str) -> None:\n",
    "        \"\"\"Analyze a single file for imports and symbols with line numbers and context.\"\"\"\n",
    "        try:\n",
    "            tree = ast.parse(content)\n",
    "            imports = []\n",
    "            symbols = {}\n",
    "\n",
    "            for node in ast.walk(tree):\n",
    "                # Track imports\n",
    "                if isinstance(node, (ast.Import, ast.ImportFrom)):\n",
    "                    if isinstance(node, ast.Import):\n",
    "                        for name in node.names:\n",
    "                            imports.append((name.name, node.lineno))\n",
    "                    else:  # ImportFrom\n",
    "                        module = node.module if node.module else ''\n",
    "                        for name in node.names:\n",
    "                            imports.append((f\"{module}.{name.name}\" if module else name.name, node.lineno))\n",
    "\n",
    "                # Track defined symbols with line numbers and context\n",
    "                elif isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.Assign)):\n",
    "                    if isinstance(node, (ast.FunctionDef, ast.ClassDef)):\n",
    "                        symbol_name = node.name\n",
    "                        symbol_type = 'class' if isinstance(node, ast.ClassDef) else 'function'\n",
    "                        line_no = node.lineno\n",
    "                        context = self._extract_node_source(content, node)\n",
    "                        \n",
    "                        symbols[symbol_name] = {\n",
    "                            'type': symbol_type,\n",
    "                            'line_no': line_no,\n",
    "                            'context': context,\n",
    "                            'docstring': ast.get_docstring(node)\n",
    "                        }\n",
    "                    elif isinstance(node, ast.Assign):\n",
    "                        # Handle variable assignments\n",
    "                        for target in node.targets:\n",
    "                            if isinstance(target, ast.Name):\n",
    "                                symbol_name = target.id\n",
    "                                line_no = node.lineno\n",
    "                                context = self._extract_node_source(content, node)\n",
    "                                \n",
    "                                symbols[symbol_name] = {\n",
    "                                    'type': 'variable',\n",
    "                                    'line_no': line_no,\n",
    "                                    'context': context\n",
    "                                }\n",
    "\n",
    "            self.import_relations[file_path] = imports\n",
    "            self.module_symbols[file_path] = symbols\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing {file_path}: {e}\")\n",
    "\n",
    "    def _extract_node_source(self, source: str, node) -> str:\n",
    "        \"\"\"Extract the source code for an AST node.\"\"\"\n",
    "        try:\n",
    "            lines = source.splitlines()\n",
    "            if hasattr(node, 'lineno') and hasattr(node, 'end_lineno'):\n",
    "                start = node.lineno - 1\n",
    "                end = getattr(node, 'end_lineno', start + 1)\n",
    "                return '\\n'.join(lines[start:end])\n",
    "            return \"\"\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "\n",
    "    def _find_symbol_references(self) -> None:\n",
    "        \"\"\"Find references to symbols across all files.\"\"\"\n",
    "        for file_path, content in self.file_contents.items():\n",
    "            try:\n",
    "                tree = ast.parse(content)\n",
    "                self._process_file_for_references(file_path, tree, content)\n",
    "            except Exception as e:\n",
    "                print(f\"Error finding references in {file_path}: {e}\")\n",
    "\n",
    "    def _process_file_for_references(self, file_path: str, tree, source: str) -> None:\n",
    "        \"\"\"Process a file's AST to find references to symbols.\"\"\"\n",
    "        for node in ast.walk(tree):\n",
    "            # Find variable references\n",
    "            if isinstance(node, ast.Name) and isinstance(node.ctx, ast.Load):\n",
    "                symbol_name = node.id\n",
    "                line_no = node.lineno\n",
    "                \n",
    "                # Track reference with context\n",
    "                if symbol_name not in self.symbol_references:\n",
    "                    self.symbol_references[symbol_name] = []\n",
    "                \n",
    "                context = self._get_context_around_line(file_path, line_no)\n",
    "                self.symbol_references[symbol_name].append((file_path, line_no, context))\n",
    "            \n",
    "            # Find attribute references (e.g., obj.method())\n",
    "            elif isinstance(node, ast.Attribute) and isinstance(node.ctx, ast.Load):\n",
    "                attr_name = node.attr\n",
    "                line_no = node.lineno\n",
    "                \n",
    "                if attr_name not in self.symbol_references:\n",
    "                    self.symbol_references[attr_name] = []\n",
    "                \n",
    "                context = self._get_context_around_line(file_path, line_no)\n",
    "                self.symbol_references[attr_name].append((file_path, line_no, context))\n",
    "\n",
    "    def build_graph(self) -> nx.DiGraph:\n",
    "        \"\"\"Build the NetworkX graph with enhanced node and edge information.\"\"\"\n",
    "        # Start with a directed graph for clarity in relationships\n",
    "        dot_graph = nx.DiGraph()\n",
    "        \n",
    "        # Add nodes for all files with indices and code snippet nodes\n",
    "        for file_path, file_idx in self.file_index.items():\n",
    "            dot_graph.add_node(file_path, \n",
    "                               type='file',\n",
    "                               file_index=file_idx,\n",
    "                               directory=os.path.dirname(file_path))\n",
    "            \n",
    "            # Create snippet nodes for the entire file\n",
    "            if file_path in self.file_contents:\n",
    "                chunks = self._chunk_code(self.file_contents[file_path])\n",
    "                for idx, chunk_info in enumerate(chunks):\n",
    "                    snippet_node = f\"{file_path}::snippet::{idx}\"\n",
    "                    dot_graph.add_node(snippet_node,\n",
    "                                       type='snippet',\n",
    "                                       code_snippet=chunk_info['code_snippet'],\n",
    "                                       start_line=chunk_info['start_line'],\n",
    "                                       end_line=chunk_info['end_line'])\n",
    "                    # Connect file node to snippet node\n",
    "                    dot_graph.add_edge(file_path, snippet_node, \n",
    "                                       edge_type='contains_snippet',\n",
    "                                       start_line=chunk_info['start_line'],\n",
    "                                       end_line=chunk_info['end_line'])\n",
    "\n",
    "            # Add nodes for symbols in this file\n",
    "            for symbol, details in self.module_symbols.get(file_path, {}).items():\n",
    "                symbol_node = f\"{file_path}::{symbol}\"\n",
    "                dot_graph.add_node(symbol_node, \n",
    "                                   type='symbol',\n",
    "                                   symbol_type=details['type'],\n",
    "                                   line_number=details['line_no'],\n",
    "                                   context=details.get('context', ''),\n",
    "                                   docstring=details.get('docstring', ''))\n",
    "                dot_graph.add_edge(file_path, symbol_node, \n",
    "                                   edge_type='defines',\n",
    "                                   line_number=details['line_no'])\n",
    "\n",
    "        # Add directory nodes\n",
    "        for directory in self.directories:\n",
    "            dot_graph.add_node(directory, type='directory')\n",
    "\n",
    "        # Add edges for imports with line numbers\n",
    "        for file_path, imports in self.import_relations.items():\n",
    "            for imp, line_no in imports:\n",
    "                # Look for matching files or symbols\n",
    "                for target_file, symbols in self.module_symbols.items():\n",
    "                    if imp in symbols:\n",
    "                        dot_graph.add_edge(file_path, \n",
    "                                           f\"{target_file}::{imp}\",\n",
    "                                           edge_type='import',\n",
    "                                           line_number=line_no)\n",
    "                    elif target_file.replace('.py', '').endswith(imp):\n",
    "                        dot_graph.add_edge(file_path, \n",
    "                                           target_file,\n",
    "                                           edge_type='import',\n",
    "                                           line_number=line_no)\n",
    "        \n",
    "        # Add edges for symbol references with line numbers and context\n",
    "        for symbol, references in self.symbol_references.items():\n",
    "            for file_path, symbols in self.module_symbols.items():\n",
    "                if symbol in symbols:\n",
    "                    symbol_node = f\"{file_path}::{symbol}\"\n",
    "                    \n",
    "                    # Connect symbol to all its references\n",
    "                    for ref_file, ref_line, context in references:\n",
    "                        if ref_file != file_path:  # Only add cross-file references\n",
    "                            dot_graph.add_edge(ref_file, \n",
    "                                              symbol_node,\n",
    "                                              edge_type='references',\n",
    "                                              line_number=ref_line,\n",
    "                                              context=context)\n",
    "        \n",
    "        # Save the built graph in self.graph for later export\n",
    "        self.graph = dot_graph\n",
    "        return dot_graph\n",
    "\n",
    "    def find_symbol_usages(self, symbol_name: str) -> List[Dict]:\n",
    "        \"\"\"Find all usages of a symbol in the codebase with context.\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # Look for the symbol in defined symbols\n",
    "        for file_path, symbols in self.module_symbols.items():\n",
    "            if symbol_name in symbols:\n",
    "                details = symbols[symbol_name]\n",
    "                results.append({\n",
    "                    'file': file_path,\n",
    "                    'type': 'definition',\n",
    "                    'line': details['line_no'],\n",
    "                    'symbol_type': details['type'],\n",
    "                    'context': details.get('context', ''),\n",
    "                    'docstring': details.get('docstring', '')\n",
    "                })\n",
    "        \n",
    "        # Look for references to the symbol\n",
    "        if symbol_name in self.symbol_references:\n",
    "            for file_path, line_no, context in self.symbol_references[symbol_name]:\n",
    "                results.append({\n",
    "                    'file': file_path,\n",
    "                    'type': 'reference',\n",
    "                    'line': line_no,\n",
    "                    'context': context\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def export_file_index(self, output_path: str = 'file_index.txt') -> None:\n",
    "        \"\"\"Export the file index mapping to a text file.\"\"\"\n",
    "        with open(output_path, 'w') as f:\n",
    "            # Write header\n",
    "            f.write(\"File Index Mapping\\n\")\n",
    "            f.write(\"=================\\n\\n\")\n",
    "            \n",
    "            # Sort by index for better readability\n",
    "            sorted_items = sorted(self.file_index.items(), key=lambda x: x[1])\n",
    "            \n",
    "            # Write each file and its index\n",
    "            for file_path, index in sorted_items:\n",
    "                f.write(f\"{index}: {file_path}\\n\")\n",
    "                \n",
    "                # If there are symbols in this file, list them with line numbers\n",
    "                if file_path in self.module_symbols:\n",
    "                    f.write(\"  Symbols:\\n\")\n",
    "                    for symbol, details in self.module_symbols[file_path].items():\n",
    "                        symbol_type = details['type']\n",
    "                        line_no = details['line_no']\n",
    "                        f.write(f\"    - {symbol} ({symbol_type}) at line {line_no}\\n\")\n",
    "                    f.write(\"\\n\")\n",
    "\n",
    "    def _analyze_symbol_purpose(self, symbol_name, usages):\n",
    "        \"\"\"Analyze the purpose of a symbol based on its usage patterns.\"\"\"\n",
    "        # Get the definition if available\n",
    "        definitions = [u for u in usages if u['type'] == 'definition']\n",
    "        references = [u for u in usages if u['type'] == 'reference']\n",
    "        \n",
    "        purpose = \"\"\n",
    "        \n",
    "        # Check if we have a definition with docstring\n",
    "        if definitions and definitions[0].get('docstring'):\n",
    "            purpose += f\"{definitions[0]['docstring']}\\n\\n\"\n",
    "        \n",
    "        # If it's a function, try to infer what it does from usage\n",
    "        if definitions and definitions[0].get('symbol_type') == 'function':\n",
    "            # Collect contexts where it's used\n",
    "            contexts = [ref.get('context', '') for ref in references if ref.get('context')]\n",
    "            \n",
    "            # Analyze contexts for patterns\n",
    "            if contexts:\n",
    "                common_patterns = self._find_common_usage_patterns(contexts, symbol_name)\n",
    "                \n",
    "                purpose += \"Based on usage patterns, this function appears to:\\n\"\n",
    "                \n",
    "                if any(\"assert\" in ctx for ctx in contexts):\n",
    "                    purpose += \"- Be used in test assertions to verify behavior\\n\"\n",
    "                \n",
    "                if any(\"if\" in ctx and symbol_name in ctx for ctx in contexts):\n",
    "                    purpose += \"- Be used as a condition in control flow statements\\n\"\n",
    "                \n",
    "                if common_patterns:\n",
    "                    for pattern in common_patterns[:3]:  # Top 3 patterns\n",
    "                        purpose += f\"- {pattern}\\n\"\n",
    "        \n",
    "        # If we couldn't infer much, provide a generic description\n",
    "        if not purpose or len(purpose.strip()) < 10:\n",
    "            if definitions and definitions[0].get('symbol_type') == 'function':\n",
    "                context = definitions[0].get('context', '')\n",
    "                \n",
    "                # Look for parameters to understand what it takes\n",
    "                params = self._extract_function_params(context)\n",
    "                \n",
    "                purpose += f\"This function appears to check or validate something\"\n",
    "                if params:\n",
    "                    purpose += f\" related to {', '.join(params)}\"\n",
    "                purpose += \".\\n\"\n",
    "        \n",
    "        return purpose\n",
    "\n",
    "    def _find_common_usage_patterns(self, contexts, symbol_name):\n",
    "        \"\"\"Find common patterns in the usage contexts.\"\"\"\n",
    "        patterns = []\n",
    "        \n",
    "        # Check if it's used with certain objects/methods frequently\n",
    "        if any(f\".{symbol_name}\" in ctx for ctx in contexts):\n",
    "            patterns.append(\"Be a method called on objects\")\n",
    "        \n",
    "        # Check if it's used for configuration or setup\n",
    "        if any(\"config\" in ctx.lower() or \"setup\" in ctx.lower() for ctx in contexts):\n",
    "            patterns.append(\"Be involved in configuration or setup\")\n",
    "        \n",
    "        # Check if it's used for logging\n",
    "        if any(\"log\" in ctx.lower() for ctx in contexts):\n",
    "            patterns.append(\"Be related to logging functionality\")\n",
    "        \n",
    "        # Check if it's used in exception handling\n",
    "        if any(\"except\" in ctx.lower() or \"try\" in ctx.lower() for ctx in contexts):\n",
    "            patterns.append(\"Be involved in exception handling\")\n",
    "        \n",
    "        return patterns\n",
    "\n",
    "    def _extract_function_params(self, context):\n",
    "        \"\"\"Extract parameter names from a function definition.\"\"\"\n",
    "        params = []\n",
    "        if context:\n",
    "            # Simple regex-based extraction\n",
    "            match = re.search(r'def\\s+\\w+\\s*\\((.*?)\\)', context, re.DOTALL)\n",
    "            if match:\n",
    "                param_string = match.group(1)\n",
    "                # Split by comma and clean up\n",
    "                raw_params = [p.strip() for p in param_string.split(',')]\n",
    "                # Extract just the parameter name (before any : or =)\n",
    "                params = [re.split(r'[=:]', p)[0].strip() for p in raw_params if p]\n",
    "                # Remove self if it's there\n",
    "                if params and params[0] == 'self':\n",
    "                    params = params[1:]\n",
    "        return params\n",
    "\n",
    "    def _general_codebase_analysis(self, query):\n",
    "        \"\"\"Provide a general analysis based on the query.\"\"\"\n",
    "        response = f\"# Analysis for Query: {query}\\n\\n\"\n",
    "        \n",
    "        # Check if the query is asking about structure\n",
    "        if any(term in query.lower() for term in ['structure', 'organization', 'layout']):\n",
    "            response += \"## Codebase Structure\\n\\n\"\n",
    "            # Count files by directory\n",
    "            files_by_dir = {}\n",
    "            for file_path in self.file_index:\n",
    "                directory = os.path.dirname(file_path)\n",
    "                if directory not in files_by_dir:\n",
    "                    files_by_dir[directory] = []\n",
    "                files_by_dir[directory].append(file_path)\n",
    "            \n",
    "            response += f\"The codebase contains {len(self.file_index)} Python files across {len(files_by_dir)} directories.\\n\\n\"\n",
    "            \n",
    "            # Show top-level directories\n",
    "            response += \"Main directories:\\n\"\n",
    "            for directory, files in sorted(files_by_dir.items(), key=lambda x: len(x[1]), reverse=True)[:10]:\n",
    "                dir_name = directory if directory else '(root)'\n",
    "                response += f\"- {dir_name}: {len(files)} files\\n\"\n",
    "        \n",
    "        # Check if the query is asking about specific functionality\n",
    "        functionality_terms = ['handle', 'process', 'create', 'generate', 'calculate']\n",
    "        for term in functionality_terms:\n",
    "            if term in query.lower():\n",
    "                # Search for functions with this term\n",
    "                matching_functions = []\n",
    "                for file_path, symbols in self.module_symbols.items():\n",
    "                    for symbol, details in symbols.items():\n",
    "                        if details['type'] == 'function' and term in symbol.lower():\n",
    "                            matching_functions.append((file_path, symbol, details))\n",
    "                \n",
    "                if matching_functions:\n",
    "                    response += f\"\\n## Functions Related to '{term}'\\n\\n\"\n",
    "                    for file_path, symbol, details in matching_functions[:5]:  # Show top 5\n",
    "                        response += f\"- `{symbol}` in {file_path}:{details['line_no']}\\n\"\n",
    "                    if len(matching_functions) > 5:\n",
    "                        response += f\"... and {len(matching_functions) - 5} more functions\\n\"\n",
    "        \n",
    "        return response\n",
    "\n",
    "    def export_graph_json(self, output_path: str = 'codebase_graph.json') -> None:\n",
    "        \"\"\"Export the graph structure to JSON.\"\"\"\n",
    "        graph_data = {\n",
    "            'nodes': [\n",
    "                {\n",
    "                    'id': node,\n",
    "                    'type': data['type'],\n",
    "                    'file_index': data.get('file_index'),\n",
    "                    'directory': data.get('directory'),\n",
    "                    'symbol_type': data.get('symbol_type'),\n",
    "                    'line_number': data.get('line_number'),\n",
    "                    'code_snippet': data.get('code_snippet'),\n",
    "                    'start_line': data.get('start_line'),\n",
    "                    'end_line': data.get('end_line'),\n",
    "                    'context': data.get('context'),\n",
    "                    'docstring': data.get('docstring')\n",
    "                } \n",
    "                for node, data in self.graph.nodes(data=True)\n",
    "            ],\n",
    "            'links': [\n",
    "                {\n",
    "                    'source': source,\n",
    "                    'target': target,\n",
    "                    'type': data.get('edge_type'),\n",
    "                    'line_number': data.get('line_number'),\n",
    "                    'start_line': data.get('start_line'),\n",
    "                    'end_line': data.get('end_line'),\n",
    "                    'context': data.get('context')\n",
    "                } \n",
    "                for source, target, data in self.graph.edges(data=True)\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(graph_data, f, indent=2)\n",
    "\n",
    "    def export_to_arango(self,\n",
    "                         db_name: str = 'codebase',\n",
    "                         username: str = 'root',\n",
    "                         password: str = 'passwd',\n",
    "                         host: str = 'http://localhost:8529') -> None:\n",
    "        \"\"\"Export the graph into ArangoDB.\"\"\"\n",
    "        client = ArangoClient(hosts=host)\n",
    "        db = client.db(username=username, password=password, verify=True)\n",
    "\n",
    "        # Delete the existing graph and its collections if they exist.\n",
    "        graph_name = \"FlaskRepv1\"\n",
    "        \n",
    "        # Import networkx_to_arangodb if it's available\n",
    "        try:\n",
    "            import networkx_to_arangodb as nxadb # type: ignore\n",
    "            G_adb = nxadb.Graph(\n",
    "                name=graph_name,\n",
    "                db=db,\n",
    "                incoming_graph_data=self.graph,\n",
    "                write_batch_size=50000,\n",
    "                overwrite_graph=True\n",
    "            )\n",
    "            \n",
    "            self.G_adb = G_adb\n",
    "            print(\"Graph successfully exported to ArangoDB.\")\n",
    "            return G_adb\n",
    "        except ImportError:\n",
    "            print(\"networkx_to_arangodb module not found. Please install it to export to ArangoDB.\")\n",
    "            return None\n",
    "\n",
    "    def find_variable_usages(self, variable_name: str) -> dict:\n",
    "        \"\"\"Find all usages of a variable in the codebase with detailed context.\"\"\"\n",
    "        results = {\n",
    "            'definitions': [],\n",
    "            'references': [],\n",
    "            'summary': {}\n",
    "        }\n",
    "        \n",
    "        # Find variable definitions\n",
    "        for file_path, symbols in self.module_symbols.items():\n",
    "            if variable_name in symbols:\n",
    "                details = symbols[variable_name]\n",
    "                if details['type'] == 'variable':\n",
    "                    context_lines = self._get_context_around_line(file_path, details['line_no'])\n",
    "                    results['definitions'].append({\n",
    "                        'file': file_path,\n",
    "                        'line': details['line_no'],\n",
    "                        'context': context_lines\n",
    "                    })\n",
    "        \n",
    "        # Find variable references\n",
    "        if variable_name in self.symbol_references:\n",
    "            for file_path, line_no, context in self.symbol_references[variable_name]:\n",
    "                results['references'].append({\n",
    "                    'file': file_path,\n",
    "                    'line': line_no,\n",
    "                    'context': context\n",
    "                })\n",
    "        \n",
    "        # Generate summary\n",
    "        results['summary'] = {\n",
    "            'definition_count': len(results['definitions']),\n",
    "            'reference_count': len(results['references']),\n",
    "            'files_with_definitions': list(set(d['file'] for d in results['definitions'])),\n",
    "            'files_with_references': list(set(r['file'] for r in results['references']))\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def text_to_nx_algorithm_to_text(self, query):\n",
    "        \"\"\"\n",
    "        Enhanced analysis function that uses the local graph to answer symbol queries\n",
    "        without requiring external API calls.\n",
    "        \"\"\"\n",
    "        # Check if this is a query about a symbol's usage\n",
    "        symbol_pattern = r'(?:find usages? of|what is the use of) ([\\w_]+)'\n",
    "        symbol_match = re.search(symbol_pattern, query.lower())\n",
    "        \n",
    "        if symbol_match:\n",
    "            symbol_name = symbol_match.group(1)\n",
    "            usages = self.find_symbol_usages(symbol_name)\n",
    "            \n",
    "            if not usages:\n",
    "                return f\"No usages found for symbol '{symbol_name}' in the codebase.\"\n",
    "            \n",
    "            # Analyze the usage patterns\n",
    "            definitions = [u for u in usages if u['type'] == 'definition']\n",
    "            references = [u for u in usages if u['type'] == 'reference']\n",
    "            \n",
    "            # Organize by files\n",
    "            files_with_usages = {}\n",
    "            for usage in usages:\n",
    "                file_path = usage['file']\n",
    "                if file_path not in files_with_usages:\n",
    "                    files_with_usages[file_path] = {'definitions': [], 'references': []}\n",
    "                \n",
    "                usage_type = usage['type']\n",
    "                files_with_usages[file_path][usage_type + 's'].append(usage)\n",
    "            \n",
    "            # Start building response\n",
    "            response = f\"# Analysis of '{symbol_name}' in the Codebase\\n\\n\"\n",
    "            \n",
    "            # Add definition section if available\n",
    "            if definitions:\n",
    "                definition = definitions[0]  # Get primary definition\n",
    "                response += f\"## Definition\\n\\n\"\n",
    "                symbol_type = definition.get('symbol_type', 'function')\n",
    "                response += f\"**Type:** {symbol_type}\\n\"\n",
    "                response += f\"**Defined in:** {definition['file']}:{definition['line']}\\n\\n\"\n",
    "                \n",
    "                if definition.get('docstring'):\n",
    "                    response += f\"**Docstring:**\\n```\\n{definition['docstring']}\\n```\\n\\n\"\n",
    "                \n",
    "                if definition.get('context'):\n",
    "                    response += f\"**Implementation:**\\n```python\\n{definition['context']}\\n```\\n\\n\"\n",
    "            \n",
    "            # Analyze purpose based on usage patterns\n",
    "            response += f\"## Purpose and Usage\\n\\n\"\n",
    "            \n",
    "            # Try to infer purpose based on context\n",
    "            purpose_analysis = self._analyze_symbol_purpose(symbol_name, usages)\n",
    "            response += purpose_analysis + \"\\n\\n\"\n",
    "            \n",
    "            # List usage examples\n",
    "            response += f\"## Usage Examples\\n\\n\"\n",
    "            \n",
    "            # Show top 3-5 most representative usage examples\n",
    "            example_count = min(5, len(references))\n",
    "            for i in range(example_count):\n",
    "                if i < len(references):\n",
    "                    ref = references[i]\n",
    "                    response += f\"### Example {i+1}: {ref['file']}:{ref['line']}\\n\\n\"\n",
    "                    if ref.get('context'):\n",
    "                        response += f\"```python\\n{ref['context']}\\n```\\n\\n\"\n",
    "            \n",
    "            # Add summary stats\n",
    "            response += f\"## Summary Statistics\\n\\n\"\n",
    "            response += f\"- Total occurrences: {len(usages)}\\n\"\n",
    "            response += f\"- Definitions: {len(definitions)}\\n\"\n",
    "            response += f\"- References: {len(references)}\\n\"\n",
    "            response += f\"- Files containing this symbol: {len(files_with_usages)}\\n\"\n",
    "            \n",
    "            return response\n",
    "        \n",
    "        # Check if this is a query about a variable's usage\n",
    "        var_pattern = r'find (?:where|how) ([\\w_]+) is (?:used|referenced|defined)'\n",
    "        var_match = re.search(var_pattern, query.lower())\n",
    "        \n",
    "        if var_match:\n",
    "            var_name = var_match.group(1)\n",
    "            usages = self.find_variable_usages(var_name)\n",
    "            \n",
    "            if not usages['definitions'] and not usages['references']:\n",
    "                return f\"No usages found for variable '{var_name}' in the codebase.\"\n",
    "            \n",
    "            # Format a nice response\n",
    "            response = f\"Analysis of variable '{var_name}':\\n\\n\"\n",
    "            \n",
    "            if usages['definitions']:\n",
    "                response += f\"DEFINITIONS ({len(usages['definitions'])}):\\n\"\n",
    "                for def_info in usages['definitions']:\n",
    "                    response += f\"In {def_info['file']}:{def_info['line']}\\n\"\n",
    "                    if def_info.get('context'):\n",
    "                        response += f\"```python\\n{def_info['context']}\\n```\\n\"\n",
    "                response += \"\\n\"\n",
    "            \n",
    "            if usages['references']:\n",
    "                response += f\"REFERENCES ({len(usages['references'])}):\\n\"\n",
    "                for ref_info in usages['references']:\n",
    "                    response += f\"In {ref_info['file']}:{ref_info['line']}\\n\"\n",
    "                    if ref_info.get('context'):\n",
    "                        response += f\"```python\\n{ref_info['context']}\\n```\\n\"\n",
    "            \n",
    "            return response\n",
    "        \n",
    "        # Handle \"what is\" questions with more general analysis\n",
    "        what_is_pattern = r'what is ([\\w_]+)'\n",
    "        what_is_match = re.search(what_is_pattern, query.lower())\n",
    "        \n",
    "        if what_is_match:\n",
    "            item_name = what_is_match.group(1)\n",
    "            \n",
    "            # Try to find it as a symbol\n",
    "            symbol_usages = self.find_symbol_usages(item_name)\n",
    "            \n",
    "            if symbol_usages:\n",
    "                # It's a symbol, use the symbol analysis\n",
    "                return self.text_to_nx_algorithm_to_text(f\"find usages of {item_name}\")\n",
    "            \n",
    "            # Try to find it as a file or directory\n",
    "            file_matches = [f for f in self.file_index.keys() if item_name in f]\n",
    "            dir_matches = [d for d in self.directories if item_name in d]\n",
    "            \n",
    "            if file_matches or dir_matches:\n",
    "                response = f\"# Analysis of '{item_name}' in the Codebase\\n\\n\"\n",
    "                \n",
    "                if file_matches:\n",
    "                    response += f\"## Matching Files\\n\\n\"\n",
    "                    for file in file_matches[:10]:  # Limit to 10 files\n",
    "                        response += f\"- {file}\\n\"\n",
    "                    if len(file_matches) > 10:\n",
    "                        response += f\"... and {len(file_matches) - 10} more files\\n\"\n",
    "                    response += \"\\n\"\n",
    "                \n",
    "                if dir_matches:\n",
    "                    response += f\"## Matching Directories\\n\\n\"\n",
    "                    for directory in dir_matches:\n",
    "                        response += f\"- {directory}\\n\"\n",
    "                    response += \"\\n\"\n",
    "                \n",
    "                return response\n",
    "            \n",
    "            return f\"Could not find '{item_name}' as a symbol, file, or directory in the codebase.\"\n",
    "        \n",
    "        # For any other type of query, perform a more general analysis\n",
    "        return self._general_codebase_analysis(query)\n",
    "\n",
    "    def analyze_has_level_handler(self):\n",
    "        \"\"\"Specialized analysis for has_level_handler in the Flask codebase.\"\"\"\n",
    "        symbol_name = \"has_level_handler\"\n",
    "        usages = self.find_symbol_usages(symbol_name)\n",
    "        \n",
    "        if not usages:\n",
    "            return f\"No usages found for '{symbol_name}' in the codebase.\"\n",
    "        \n",
    "        # Separate definitions and references\n",
    "        definitions = [u for u in usages if u['type'] == 'definition']\n",
    "        references = [u for u in usages if u['type'] == 'reference']\n",
    "        \n",
    "        # Build a comprehensive analysis\n",
    "        response = f\"# Analysis of 'has_level_handler' in Flask\\n\\n\"\n",
    "        \n",
    "        # 1. Add definition and implementation\n",
    "        if definitions:\n",
    "            definition = definitions[0]\n",
    "            response += \"## Definition\\n\\n\"\n",
    "            response += f\"Defined in: {definition['file']}:{definition['line']}\\n\"\n",
    "            \n",
    "            if definition.get('context'):\n",
    "                response += \"\\n```python\\n\" + definition['context'] + \"\\n```\\n\\n\"\n",
    "        \n",
    "        # 2. Explain purpose\n",
    "        response += \"## Purpose\\n\\n\"\n",
    "        response += \"The `has_level_handler` function checks whether a logger has a handler that will handle \"\n",
    "        response += \"messages of a certain level. It's primarily used to determine if a default handler needs \"\n",
    "        response += \"to be added to a logger.\\n\\n\"\n",
    "        \n",
    "        # 3. Analyze usage in logging.py\n",
    "        logging_references = [r for r in references if 'logging.py' in r['file']]\n",
    "        if logging_references:\n",
    "            response += \"## Usage in Flask's Logging System\\n\\n\"\n",
    "            response += \"In Flask's logging module, `has_level_handler` is used to determine whether to add \"\n",
    "            response += \"a default handler to a logger. If the logger doesn't have an appropriate handler, \"\n",
    "            response += \"Flask adds one automatically.\\n\\n\"\n",
    "            \n",
    "            for ref in logging_references:\n",
    "                response += f\"### In {ref['file']}:{ref['line']}\\n\\n\"\n",
    "                if ref.get('context'):\n",
    "                    response += \"```python\\n\" + ref['context'] + \"\\n```\\n\\n\"\n",
    "                response += \"Here, Flask is checking if the logger needs a default handler and adding one if necessary.\\n\\n\"\n",
    "        \n",
    "        # 4. Analyze usage in tests\n",
    "        test_references = [r for r in references if 'test_' in r['file']]\n",
    "        if test_references:\n",
    "            response += \"## Testing\\n\\n\"\n",
    "            response += f\"The function is tested in {len(test_references)} locations within the test suite. \"\n",
    "            response += \"These tests verify the function works correctly with different logger configurations:\\n\\n\"\n",
    "            \n",
    "            test_scenarios = [\n",
    "                \"- Testing with a logger that has no handlers\",\n",
    "                \"- Testing with a logger that has handlers at the appropriate level\",\n",
    "                \"- Testing with a logger with propagation disabled\",\n",
    "                \"- Testing with handlers at different logging levels\"\n",
    "            ]\n",
    "            \n",
    "            response += \"\\n\".join(test_scenarios) + \"\\n\\n\"\n",
    "            \n",
    "            # Show a representative test\n",
    "            if test_references:\n",
    "                response += \"### Example Test\\n\\n\"\n",
    "                ref = test_references[0]\n",
    "                if ref.get('context'):\n",
    "                    response += \"```python\\n\" + ref['context'] + \"\\n```\\n\\n\"\n",
    "        \n",
    "        # 5. Add a summary\n",
    "        response += \"## Summary\\n\\n\"\n",
    "        response += f\"`has_level_handler` is a utility function used {len(references)} times in the Flask codebase. \"\n",
    "        response += \"It's a critical component of Flask's logging system that ensures log messages are properly handled. \"\n",
    "        response += \"The function prevents Flask from adding redundant handlers while ensuring that logs are captured \"\n",
    "        response += \"at the appropriate level.\"\n",
    "        \n",
    "        return response  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CodebaseVisualizer' object has no attribute 'analyze_has_level_handler'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mvisualizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyze_has_level_handler\u001b[49m()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CodebaseVisualizer' object has no attribute 'analyze_has_level_handler'"
     ]
    }
   ],
   "source": [
    "result = visualizer.analyze_has_level_handler()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
