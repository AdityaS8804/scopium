{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T11:46:51.549030Z","iopub.execute_input":"2025-03-01T11:46:51.549356Z","iopub.status.idle":"2025-03-01T11:46:52.510021Z","shell.execute_reply.started":"2025-03-01T11:46:51.549323Z","shell.execute_reply":"2025-03-01T11:46:52.509301Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"### Installing packages - **DONT TOUCH IT**","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/pallets/flask.git","metadata":{"execution":{"iopub.status.busy":"2025-03-01T11:46:56.893360Z","iopub.execute_input":"2025-03-01T11:46:56.893691Z","iopub.status.idle":"2025-03-01T11:46:58.519927Z","shell.execute_reply.started":"2025-03-01T11:46:56.893663Z","shell.execute_reply":"2025-03-01T11:46:58.518819Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Cloning into 'flask'...\nremote: Enumerating objects: 25360, done.\u001b[K\nremote: Total 25360 (delta 0), reused 0 (delta 0), pack-reused 25360 (from 1)\u001b[K\nReceiving objects: 100% (25360/25360), 10.38 MiB | 28.34 MiB/s, done.\nResolving deltas: 100% (17002/17002), done.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install nx-arangodb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T11:47:00.011549Z","iopub.execute_input":"2025-03-01T11:47:00.011872Z","iopub.status.idle":"2025-03-01T11:47:06.608546Z","shell.execute_reply.started":"2025-03-01T11:47:00.011848Z","shell.execute_reply":"2025-03-01T11:47:06.607708Z"}},"outputs":[{"name":"stdout","text":"Collecting nx-arangodb\n  Downloading nx_arangodb-1.3.0-py3-none-any.whl.metadata (9.3 kB)\nCollecting networkx<=3.4,>=3.0 (from nx-arangodb)\n  Downloading networkx-3.4-py3-none-any.whl.metadata (6.3 kB)\nCollecting phenolrs~=0.5 (from nx-arangodb)\n  Downloading phenolrs-0.5.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\nCollecting python-arango~=8.1 (from nx-arangodb)\n  Downloading python_arango-8.1.6-py3-none-any.whl.metadata (8.2 kB)\nCollecting adbnx-adapter~=5.0.5 (from nx-arangodb)\n  Downloading adbnx_adapter-5.0.6-py3-none-any.whl.metadata (21 kB)\nRequirement already satisfied: requests>=2.27.1 in /usr/local/lib/python3.10/dist-packages (from adbnx-adapter~=5.0.5->nx-arangodb) (2.32.3)\nRequirement already satisfied: rich>=12.5.1 in /usr/local/lib/python3.10/dist-packages (from adbnx-adapter~=5.0.5->nx-arangodb) (13.9.4)\nRequirement already satisfied: setuptools>=45 in /usr/local/lib/python3.10/dist-packages (from adbnx-adapter~=5.0.5->nx-arangodb) (75.1.0)\nRequirement already satisfied: numpy~=1.26 in /usr/local/lib/python3.10/dist-packages (from phenolrs~=0.5->nx-arangodb) (1.26.4)\nRequirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from python-arango~=8.1->nx-arangodb) (2.3.0)\nRequirement already satisfied: requests_toolbelt in /usr/local/lib/python3.10/dist-packages (from python-arango~=8.1->nx-arangodb) (1.0.0)\nRequirement already satisfied: PyJWT in /usr/local/lib/python3.10/dist-packages (from python-arango~=8.1->nx-arangodb) (2.10.1)\nRequirement already satisfied: importlib_metadata>=4.7.1 in /usr/local/lib/python3.10/dist-packages (from python-arango~=8.1->nx-arangodb) (8.5.0)\nRequirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.10/dist-packages (from python-arango~=8.1->nx-arangodb) (24.2)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata>=4.7.1->python-arango~=8.1->nx-arangodb) (3.21.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy~=1.26->phenolrs~=0.5->nx-arangodb) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy~=1.26->phenolrs~=0.5->nx-arangodb) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy~=1.26->phenolrs~=0.5->nx-arangodb) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy~=1.26->phenolrs~=0.5->nx-arangodb) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy~=1.26->phenolrs~=0.5->nx-arangodb) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy~=1.26->phenolrs~=0.5->nx-arangodb) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->adbnx-adapter~=5.0.5->nx-arangodb) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->adbnx-adapter~=5.0.5->nx-arangodb) (3.10)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->adbnx-adapter~=5.0.5->nx-arangodb) (2025.1.31)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.5.1->adbnx-adapter~=5.0.5->nx-arangodb) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.5.1->adbnx-adapter~=5.0.5->nx-arangodb) (2.19.1)\nRequirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.5.1->adbnx-adapter~=5.0.5->nx-arangodb) (4.12.2)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.5.1->adbnx-adapter~=5.0.5->nx-arangodb) (0.1.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy~=1.26->phenolrs~=0.5->nx-arangodb) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy~=1.26->phenolrs~=0.5->nx-arangodb) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy~=1.26->phenolrs~=0.5->nx-arangodb) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy~=1.26->phenolrs~=0.5->nx-arangodb) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy~=1.26->phenolrs~=0.5->nx-arangodb) (2024.2.0)\nDownloading nx_arangodb-1.3.0-py3-none-any.whl (67 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading adbnx_adapter-5.0.6-py3-none-any.whl (21 kB)\nDownloading networkx-3.4-py3-none-any.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading phenolrs-0.5.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m95.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading python_arango-8.1.6-py3-none-any.whl (114 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: networkx, python-arango, adbnx-adapter, phenolrs, nx-arangodb\n  Attempting uninstall: networkx\n    Found existing installation: networkx 3.4.2\n    Uninstalling networkx-3.4.2:\n      Successfully uninstalled networkx-3.4.2\nSuccessfully installed adbnx-adapter-5.0.6 networkx-3.4 nx-arangodb-1.3.0 phenolrs-0.5.9 python-arango-8.1.6\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!nvidia-smi\n!nvcc --version","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T11:47:08.255403Z","iopub.execute_input":"2025-03-01T11:47:08.255736Z","iopub.status.idle":"2025-03-01T11:47:08.554029Z","shell.execute_reply.started":"2025-03-01T11:47:08.255712Z","shell.execute_reply":"2025-03-01T11:47:08.552696Z"}},"outputs":[{"name":"stdout","text":"Sat Mar  1 11:47:08 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n| N/A   35C    P0             26W /  250W |       0MiB /  16384MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2023 NVIDIA Corporation\nBuilt on Tue_Aug_15_22:02:13_PDT_2023\nCuda compilation tools, release 12.2, V12.2.140\nBuild cuda_12.2.r12.2/compiler.33191640_0\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!pip install nx-cugraph-cu12 --extra-index-url https://pypi.nvidia.com # Requires CUDA-capable GPU","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T11:47:10.750977Z","iopub.execute_input":"2025-03-01T11:47:10.751298Z","iopub.status.idle":"2025-03-01T11:47:25.246157Z","shell.execute_reply.started":"2025-03-01T11:47:10.751271Z","shell.execute_reply":"2025-03-01T11:47:25.245164Z"}},"outputs":[{"name":"stdout","text":"Looking in indexes: https://pypi.org/simple, https://pypi.nvidia.com\nRequirement already satisfied: nx-cugraph-cu12 in /usr/local/lib/python3.10/dist-packages (24.10.0)\nRequirement already satisfied: cupy-cuda12x>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from nx-cugraph-cu12) (12.2.0)\nRequirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from nx-cugraph-cu12) (3.4)\nRequirement already satisfied: numpy<3.0a0,>=1.23 in /usr/local/lib/python3.10/dist-packages (from nx-cugraph-cu12) (1.26.4)\nRequirement already satisfied: pylibcugraph-cu12==24.10.* in /usr/local/lib/python3.10/dist-packages (from nx-cugraph-cu12) (24.10.0)\nRequirement already satisfied: nvidia-cublas-cu12 in /usr/local/lib/python3.10/dist-packages (from pylibcugraph-cu12==24.10.*->nx-cugraph-cu12) (12.6.4.1)\nRequirement already satisfied: nvidia-curand-cu12 in /usr/local/lib/python3.10/dist-packages (from pylibcugraph-cu12==24.10.*->nx-cugraph-cu12) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12 in /usr/local/lib/python3.10/dist-packages (from pylibcugraph-cu12==24.10.*->nx-cugraph-cu12) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12 in /usr/local/lib/python3.10/dist-packages (from pylibcugraph-cu12==24.10.*->nx-cugraph-cu12) (12.5.4.2)\nCollecting pylibraft-cu12==24.10.* (from pylibcugraph-cu12==24.10.*->nx-cugraph-cu12)\n  Downloading https://pypi.nvidia.com/pylibraft-cu12/pylibraft_cu12-24.10.0-cp310-cp310-manylinux_2_28_x86_64.whl (453.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting rmm-cu12==24.10.* (from pylibcugraph-cu12==24.10.*->nx-cugraph-cu12)\n  Downloading https://pypi.nvidia.com/rmm-cu12/rmm_cu12-24.10.0-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m183.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: cuda-python<13.0a0,>=12.0 in /usr/local/lib/python3.10/dist-packages (from pylibraft-cu12==24.10.*->pylibcugraph-cu12==24.10.*->nx-cugraph-cu12) (12.8.0)\nRequirement already satisfied: numba>=0.57 in /usr/local/lib/python3.10/dist-packages (from rmm-cu12==24.10.*->pylibcugraph-cu12==24.10.*->nx-cugraph-cu12) (0.60.0)\nRequirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.10/dist-packages (from cupy-cuda12x>=12.0.0->nx-cugraph-cu12) (0.8.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<3.0a0,>=1.23->nx-cugraph-cu12) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<3.0a0,>=1.23->nx-cugraph-cu12) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<3.0a0,>=1.23->nx-cugraph-cu12) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<3.0a0,>=1.23->nx-cugraph-cu12) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<3.0a0,>=1.23->nx-cugraph-cu12) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<3.0a0,>=1.23->nx-cugraph-cu12) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0a0,>=1.23->nx-cugraph-cu12) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0a0,>=1.23->nx-cugraph-cu12) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<3.0a0,>=1.23->nx-cugraph-cu12) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<3.0a0,>=1.23->nx-cugraph-cu12) (2024.2.0)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12->pylibcugraph-cu12==24.10.*->nx-cugraph-cu12) (12.6.85)\nRequirement already satisfied: cuda-bindings~=12.8.0 in /usr/local/lib/python3.10/dist-packages (from cuda-python<13.0a0,>=12.0->pylibraft-cu12==24.10.*->pylibcugraph-cu12==24.10.*->nx-cugraph-cu12) (12.8.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<3.0a0,>=1.23->nx-cugraph-cu12) (2024.2.0)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.57->rmm-cu12==24.10.*->pylibcugraph-cu12==24.10.*->nx-cugraph-cu12) (0.43.0)\nInstalling collected packages: rmm-cu12, pylibraft-cu12\n  Attempting uninstall: rmm-cu12\n    Found existing installation: rmm-cu12 25.2.0\n    Uninstalling rmm-cu12-25.2.0:\n      Successfully uninstalled rmm-cu12-25.2.0\n  Attempting uninstall: pylibraft-cu12\n    Found existing installation: pylibraft-cu12 25.2.0\n    Uninstalling pylibraft-cu12-25.2.0:\n      Successfully uninstalled pylibraft-cu12-25.2.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf-cu12 25.2.0 requires rmm-cu12==25.2.*, but you have rmm-cu12 24.10.0 which is incompatible.\ncuml-cu12 25.2.0 requires pylibraft-cu12==25.2.*, but you have pylibraft-cu12 24.10.0 which is incompatible.\ncuml-cu12 25.2.0 requires rmm-cu12==25.2.*, but you have rmm-cu12 24.10.0 which is incompatible.\ncuvs-cu12 25.2.0 requires pylibraft-cu12==25.2.*, but you have pylibraft-cu12 24.10.0 which is incompatible.\npylibcudf-cu12 25.2.0 requires rmm-cu12==25.2.*, but you have rmm-cu12 24.10.0 which is incompatible.\nraft-dask-cu12 25.2.0 requires pylibraft-cu12==25.2.*, but you have pylibraft-cu12 24.10.0 which is incompatible.\nucxx-cu12 0.42.0 requires rmm-cu12==25.2.*, but you have rmm-cu12 24.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed pylibraft-cu12-24.10.0 rmm-cu12-24.10.0\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!pip install --upgrade langchain langchain-community langchain-openai langgraph langchain_mistralai","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T11:48:09.103368Z","iopub.execute_input":"2025-03-01T11:48:09.103721Z","iopub.status.idle":"2025-03-01T11:48:19.961210Z","shell.execute_reply.started":"2025-03-01T11:48:09.103691Z","shell.execute_reply":"2025-03-01T11:48:19.960385Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.12)\nCollecting langchain\n  Using cached langchain-0.3.19-py3-none-any.whl.metadata (7.9 kB)\nCollecting langchain-community\n  Using cached langchain_community-0.3.18-py3-none-any.whl.metadata (2.4 kB)\nCollecting langchain-openai\n  Using cached langchain_openai-0.3.7-py3-none-any.whl.metadata (2.3 kB)\nCollecting langgraph\n  Using cached langgraph-0.3.2-py3-none-any.whl.metadata (17 kB)\nCollecting langchain_mistralai\n  Downloading langchain_mistralai-0.2.7-py3-none-any.whl.metadata (2.0 kB)\nCollecting langchain-core<1.0.0,>=0.3.35 (from langchain)\n  Downloading langchain_core-0.3.40-py3-none-any.whl.metadata (5.9 kB)\nCollecting langchain-text-splitters<1.0.0,>=0.3.6 (from langchain)\n  Downloading langchain_text_splitters-0.3.6-py3-none-any.whl.metadata (1.9 kB)\nRequirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.3)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.11.0a2)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\nRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.12)\nRequirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\nRequirement already satisfied: numpy<2,>=1.26.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\nCollecting async-timeout<5.0.0,>=4.0.0 (from langchain)\n  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.6.7)\nCollecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\nCollecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\nCollecting openai<2.0.0,>=1.58.1 (from langchain-openai)\n  Downloading openai-1.65.1-py3-none-any.whl.metadata (27 kB)\nRequirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (0.9.0)\nCollecting langgraph-checkpoint<3.0.0,>=2.0.10 (from langgraph)\n  Downloading langgraph_checkpoint-2.0.16-py3-none-any.whl.metadata (4.6 kB)\nCollecting langgraph-prebuilt<0.2,>=0.1.1 (from langgraph)\n  Downloading langgraph_prebuilt-0.1.1-py3-none-any.whl.metadata (5.0 kB)\nCollecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph)\n  Downloading langgraph_sdk-0.1.53-py3-none-any.whl.metadata (1.8 kB)\nRequirement already satisfied: tokenizers<1,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from langchain_mistralai) (0.21.0)\nRequirement already satisfied: httpx<1,>=0.25.2 in /usr/local/lib/python3.10/dist-packages (from langchain_mistralai) (0.28.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\nRequirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.25.2->langchain_mistralai) (3.7.1)\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.25.2->langchain_mistralai) (2025.1.31)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.25.2->langchain_mistralai) (1.0.7)\nRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.25.2->langchain_mistralai) (3.10)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.25.2->langchain_mistralai) (0.14.0)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (1.33)\nRequirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (24.2)\nRequirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (4.12.2)\nRequirement already satisfied: msgpack<2.0.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph) (1.1.0)\nRequirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.10/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.12)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.26.4->langchain) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.26.4->langchain) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.26.4->langchain) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.26.4->langchain) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.26.4->langchain) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.26.4->langchain) (2.4.1)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.9.0)\nRequirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (0.8.2)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.3.1)\nRequirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (4.67.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.29.0)\nCollecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.1)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.3.0)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\nRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\nRequirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers<1,>=0.15.1->langchain_mistralai) (0.29.0)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.25.2->langchain_mistralai) (1.2.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain_mistralai) (3.17.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain_mistralai) (2024.12.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.35->langchain) (3.0.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2,>=1.26.4->langchain) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2,>=1.26.4->langchain) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<2,>=1.26.4->langchain) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<2,>=1.26.4->langchain) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<2,>=1.26.4->langchain) (2024.2.0)\nDownloading langchain-0.3.19-py3-none-any.whl (1.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading langchain_community-0.3.18-py3-none-any.whl (2.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading langchain_openai-0.3.7-py3-none-any.whl (55 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.3/55.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langgraph-0.3.2-py3-none-any.whl (130 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.9/130.9 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_mistralai-0.2.7-py3-none-any.whl (15 kB)\nDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\nDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\nDownloading langchain_core-0.3.40-py3-none-any.whl (414 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m414.3/414.3 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_text_splitters-0.3.6-py3-none-any.whl (31 kB)\nDownloading langgraph_checkpoint-2.0.16-py3-none-any.whl (38 kB)\nDownloading langgraph_prebuilt-0.1.1-py3-none-any.whl (24 kB)\nDownloading langgraph_sdk-0.1.53-py3-none-any.whl (45 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading openai-1.65.1-py3-none-any.whl (472 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\nDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\nInstalling collected packages: python-dotenv, httpx-sse, async-timeout, pydantic-settings, openai, langgraph-sdk, langchain-core, langgraph-checkpoint, langchain-text-splitters, langchain-openai, langchain_mistralai, langgraph-prebuilt, langgraph, langchain, langchain-community\n  Attempting uninstall: async-timeout\n    Found existing installation: async-timeout 5.0.1\n    Uninstalling async-timeout-5.0.1:\n      Successfully uninstalled async-timeout-5.0.1\n  Attempting uninstall: openai\n    Found existing installation: openai 1.57.4\n    Uninstalling openai-1.57.4:\n      Successfully uninstalled openai-1.57.4\n  Attempting uninstall: langchain-core\n    Found existing installation: langchain-core 0.3.25\n    Uninstalling langchain-core-0.3.25:\n      Successfully uninstalled langchain-core-0.3.25\n  Attempting uninstall: langchain-text-splitters\n    Found existing installation: langchain-text-splitters 0.3.3\n    Uninstalling langchain-text-splitters-0.3.3:\n      Successfully uninstalled langchain-text-splitters-0.3.3\n  Attempting uninstall: langchain\n    Found existing installation: langchain 0.3.12\n    Uninstalling langchain-0.3.12:\n      Successfully uninstalled langchain-0.3.12\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed async-timeout-4.0.3 httpx-sse-0.4.0 langchain-0.3.19 langchain-community-0.3.18 langchain-core-0.3.40 langchain-openai-0.3.7 langchain-text-splitters-0.3.6 langchain_mistralai-0.2.7 langgraph-0.3.2 langgraph-checkpoint-2.0.16 langgraph-prebuilt-0.1.1 langgraph-sdk-0.1.53 openai-1.65.1 pydantic-settings-2.8.1 python-dotenv-1.0.1\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!pip install networkx==3.4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T11:52:36.830718Z","iopub.execute_input":"2025-03-01T11:52:36.831083Z","iopub.status.idle":"2025-03-01T11:52:40.154122Z","shell.execute_reply.started":"2025-03-01T11:52:36.831060Z","shell.execute_reply":"2025-03-01T11:52:40.153261Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: networkx==3.4 in /usr/local/lib/python3.10/dist-packages (3.4)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import networkx as nx\nimport matplotlib.pyplot as plt\nimport random\nimport os\nimport ast\nfrom typing import Dict, Set, List, Tuple, Optional,Any\nimport json\nfrom arango import ArangoClient\nimport nx_arangodb as nxadb\nfrom langgraph.prebuilt import create_react_agent\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langchain_openai import ChatOpenAI\nfrom langchain_community.graphs import ArangoGraph\nfrom langchain_community.chains.graph_qa.arangodb import ArangoGraphQAChain\nfrom langchain_core.tools import tool\nfrom langchain_mistralai import ChatMistralAI\nimport re","metadata":{"execution":{"iopub.status.busy":"2025-03-01T11:48:25.127596Z","iopub.execute_input":"2025-03-01T11:48:25.127909Z","iopub.status.idle":"2025-03-01T11:48:30.036179Z","shell.execute_reply.started":"2025-03-01T11:48:25.127886Z","shell.execute_reply":"2025-03-01T11:48:30.035519Z"},"trusted":true},"outputs":[{"name":"stderr","text":"[11:48:29 +0000] [INFO]: NetworkX-cuGraph is available.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"### Building graph","metadata":{}},{"cell_type":"code","source":"# import os\n# import ast\n# import networkx as nx\n# from typing import Dict, Set, List\n# import json\n# from arango import ArangoClient\n# os.environ[\"MISTRAL_API_KEY\"]=\"jJAuJZkjVcy2ynUhan375sHNviHiBeJU\"\n\n# class CodebaseVisualizer:\n#     def __init__(self, root_dir: str):\n#         self.root_dir = root_dir\n#         self.graph = nx.DiGraph()\n#         self.file_contents: Dict[str, str] = {}\n#         self.import_relations: Dict[str, Set[str]] = {}\n#         self.module_symbols: Dict[str, Dict[str, Dict[str, int]]] = {}  # file -> {symbol -> {type, line_no}}\n#         self.file_index: Dict[str, int] = {}  # Maps files to indices\n#         self.current_index = 0\n#         self.directories: Set[str] = set()\n\n#     def _get_next_index(self) -> int:\n#         \"\"\"Get next available index for file indexing.\"\"\"\n#         self.current_index += 1\n#         return self.current_index\n\n#     def _chunk_code(self, code: str, lines_per_chunk: int = 20) -> List[Dict]:\n#         \"\"\"\n#         Chunk the given code into snippets.\n#         Returns a list of dictionaries with 'code_snippet', 'start_line', and 'end_line'.\n#         \"\"\"\n#         lines = code.splitlines()\n#         chunks = []\n#         for i in range(0, len(lines), lines_per_chunk):\n#             chunk_lines = lines[i:i + lines_per_chunk]\n#             chunk = {\n#                 'code_snippet': '\\n'.join(chunk_lines),\n#                 'start_line': i + 1,\n#                 'end_line': i + len(chunk_lines)\n#             }\n#             chunks.append(chunk)\n#         return chunks\n\n#     def parse_files(self) -> None:\n#         \"\"\"Parse all Python files in the directory and build relationships.\"\"\"\n#         # First pass: Index all files and create directory nodes\n#         for root, dirs, files in os.walk(self.root_dir):\n#             # Add directory node\n#             rel_dir = os.path.relpath(root, self.root_dir)\n#             if rel_dir != '.':\n#                 self.directories.add(rel_dir)\n#                 self.graph.add_node(rel_dir, type='directory')\n\n#             # Index Python files\n#             for file in files:\n#                 if file.endswith('.py'):\n#                     file_path = os.path.join(root, file)\n#                     rel_path = os.path.relpath(file_path, self.root_dir)\n#                     self.file_index[rel_path] = self._get_next_index()\n                    \n#                     try:\n#                         with open(file_path, 'r', encoding='utf-8') as f:\n#                             content = f.read()\n#                             self.file_contents[rel_path] = content\n#                             self._analyze_file(rel_path, content)\n#                     except Exception as e:\n#                         print(f\"Error parsing {file_path}: {e}\")\n\n#     def _analyze_file(self, file_path: str, content: str) -> None:\n#         \"\"\"Analyze a single file for imports and symbols with line numbers.\"\"\"\n#         try:\n#             tree = ast.parse(content)\n#             imports = set()\n#             symbols = {}\n\n#             for node in ast.walk(tree):\n#                 # Track imports\n#                 if isinstance(node, (ast.Import, ast.ImportFrom)):\n#                     if isinstance(node, ast.Import):\n#                         for name in node.names:\n#                             imports.add((name.name, node.lineno))\n#                     else:  # ImportFrom\n#                         module = node.module if node.module else ''\n#                         imports.add((module, node.lineno))\n\n#                 # Track defined symbols with line numbers\n#                 elif isinstance(node, (ast.FunctionDef, ast.ClassDef)):\n#                     symbols[node.name] = {\n#                         'type': 'class' if isinstance(node, ast.ClassDef) else 'function',\n#                         'line_no': node.lineno\n#                     }\n\n#             self.import_relations[file_path] = imports\n#             self.module_symbols[file_path] = symbols\n\n#         except Exception as e:\n#             print(f\"Error analyzing {file_path}: {e}\")\n\n#     def build_graph(self) -> nx.DiGraph:\n#         \"\"\"Build the NetworkX graph with enhanced node and edge information.\"\"\"\n#         # Start with a directed graph for clarity in relationships\n#         dot_graph = nx.DiGraph()\n        \n#         # Add nodes for all files with indices and code snippet nodes\n#         for file_path, file_idx in self.file_index.items():\n#             dot_graph.add_node(file_path, \n#                                type='file',\n#                                file_index=file_idx,\n#                                directory=os.path.dirname(file_path))\n            \n#             # Create snippet nodes for the entire file\n#             if file_path in self.file_contents:\n#                 chunks = self._chunk_code(self.file_contents[file_path])\n#                 for idx, chunk_info in enumerate(chunks):\n#                     snippet_node = f\"{file_path}::snippet::{idx}\"\n#                     dot_graph.add_node(snippet_node,\n#                                        type='snippet',\n#                                        code_snippet=chunk_info['code_snippet'],\n#                                        start_line=chunk_info['start_line'],\n#                                        end_line=chunk_info['end_line'])\n#                     # Connect file node to snippet node\n#                     dot_graph.add_edge(file_path, snippet_node, \n#                                        edge_type='contains_snippet',\n#                                        start_line=chunk_info['start_line'],\n#                                        end_line=chunk_info['end_line'])\n\n#             # Add nodes for symbols in this file\n#             for symbol, details in self.module_symbols.get(file_path, {}).items():\n#                 symbol_node = f\"{file_path}::{symbol}\"\n#                 dot_graph.add_node(symbol_node, \n#                                    type='symbol',\n#                                    symbol_type=details['type'],\n#                                    line_number=details['line_no'])\n#                 dot_graph.add_edge(file_path, symbol_node, \n#                                    edge_type='defines',\n#                                    line_number=details['line_no'])\n\n#         # Add directory nodes\n#         for directory in self.directories:\n#             dot_graph.add_node(directory, type='directory')\n\n#         # Add edges for imports with line numbers\n#         for file_path, imports in self.import_relations.items():\n#             for imp, line_no in imports:\n#                 # Look for matching files or symbols\n#                 for target_file, symbols in self.module_symbols.items():\n#                     if imp in symbols:\n#                         dot_graph.add_edge(file_path, \n#                                            f\"{target_file}::{imp}\",\n#                                            edge_type='import',\n#                                            line_number=line_no)\n#                     elif target_file.replace('.py', '').endswith(imp):\n#                         dot_graph.add_edge(file_path, \n#                                            target_file,\n#                                            edge_type='import',\n#                                            line_number=line_no)\n        \n#         # Save the built graph in self.graph for later export (to ArangoDB, JSON, etc.)\n#         self.graph = dot_graph\n#         return dot_graph\n\n#     def export_file_index(self, output_path: str = 'file_index.txt') -> None:\n#         \"\"\"Export the file index mapping to a text file.\"\"\"\n#         with open(output_path, 'w') as f:\n#             # Write header\n#             f.write(\"File Index Mapping\\n\")\n#             f.write(\"=================\\n\\n\")\n            \n#             # Sort by index for better readability\n#             sorted_items = sorted(self.file_index.items(), key=lambda x: x[1])\n            \n#             # Write each file and its index\n#             for file_path, index in sorted_items:\n#                 f.write(f\"{index}: {file_path}\\n\")\n                \n#                 # If there are symbols in this file, list them with line numbers\n#                 if file_path in self.module_symbols:\n#                     f.write(\"  Symbols:\\n\")\n#                     for symbol, details in self.module_symbols[file_path].items():\n#                         symbol_type = details['type']\n#                         line_no = details['line_no']\n#                         f.write(f\"    - {symbol} ({symbol_type}) at line {line_no}\\n\")\n#                     f.write(\"\\n\")\n\n#     def export_graph_json(self, output_path: str = 'codebase_graph.json') -> None:\n#         \"\"\"Export the graph structure to JSON.\"\"\"\n#         graph_data = {\n#             'nodes': [\n#                 {\n#                     'id': node,\n#                     'type': data['type'],\n#                     'file_index': data.get('file_index'),\n#                     'directory': data.get('directory'),\n#                     'symbol_type': data.get('symbol_type'),\n#                     'line_number': data.get('line_number'),\n#                     'code_snippet': data.get('code_snippet'),\n#                     'start_line': data.get('start_line'),\n#                     'end_line': data.get('end_line')\n#                 } \n#                 for node, data in self.graph.nodes(data=True)\n#             ],\n#             'links': [\n#                 {\n#                     'source': source,\n#                     'target': target,\n#                     'type': data.get('edge_type'),\n#                     'line_number': data.get('line_number'),\n#                     'start_line': data.get('start_line'),\n#                     'end_line': data.get('end_line')\n#                 } \n#                 for source, target, data in self.graph.edges(data=True)\n#             ]\n#         }\n        \n#         with open(output_path, 'w') as f:\n#             json.dump(graph_data, f, indent=2)\n#     def export_to_arango(self,\n#                          db_name: str = 'codebase',\n#                          username: str = 'root',\n#                          password: str = 'passwd',\n#                          host: str = 'http://localhost:8529') -> None:\n#         \"\"\"\n#         Export the graph into ArangoDB.\n#         IMPORTANT: This method first deletes any existing graph and associated data in ArangoDB.\n#         \"\"\"\n#         client = ArangoClient(hosts=host)\n#         db = client.db(username=username, password=password,verify=True)\n\n#         # Delete the existing graph and its collections if they exist.\n#         graph_name = \"FlaskRepv1\"\n#         '''\n#         if db.has_graph(graph_name):\n#             try:\n#                 # Delete the entire graph and its collections.\n#                 db.graph(graph_name).delete(delete_collections=True)\n#                 print(\"Existing ArangoDB graph deleted.\")\n#             except Exception as e:\n#                 print(f\"Error deleting graph: {e}\")\n#                 '''\n        \n#         G_adb = nxadb.Graph(\n#             name=graph_name,\n#             db=db,\n#             incoming_graph_data=self.graph,\n#             write_batch_size=50000,\n#             overwrite_graph=True\n#         )\n        \n#         self.G_adb=G_adb\n#         return G_adb\n#         print(\"Graph successfully exported to ArangoDB.\")\n#     def text_to_nx_algorithm_to_text(self,query):\n#         \"\"\"This tool is available to invoke a NetworkX Algorithm on\n#         the ArangoDB Graph. You are responsible for accepting the\n#         Natural Language Query, establishing which algorithm needs to\n#         be executed, executing the algorithm, and translating the results back\n#         to Natural Language, with respect to the original query.\n    \n#         If the query (e.g traversals, shortest path, etc.) can be solved using the Arango Query Language, then do not use\n#         this tool.\n#         \"\"\"\n#         llm = ChatMistralAI(\n#             model=\"mistral-large-latest\",\n#             temperature=0,\n#             max_retries=2,\n#             # other params...\n#         )\n#         ######################\n#         print(\"1) Generating NetworkX code\")\n    \n#         text_to_nx = llm.invoke(f\"\"\"\n#         I have a NetworkX Graph called `G_adb`. It has the following schema: {arango_graph.schema}\n    \n#         I have the following graph analysis query: {query}.\n    \n#         Generate the Python Code required to answer the query using the `G_adb` object.\n        \n#         It should give code so that is gives ALL the necessary contents that use this part of the code in terms of ALL nested imports. Consider the nested directories and sub-directory informations as well.\n    \n#         Be very precise on the NetworkX algorithm you select to answer this query. Think step by step.\n    \n#         Only assume that networkx is installed, and other base python dependencies.\n    \n#         Always set the last variable as `FINAL_RESULT`, which represents the answer to the original query.\n    \n#         Only provide python code that I can directly execute via `exec()`. Do not provide any instructions.\n    \n#         Make sure that `FINAL_RESULT` stores a short & consice answer. Avoid setting this variable to a long sequence.\n    \n#         Your code:\n#         \"\"\").content\n    \n#         text_to_nx_cleaned = re.sub(r\"^```python\\n|```$\", \"\", text_to_nx, flags=re.MULTILINE).strip()\n        \n#         print('-'*10)\n#         print(text_to_nx_cleaned)\n#         print('-'*10)\n    \n#         ######################\n    \n#         print(\"\\n2) Executing NetworkX code\")\n#         global_vars = {\"G_adb\":self.G_adb , \"nx\": nx}\n#         local_vars = {}\n    \n#         try:\n#             exec(text_to_nx_cleaned, global_vars, local_vars)\n#             text_to_nx_final = text_to_nx\n#         except Exception as e:\n#             print(f\"EXEC ERROR: {e}\")\n#             return f\"EXEC ERROR: {e}\"\n    \n#             # TODO: Consider experimenting with a code corrector!\n#             attempt = 1\n#             MAX_ATTEMPTS = 3\n    \n#             # while attempt <= MAX_ATTEMPTS\n#                 # ...\n    \n#         print('-'*10)\n#         FINAL_RESULT = local_vars[\"FINAL_RESULT\"]\n#         print(f\"FINAL_RESULT: {FINAL_RESULT}\")\n#         print('-'*10)\n    \n#         ######################\n    \n#         print(\"3) Formulating final answer\")\n    \n#         nx_to_text = llm.invoke(f\"\"\"\n#             I have a NetworkX Graph called `G_adb`. It has the following schema: {arango_graph.schema}\n    \n#             I have the following graph analysis query: {query}.\n    \n#             I have executed the following python code to help me answer my query:\n    \n#             ---\n#             {text_to_nx_final}\n#             ---\n    \n#             The `FINAL_RESULT` variable is set to the following: {FINAL_RESULT}.\n    \n#             Based on my original Query and FINAL_RESULT, generate a short and concise response to\n#             answer my query.\n            \n#             Your response:\n#         \"\"\").content\n    \n#         return nx_to_text\n\n# # Example usage\n\n# # Initialize and parse the codebase\n# visualizer = CodebaseVisualizer(\"flask\")\n# visualizer.parse_files()\n# visualizer.build_graph()\n    \n# # Export the file index and JSON (for local inspection)\n# visualizer.export_file_index()\n# visualizer.export_graph_json()\n# # Export the enriched graph to ArangoDB (this will delete any existing graph data first)\n# #visualizer.export_to_arango(db_name=\"FlaskRepv1\",username=\"root\",password=\"cUZ0YaNdcwfUTw6VjRny\",host=\"http://localhost:8529\")\n","metadata":{"execution":{"iopub.execute_input":"2025-03-01T04:38:30.721707Z","iopub.status.busy":"2025-03-01T04:38:30.721365Z","iopub.status.idle":"2025-03-01T04:38:31.163453Z","shell.execute_reply":"2025-03-01T04:38:31.162755Z","shell.execute_reply.started":"2025-03-01T04:38:30.721684Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import os\nimport ast\nimport networkx as nx\nfrom typing import Dict, Set, List, Tuple, Optional\nimport json\nfrom arango import ArangoClient\nimport re\n\nclass CodebaseVisualizer:\n    def __init__(self, root_dir: str):\n        self.root_dir = root_dir\n        self.graph = nx.DiGraph()\n        self.file_contents: Dict[str, str] = {}\n        self.import_relations: Dict[str, List[Tuple[str, int]]] = {}  # file -> [(module, line_no)]\n        self.module_symbols: Dict[str, Dict[str, Dict[str, any]]] = {}  # file -> {symbol -> {type, line_no, context}}\n        self.symbol_references: Dict[str, List[Tuple[str, int]]] = {}  # symbol -> [(file, line_no)]\n        self.file_index: Dict[str, int] = {}  # Maps files to indices\n        self.current_index = 0\n        self.directories: Set[str] = set()\n\n    def _get_next_index(self) -> int:\n        \"\"\"Get next available index for file indexing.\"\"\"\n        self.current_index += 1\n        return self.current_index\n\n    def _chunk_code(self, code: str, lines_per_chunk: int = 20) -> List[Dict]:\n        \"\"\"\n        Chunk the given code into snippets.\n        Returns a list of dictionaries with 'code_snippet', 'start_line', and 'end_line'.\n        \"\"\"\n        lines = code.splitlines()\n        chunks = []\n        for i in range(0, len(lines), lines_per_chunk):\n            chunk_lines = lines[i:i + lines_per_chunk]\n            chunk = {\n                'code_snippet': '\\n'.join(chunk_lines),\n                'start_line': i + 1,\n                'end_line': i + len(chunk_lines)\n            }\n            chunks.append(chunk)\n        return chunks\n\n    def _get_context_around_line(self, file_path: str, line_no: int, context_lines: int = 3) -> str:\n        \"\"\"Extract context around a specific line in a file.\"\"\"\n        if file_path not in self.file_contents:\n            return \"\"\n        \n        lines = self.file_contents[file_path].splitlines()\n        start = max(0, line_no - context_lines - 1)\n        end = min(len(lines), line_no + context_lines)\n        \n        context = \"\\n\".join(lines[start:end])\n        return context\n\n    def parse_files(self) -> None:\n        \"\"\"Parse all Python files in the directory and build relationships.\"\"\"\n        # First pass: Index all files and create directory nodes\n        for root, dirs, files in os.walk(self.root_dir):\n            # Add directory node\n            rel_dir = os.path.relpath(root, self.root_dir)\n            if rel_dir != '.':\n                self.directories.add(rel_dir)\n                self.graph.add_node(rel_dir, type='directory')\n\n            # Index Python files\n            for file in files:\n                if file.endswith('.py'):\n                    file_path = os.path.join(root, file)\n                    rel_path = os.path.relpath(file_path, self.root_dir)\n                    self.file_index[rel_path] = self._get_next_index()\n                    \n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            content = f.read()\n                            self.file_contents[rel_path] = content\n                            self._analyze_file(rel_path, content)\n                    except Exception as e:\n                        print(f\"Error parsing {file_path}: {e}\")\n        \n        # Second pass: Find symbol references across files\n        self._find_symbol_references()\n\n    def _analyze_file(self, file_path: str, content: str) -> None:\n        \"\"\"Analyze a single file for imports and symbols with line numbers and context.\"\"\"\n        try:\n            tree = ast.parse(content)\n            imports = []\n            symbols = {}\n\n            for node in ast.walk(tree):\n                # Track imports\n                if isinstance(node, (ast.Import, ast.ImportFrom)):\n                    if isinstance(node, ast.Import):\n                        for name in node.names:\n                            imports.append((name.name, node.lineno))\n                    else:  # ImportFrom\n                        module = node.module if node.module else ''\n                        for name in node.names:\n                            imports.append((f\"{module}.{name.name}\" if module else name.name, node.lineno))\n\n                # Track defined symbols with line numbers and context\n                elif isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.Assign)):\n                    if isinstance(node, (ast.FunctionDef, ast.ClassDef)):\n                        symbol_name = node.name\n                        symbol_type = 'class' if isinstance(node, ast.ClassDef) else 'function'\n                        line_no = node.lineno\n                        context = self._extract_node_source(content, node)\n                        \n                        symbols[symbol_name] = {\n                            'type': symbol_type,\n                            'line_no': line_no,\n                            'context': context,\n                            'docstring': ast.get_docstring(node)\n                        }\n                    elif isinstance(node, ast.Assign):\n                        # Handle variable assignments\n                        for target in node.targets:\n                            if isinstance(target, ast.Name):\n                                symbol_name = target.id\n                                line_no = node.lineno\n                                context = self._extract_node_source(content, node)\n                                \n                                symbols[symbol_name] = {\n                                    'type': 'variable',\n                                    'line_no': line_no,\n                                    'context': context\n                                }\n\n            self.import_relations[file_path] = imports\n            self.module_symbols[file_path] = symbols\n\n        except Exception as e:\n            print(f\"Error analyzing {file_path}: {e}\")\n\n    def _extract_node_source(self, source: str, node) -> str:\n        \"\"\"Extract the source code for an AST node.\"\"\"\n        try:\n            lines = source.splitlines()\n            if hasattr(node, 'lineno') and hasattr(node, 'end_lineno'):\n                start = node.lineno - 1\n                end = getattr(node, 'end_lineno', start + 1)\n                return '\\n'.join(lines[start:end])\n            return \"\"\n        except Exception:\n            return \"\"\n\n    def _find_symbol_references(self) -> None:\n        \"\"\"Find references to symbols across all files.\"\"\"\n        for file_path, content in self.file_contents.items():\n            try:\n                tree = ast.parse(content)\n                self._process_file_for_references(file_path, tree, content)\n            except Exception as e:\n                print(f\"Error finding references in {file_path}: {e}\")\n\n    def _process_file_for_references(self, file_path: str, tree, source: str) -> None:\n        \"\"\"Process a file's AST to find references to symbols.\"\"\"\n        for node in ast.walk(tree):\n            # Find variable references\n            if isinstance(node, ast.Name) and isinstance(node.ctx, ast.Load):\n                symbol_name = node.id\n                line_no = node.lineno\n                \n                # Track reference with context\n                if symbol_name not in self.symbol_references:\n                    self.symbol_references[symbol_name] = []\n                \n                context = self._get_context_around_line(file_path, line_no)\n                self.symbol_references[symbol_name].append((file_path, line_no, context))\n            \n            # Find attribute references (e.g., obj.method())\n            elif isinstance(node, ast.Attribute) and isinstance(node.ctx, ast.Load):\n                attr_name = node.attr\n                line_no = node.lineno\n                \n                if attr_name not in self.symbol_references:\n                    self.symbol_references[attr_name] = []\n                \n                context = self._get_context_around_line(file_path, line_no)\n                self.symbol_references[attr_name].append((file_path, line_no, context))\n\n    def build_graph(self) -> nx.DiGraph:\n        \"\"\"Build the NetworkX graph with enhanced node and edge information.\"\"\"\n        # Start with a directed graph for clarity in relationships\n        dot_graph = nx.DiGraph()\n        \n        # Add nodes for all files with indices and code snippet nodes\n        for file_path, file_idx in self.file_index.items():\n            dot_graph.add_node(file_path, \n                               type='file',\n                               file_index=file_idx,\n                               directory=os.path.dirname(file_path))\n            \n            # Create snippet nodes for the entire file\n            if file_path in self.file_contents:\n                chunks = self._chunk_code(self.file_contents[file_path])\n                for idx, chunk_info in enumerate(chunks):\n                    snippet_node = f\"{file_path}::snippet::{idx}\"\n                    dot_graph.add_node(snippet_node,\n                                       type='snippet',\n                                       code_snippet=chunk_info['code_snippet'],\n                                       start_line=chunk_info['start_line'],\n                                       end_line=chunk_info['end_line'])\n                    # Connect file node to snippet node\n                    dot_graph.add_edge(file_path, snippet_node, \n                                       edge_type='contains_snippet',\n                                       start_line=chunk_info['start_line'],\n                                       end_line=chunk_info['end_line'])\n\n            # Add nodes for symbols in this file\n            for symbol, details in self.module_symbols.get(file_path, {}).items():\n                symbol_node = f\"{file_path}::{symbol}\"\n                dot_graph.add_node(symbol_node, \n                                   type='symbol',\n                                   symbol_type=details['type'],\n                                   line_number=details['line_no'],\n                                   context=details.get('context', ''),\n                                   docstring=details.get('docstring', ''))\n                dot_graph.add_edge(file_path, symbol_node, \n                                   edge_type='defines',\n                                   line_number=details['line_no'])\n\n        # Add directory nodes\n        for directory in self.directories:\n            dot_graph.add_node(directory, type='directory')\n\n        # Add edges for imports with line numbers\n        for file_path, imports in self.import_relations.items():\n            for imp, line_no in imports:\n                # Look for matching files or symbols\n                for target_file, symbols in self.module_symbols.items():\n                    if imp in symbols:\n                        dot_graph.add_edge(file_path, \n                                           f\"{target_file}::{imp}\",\n                                           edge_type='import',\n                                           line_number=line_no)\n                    elif target_file.replace('.py', '').endswith(imp):\n                        dot_graph.add_edge(file_path, \n                                           target_file,\n                                           edge_type='import',\n                                           line_number=line_no)\n        \n        # Add edges for symbol references with line numbers and context\n        for symbol, references in self.symbol_references.items():\n            for file_path, symbols in self.module_symbols.items():\n                if symbol in symbols:\n                    symbol_node = f\"{file_path}::{symbol}\"\n                    \n                    # Connect symbol to all its references\n                    for ref_file, ref_line, context in references:\n                        if ref_file != file_path:  # Only add cross-file references\n                            dot_graph.add_edge(ref_file, \n                                              symbol_node,\n                                              edge_type='references',\n                                              line_number=ref_line,\n                                              context=context)\n        \n        # Save the built graph in self.graph for later export\n        self.graph = dot_graph\n        return dot_graph\n\n    def find_symbol_usages(self, symbol_name: str) -> List[Dict]:\n        \"\"\"Find all usages of a symbol in the codebase with context.\"\"\"\n        results = []\n        \n        # Look for the symbol in defined symbols\n        for file_path, symbols in self.module_symbols.items():\n            if symbol_name in symbols:\n                details = symbols[symbol_name]\n                results.append({\n                    'file': file_path,\n                    'type': 'definition',\n                    'line': details['line_no'],\n                    'symbol_type': details['type'],\n                    'context': details.get('context', ''),\n                    'docstring': details.get('docstring', '')\n                })\n        \n        # Look for references to the symbol\n        if symbol_name in self.symbol_references:\n            for file_path, line_no, context in self.symbol_references[symbol_name]:\n                results.append({\n                    'file': file_path,\n                    'type': 'reference',\n                    'line': line_no,\n                    'context': context\n                })\n        \n        return results\n\n    def export_file_index(self, output_path: str = 'file_index.txt') -> None:\n        \"\"\"Export the file index mapping to a text file.\"\"\"\n        with open(output_path, 'w') as f:\n            # Write header\n            f.write(\"File Index Mapping\\n\")\n            f.write(\"=================\\n\\n\")\n            \n            # Sort by index for better readability\n            sorted_items = sorted(self.file_index.items(), key=lambda x: x[1])\n            \n            # Write each file and its index\n            for file_path, index in sorted_items:\n                f.write(f\"{index}: {file_path}\\n\")\n                \n                # If there are symbols in this file, list them with line numbers\n                if file_path in self.module_symbols:\n                    f.write(\"  Symbols:\\n\")\n                    for symbol, details in self.module_symbols[file_path].items():\n                        symbol_type = details['type']\n                        line_no = details['line_no']\n                        f.write(f\"    - {symbol} ({symbol_type}) at line {line_no}\\n\")\n                    f.write(\"\\n\")\n\n    def _analyze_symbol_purpose(self, symbol_name, usages):\n        \"\"\"Analyze the purpose of a symbol based on its usage patterns.\"\"\"\n        # Get the definition if available\n        definitions = [u for u in usages if u['type'] == 'definition']\n        references = [u for u in usages if u['type'] == 'reference']\n        \n        purpose = \"\"\n        \n        # Check if we have a definition with docstring\n        if definitions and definitions[0].get('docstring'):\n            purpose += f\"{definitions[0]['docstring']}\\n\\n\"\n        \n        # If it's a function, try to infer what it does from usage\n        if definitions and definitions[0].get('symbol_type') == 'function':\n            # Collect contexts where it's used\n            contexts = [ref.get('context', '') for ref in references if ref.get('context')]\n            \n            # Analyze contexts for patterns\n            if contexts:\n                common_patterns = self._find_common_usage_patterns(contexts, symbol_name)\n                \n                purpose += \"Based on usage patterns, this function appears to:\\n\"\n                \n                if any(\"assert\" in ctx for ctx in contexts):\n                    purpose += \"- Be used in test assertions to verify behavior\\n\"\n                \n                if any(\"if\" in ctx and symbol_name in ctx for ctx in contexts):\n                    purpose += \"- Be used as a condition in control flow statements\\n\"\n                \n                if common_patterns:\n                    for pattern in common_patterns[:3]:  # Top 3 patterns\n                        purpose += f\"- {pattern}\\n\"\n        \n        # If we couldn't infer much, provide a generic description\n        if not purpose or len(purpose.strip()) < 10:\n            if definitions and definitions[0].get('symbol_type') == 'function':\n                context = definitions[0].get('context', '')\n                \n                # Look for parameters to understand what it takes\n                params = self._extract_function_params(context)\n                \n                purpose += f\"This function appears to check or validate something\"\n                if params:\n                    purpose += f\" related to {', '.join(params)}\"\n                purpose += \".\\n\"\n        \n        return purpose\n\n    def _find_common_usage_patterns(self, contexts, symbol_name):\n        \"\"\"Find common patterns in the usage contexts.\"\"\"\n        patterns = []\n        \n        # Check if it's used with certain objects/methods frequently\n        if any(f\".{symbol_name}\" in ctx for ctx in contexts):\n            patterns.append(\"Be a method called on objects\")\n        \n        # Check if it's used for configuration or setup\n        if any(\"config\" in ctx.lower() or \"setup\" in ctx.lower() for ctx in contexts):\n            patterns.append(\"Be involved in configuration or setup\")\n        \n        # Check if it's used for logging\n        if any(\"log\" in ctx.lower() for ctx in contexts):\n            patterns.append(\"Be related to logging functionality\")\n        \n        # Check if it's used in exception handling\n        if any(\"except\" in ctx.lower() or \"try\" in ctx.lower() for ctx in contexts):\n            patterns.append(\"Be involved in exception handling\")\n        \n        return patterns\n\n    def _extract_function_params(self, context):\n        \"\"\"Extract parameter names from a function definition.\"\"\"\n        params = []\n        if context:\n            # Simple regex-based extraction\n            match = re.search(r'def\\s+\\w+\\s*\\((.*?)\\)', context, re.DOTALL)\n            if match:\n                param_string = match.group(1)\n                # Split by comma and clean up\n                raw_params = [p.strip() for p in param_string.split(',')]\n                # Extract just the parameter name (before any : or =)\n                params = [re.split(r'[=:]', p)[0].strip() for p in raw_params if p]\n                # Remove self if it's there\n                if params and params[0] == 'self':\n                    params = params[1:]\n        return params\n\n    def _general_codebase_analysis(self, query):\n        \"\"\"Provide a general analysis based on the query.\"\"\"\n        response = f\"# Analysis for Query: {query}\\n\\n\"\n        \n        # Check if the query is asking about structure\n        if any(term in query.lower() for term in ['structure', 'organization', 'layout']):\n            response += \"## Codebase Structure\\n\\n\"\n            # Count files by directory\n            files_by_dir = {}\n            for file_path in self.file_index:\n                directory = os.path.dirname(file_path)\n                if directory not in files_by_dir:\n                    files_by_dir[directory] = []\n                files_by_dir[directory].append(file_path)\n            \n            response += f\"The codebase contains {len(self.file_index)} Python files across {len(files_by_dir)} directories.\\n\\n\"\n            \n            # Show top-level directories\n            response += \"Main directories:\\n\"\n            for directory, files in sorted(files_by_dir.items(), key=lambda x: len(x[1]), reverse=True)[:10]:\n                dir_name = directory if directory else '(root)'\n                response += f\"- {dir_name}: {len(files)} files\\n\"\n        \n        # Check if the query is asking about specific functionality\n        functionality_terms = ['handle', 'process', 'create', 'generate', 'calculate']\n        for term in functionality_terms:\n            if term in query.lower():\n                # Search for functions with this term\n                matching_functions = []\n                for file_path, symbols in self.module_symbols.items():\n                    for symbol, details in symbols.items():\n                        if details['type'] == 'function' and term in symbol.lower():\n                            matching_functions.append((file_path, symbol, details))\n                \n                if matching_functions:\n                    response += f\"\\n## Functions Related to '{term}'\\n\\n\"\n                    for file_path, symbol, details in matching_functions[:5]:  # Show top 5\n                        response += f\"- `{symbol}` in {file_path}:{details['line_no']}\\n\"\n                    if len(matching_functions) > 5:\n                        response += f\"... and {len(matching_functions) - 5} more functions\\n\"\n        \n        return response\n\n    def export_graph_json(self, output_path: str = 'codebase_graph.json') -> None:\n        \"\"\"Export the graph structure to JSON.\"\"\"\n        graph_data = {\n            'nodes': [\n                {\n                    'id': node,\n                    'type': data['type'],\n                    'file_index': data.get('file_index'),\n                    'directory': data.get('directory'),\n                    'symbol_type': data.get('symbol_type'),\n                    'line_number': data.get('line_number'),\n                    'code_snippet': data.get('code_snippet'),\n                    'start_line': data.get('start_line'),\n                    'end_line': data.get('end_line'),\n                    'context': data.get('context'),\n                    'docstring': data.get('docstring')\n                } \n                for node, data in self.graph.nodes(data=True)\n            ],\n            'links': [\n                {\n                    'source': source,\n                    'target': target,\n                    'type': data.get('edge_type'),\n                    'line_number': data.get('line_number'),\n                    'start_line': data.get('start_line'),\n                    'end_line': data.get('end_line'),\n                    'context': data.get('context')\n                } \n                for source, target, data in self.graph.edges(data=True)\n            ]\n        }\n        \n        with open(output_path, 'w') as f:\n            json.dump(graph_data, f, indent=2)\n\n    def export_to_arango(self,\n                         db_name: str = 'codebase',\n                         username: str = 'root',\n                         password: str = 'passwd',\n                         host: str = 'http://localhost:8529') -> None:\n        \"\"\"Export the graph into ArangoDB.\"\"\"\n        client = ArangoClient(hosts=host)\n        db = client.db(username=username, password=password, verify=True)\n\n        # Delete the existing graph and its collections if they exist.\n        graph_name = \"FlaskRepv1\"\n        \n        # Import networkx_to_arangodb if it's available\n        try:\n            import networkx_to_arangodb as nxadb # type: ignore\n            G_adb = nxadb.Graph(\n                name=graph_name,\n                db=db,\n                incoming_graph_data=self.graph,\n                write_batch_size=50000,\n                overwrite_graph=True\n            )\n            \n            self.G_adb = G_adb\n            print(\"Graph successfully exported to ArangoDB.\")\n            return G_adb\n        except ImportError:\n            print(\"networkx_to_arangodb module not found. Please install it to export to ArangoDB.\")\n            return None\n\n ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T11:49:22.892902Z","iopub.execute_input":"2025-03-01T11:49:22.893572Z","iopub.status.idle":"2025-03-01T11:49:22.936505Z","shell.execute_reply.started":"2025-03-01T11:49:22.893540Z","shell.execute_reply":"2025-03-01T11:49:22.935803Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"#visualizer.export_to_arango(db_name=\"FlaskRepv1\",username=\"root\",password=\"cUZ0YaNdcwfUTw6VjRny\",host=\"https://d2eeb8083350.arangodb.cloud:8529\")\n","metadata":{"execution":{"iopub.execute_input":"2025-03-01T04:38:34.048801Z","iopub.status.busy":"2025-03-01T04:38:34.048501Z","iopub.status.idle":"2025-03-01T04:38:59.455318Z","shell.execute_reply":"2025-03-01T04:38:59.454208Z","shell.execute_reply.started":"2025-03-01T04:38:34.048779Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# visualizer = CodebaseVisualizer(\"flask\")\n# visualizer.parse_files()\n# G = visualizer.build_graph()","metadata":{"execution":{"iopub.execute_input":"2025-03-01T04:39:43.873809Z","iopub.status.busy":"2025-03-01T04:39:43.873462Z","iopub.status.idle":"2025-03-01T04:39:44.082162Z","shell.execute_reply":"2025-03-01T04:39:44.081407Z","shell.execute_reply.started":"2025-03-01T04:39:43.873780Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# class GraphVisualizer:\n#     def __init__(self, graph: nx.Graph):\n#         self.graph = graph\n#         self.pos = None\n        \n#     def set_layout(self, layout_type: str = 'spring', **layout_params) -> None:\n#         \"\"\"\n#         Set the layout for the graph visualization.\n        \n#         Args:\n#             layout_type: Type of layout ('spring', 'circular', 'kamada_kawai', \n#                         'random', 'shell', 'spectral')\n#             layout_params: Additional parameters for the layout algorithm\n#         \"\"\"\n#         layout_funcs = {\n#             'spring': nx.spring_layout,\n#             'circular': nx.circular_layout,\n#             'kamada_kawai': nx.kamada_kawai_layout,\n#             'random': nx.random_layout,\n#             'shell': nx.shell_layout,\n#             'spectral': nx.spectral_layout\n#         }\n        \n#         if layout_type not in layout_funcs:\n#             raise ValueError(f\"Unsupported layout type. Choose from: {list(layout_funcs.keys())}\")\n            \n#         self.pos = layout_funcs[layout_type](self.graph, **layout_params)\n    \n#     def _get_node_colors(self) -> Dict[str, str]:\n#         \"\"\"Extract node colors from graph attributes or generate defaults.\"\"\"\n#         colors = {}\n#         for node in self.graph.nodes():\n#             # Check for color in node attributes\n#             attrs = self.graph.nodes[node]\n#             if 'fillcolor' in attrs:\n#                 colors[node] = attrs['fillcolor']\n#             elif 'color' in attrs:\n#                 colors[node] = attrs['color']\n#             else:\n#                 colors[node] = 'lightblue'  # default color\n#         return colors\n    \n#     def _get_node_sizes(self) -> Dict[str, float]:\n#         \"\"\"Extract or compute node sizes.\"\"\"\n#         sizes = {}\n#         for node in self.graph.nodes():\n#             attrs = self.graph.nodes[node]\n#             if 'size' in attrs:\n#                 sizes[node] = attrs['size']\n#             else:\n#                 # Default size based on node degree\n#                 sizes[node] = 1000 * (1 + self.graph.degree(node) / 10)\n#         return sizes\n    \n#     def _get_edge_colors(self) -> Dict[Tuple[str, str], str]:\n#         \"\"\"Extract edge colors from graph attributes or generate defaults.\"\"\"\n#         colors = {}\n#         for u, v in self.graph.edges():\n#             edge_data = self.graph.get_edge_data(u, v)\n#             if 'color' in edge_data:\n#                 colors[(u, v)] = edge_data['color']\n#             else:\n#                 colors[(u, v)] = 'gray'  # default color\n#         return colors\n    \n#     def _get_node_labels(self) -> Dict[str, str]:\n#         \"\"\"Extract node labels from graph attributes.\"\"\"\n#         labels = {}\n#         for node in self.graph.nodes():\n#             attrs = self.graph.nodes[node]\n#             if 'label' in attrs:\n#                 labels[node] = attrs['label']\n#             else:\n#                 labels[node] = str(node)\n#         return labels\n    \n#     def _get_edge_labels(self) -> Dict[Tuple[str, str], str]:\n#         \"\"\"Extract edge labels from graph attributes.\"\"\"\n#         labels = {}\n#         for u, v in self.graph.edges():\n#             edge_data = self.graph.get_edge_data(u, v)\n#             if 'label' in edge_data:\n#                 labels[(u, v)] = edge_data['label']\n#         return labels\n\n#     def visualize(self, \n#                  figsize: Tuple[int, int] = (12, 8),\n#                  node_size: Optional[Dict[str, float]] = None,\n#                  node_color: Optional[Dict[str, str]] = None,\n#                  edge_color: Optional[Dict[Tuple[str, str], str]] = None,\n#                  with_labels: bool = True,\n#                  font_size: int = 8,\n#                  title: Optional[str] = None,\n#                  show_edge_labels: bool = True,\n#                  alpha: float = 0.7,\n#                  save_path: Optional[str] = None) -> None:\n#         \"\"\"\n#         Visualize the graph with customizable options.\n        \n#         Args:\n#             figsize: Size of the figure (width, height)\n#             node_size: Dictionary mapping nodes to their sizes\n#             node_color: Dictionary mapping nodes to their colors\n#             edge_color: Dictionary mapping edges to their colors\n#             with_labels: Whether to show node labels\n#             font_size: Size of the font for labels\n#             title: Title of the graph\n#             show_edge_labels: Whether to show edge labels\n#             alpha: Transparency of nodes\n#             save_path: Path to save the visualization (if None, displays instead)\n#         \"\"\"\n#         if self.pos is None:\n#             self.set_layout('spring')\n            \n#         plt.figure(figsize=figsize)\n        \n#         # Get or use provided node attributes\n#         node_colors = node_color if node_color is not None else self._get_node_colors()\n#         node_sizes = node_size if node_size is not None else self._get_node_sizes()\n#         edge_colors = edge_color if edge_color is not None else self._get_edge_colors()\n        \n#         # Draw nodes\n#         nx.draw_networkx_nodes(self.graph, self.pos,\n#                              node_color=[node_colors[node] for node in self.graph.nodes()],\n#                              node_size=[node_sizes[node] for node in self.graph.nodes()],\n#                              alpha=alpha)\n        \n#         # Draw edges\n#         for (u, v) in self.graph.edges():\n#             nx.draw_networkx_edges(self.graph, self.pos,\n#                                  edgelist=[(u, v)],\n#                                  edge_color=edge_colors.get((u, v), 'gray'),\n#                                  alpha=0.5)\n        \n#         # Add labels if requested\n#         if with_labels:\n#             labels = self._get_node_labels()\n#             nx.draw_networkx_labels(self.graph, self.pos, labels,\n#                                   font_size=font_size)\n        \n#         # Add edge labels if requested\n#         if show_edge_labels:\n#             edge_labels = self._get_edge_labels()\n#             if edge_labels:\n#                 nx.draw_networkx_edge_labels(self.graph, self.pos,\n#                                            edge_labels=edge_labels,\n#                                            font_size=font_size-2)\n        \n#         if title:\n#             plt.title(title)\n        \n#         plt.axis('off')\n        \n#         if save_path:\n#             plt.savefig(save_path, bbox_inches='tight', dpi=300)\n#             plt.close()\n#         else:\n#             plt.show()\n\n# # Example usage:\n# '''\n# # Create a sample graph\n# G = nx.Graph()\n# G.add_nodes_from([\n#     (1, {'fillcolor': 'lightblue', 'label': 'Node 1'}),\n#     (2, {'fillcolor': 'lightgreen', 'label': 'Node 2'}),\n#     (3, {'fillcolor': 'lightred', 'label': 'Node 3'})\n# ])\n# G.add_edges_from([\n#     (1, 2, {'color': 'blue', 'label': 'Edge 1-2'}),\n#     (2, 3, {'color': 'red', 'label': 'Edge 2-3'})\n# ])\n# '''\n# # Create visualizer and display graph\n# visualizer = GraphVisualizer(G)\n# visualizer.set_layout('spring', k=2)  # k controls the spacing between nodes\n# visualizer.visualize(\n#     figsize=(10, 8),\n#     font_size=10,\n#     title=\"Sample Graph Visualization\",\n# )\n","metadata":{"execution":{"iopub.execute_input":"2025-03-01T04:39:46.921588Z","iopub.status.busy":"2025-03-01T04:39:46.921252Z","iopub.status.idle":"2025-03-01T04:40:44.405541Z","shell.execute_reply":"2025-03-01T04:40:44.404664Z","shell.execute_reply.started":"2025-03-01T04:39:46.921552Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def visualize_codebase_graph(codebase_path, output_html=\"codebase_graph.html\", limit_nodes=None):\n    \"\"\"\n    Visualize the codebase graph using pyvis.\n    \n    Args:\n        codebase_path: Path to the codebase directory\n        output_html: Output HTML file for the visualization\n        limit_nodes: Optional limit on the number of nodes to display (for large codebases)\n    \"\"\"\n    # Initialize and build the graph\n    visualizer = CodebaseVisualizer(codebase_path)\n    visualizer.parse_files()\n    graph = visualizer.build_graph()\n    \n    # Export graph to JSON (optional)\n    visualizer.export_graph_json('codebase_graph.json')\n    \n    # Create a pyvis network\n    net = Network(height=\"900px\", width=\"100%\", bgcolor=\"#222222\", font_color=\"white\")\n    \n    # Configure physics\n    net.barnes_hut(gravity=-5000, central_gravity=0.3, spring_length=200)\n    \n    # Define node groups and colors\n    node_colors = {\n        'file': '#4287f5',\n        'directory': '#42f5a7',\n        'symbol': '#f542cb',\n        'snippet': '#f5a742'\n    }\n    \n    # If we need to limit nodes for performance\n    if limit_nodes and len(graph.nodes()) > limit_nodes:\n        # Focus on file and directory nodes, and limit symbol nodes\n        important_nodes = [node for node, data in graph.nodes(data=True) \n                          if data.get('type') in ['file', 'directory']]\n        \n        # Add some symbol nodes to reach the limit\n        symbols = [node for node, data in graph.nodes(data=True) \n                  if data.get('type') == 'symbol']\n        \n        # Take a subset of symbols based on connectivity\n        symbol_importance = sorted(\n            [(node, graph.degree(node)) for node in symbols],\n            key=lambda x: x[1],\n            reverse=True\n        )\n        \n        important_symbols = [node for node, _ in symbol_importance[:limit_nodes - len(important_nodes)]]\n        selected_nodes = important_nodes + important_symbols\n        \n        # Create a subgraph\n        graph = graph.subgraph(selected_nodes)\n    \n    # Add nodes with appropriate styles\n    for node, node_data in graph.nodes(data=True):\n        node_type = node_data.get('type', 'unknown')\n        label = os.path.basename(node) if '/' in node else node\n        \n        # Truncate very long labels\n        if len(label) > 30:\n            label = label[:27] + \"...\"\n        \n        # Create hover title with more details\n        title = f\"<div style='max-width:300px;'>\"\n        title += f\"<b>{node}</b><br>\"\n        title += f\"Type: {node_type}<br>\"\n        \n        if node_type == 'file':\n            title += f\"Directory: {node_data.get('directory', 'N/A')}<br>\"\n            \n        elif node_type == 'symbol':\n            title += f\"Symbol type: {node_data.get('symbol_type', 'N/A')}<br>\"\n            title += f\"Line: {node_data.get('line_number', 'N/A')}<br>\"\n            \n            if node_data.get('docstring'):\n                docstring = node_data['docstring']\n                if len(docstring) > 200:\n                    docstring = docstring[:197] + \"...\"\n                title += f\"Docstring: {docstring}<br>\"\n                \n        elif node_type == 'snippet':\n            title += f\"Lines: {node_data.get('start_line', 'N/A')}-{node_data.get('end_line', 'N/A')}<br>\"\n            \n            if node_data.get('code_snippet'):\n                snippet = node_data['code_snippet'].replace('\\n', '<br>')\n                if len(snippet) > 300:\n                    snippet = snippet[:297] + \"...\"\n                title += f\"Code:<br><pre>{snippet}</pre>\"\n                \n        title += \"</div>\"\n        \n        # Determine node size based on type and connections\n        size = 15  # Default size\n        if node_type == 'directory':\n            size = 25\n        elif node_type == 'file':\n            size = 20\n        elif node_type == 'symbol' and node_data.get('symbol_type') == 'class':\n            size = 18\n        \n        # Add node with appropriate styling\n        net.add_node(\n            node, \n            label=label, \n            title=title,\n            color=node_colors.get(node_type, '#999999'),\n            size=size,\n            shape='dot' if node_type != 'directory' else 'diamond'\n        )\n    \n    # Add edges with appropriate styles\n    for source, target, edge_data in graph.edges(data=True):\n        edge_type = edge_data.get('edge_type', 'unknown')\n        \n        # Style edges differently based on type\n        if edge_type == 'import':\n            color = '#f5f542'\n            width = 2\n            dashes = False\n        elif edge_type == 'references':\n            color = '#f54242'\n            width = 1\n            dashes = [5, 5]\n        elif edge_type == 'defines':\n            color = '#42f55a'\n            width = 3\n            dashes = False\n        elif edge_type == 'contains_snippet':\n            color = '#42c8f5'\n            width = 1\n            dashes = [2, 2]\n        else:\n            color = '#999999'\n            width = 1\n            dashes = False\n        \n        # Create hover title with edge details\n        title = f\"<div><b>{edge_type}</b><br>\"\n        \n        if edge_data.get('line_number'):\n            title += f\"Line: {edge_data['line_number']}<br>\"\n            \n        if edge_data.get('context'):\n            context = edge_data['context']\n            if len(context) > 200:\n                context = context[:197] + \"...\"\n            title += f\"Context: {context}\"\n            \n        title += \"</div>\"\n        \n        # Add edge with styling\n        net.add_edge(\n            source, \n            target, \n            title=title,\n            color=color,\n            width=width,\n            dashes=dashes\n        )\n    \n    # Enable physics, navigation and interaction options\n    net.toggle_physics(True)\n    net.show_buttons(filter_=['physics'])\n    \n    # Save the visualization\n    net.save_graph(output_html)\n    print(f\"Graph visualization saved to {output_html}\")\n    \n    return output_html\n\ndef visualize_symbol_subgraph(visualizer, symbol_name, output_html=\"symbol_graph.html\"):\n    \"\"\"\n    Create a focused visualization of a symbol and its relationships.\n    \n    Args:\n        visualizer: Initialized CodebaseVisualizer instance\n        symbol_name: Name of the symbol to visualize\n        output_html: Output HTML file for the visualization\n    \"\"\"\n    # Get the full graph\n    full_graph = visualizer.graph\n    \n    # Find all nodes related to this symbol\n    symbol_nodes = [node for node in full_graph.nodes() if f\"::{symbol_name}\" in node]\n    \n    if not symbol_nodes:\n        print(f\"Symbol '{symbol_name}' not found in the graph.\")\n        return None\n    \n    # Get nodes that are connected to symbol nodes (1-hop neighborhood)\n    related_nodes = set(symbol_nodes)\n    for node in symbol_nodes:\n        # Add predecessors (nodes that reference this symbol)\n        related_nodes.update(full_graph.predecessors(node))\n        # Add successors (nodes that this symbol references)\n        related_nodes.update(full_graph.successors(node))\n    \n    # Create a subgraph\n    subgraph = full_graph.subgraph(related_nodes)\n    \n    # Create a pyvis network\n    net = Network(height=\"750px\", width=\"100%\", bgcolor=\"#222222\", font_color=\"white\")\n    net.barnes_hut(gravity=-2000, central_gravity=0.3, spring_length=150)\n    \n    # Node colors by type\n    node_colors = {\n        'file': '#4287f5',\n        'directory': '#42f5a7',\n        'symbol': '#f542cb',\n        'snippet': '#f5a742'\n    }\n    \n    # Add nodes with appropriate styles\n    for node, node_data in subgraph.nodes(data=True):\n        node_type = node_data.get('type', 'unknown')\n        label = os.path.basename(node) if '/' in node else node\n        \n        # Make the focus symbol nodes larger and highlighted\n        if node in symbol_nodes:\n            size = 30\n            color = '#ff0000'  # Bright red for focus\n        else:\n            size = 15\n            color = node_colors.get(node_type, '#999999')\n        \n        # Add hover information\n        title = f\"<div style='max-width:300px;'>\"\n        title += f\"<b>{node}</b><br>\"\n        title += f\"Type: {node_type}<br>\"\n        \n        if node_type == 'symbol':\n            title += f\"Symbol type: {node_data.get('symbol_type', 'N/A')}<br>\"\n            if node_data.get('docstring'):\n                title += f\"Docstring: {node_data.get('docstring', '')}<br>\"\n                \n        title += \"</div>\"\n        \n        # Add node with styling\n        net.add_node(\n            node,\n            label=label,\n            title=title,\n            color=color,\n            size=size\n        )\n    \n    # Add edges with styles\n    for source, target, edge_data in subgraph.edges(data=True):\n        edge_type = edge_data.get('edge_type', 'unknown')\n        \n        # Style edges by type\n        if edge_type == 'import':\n            color = '#f5f542'  # Yellow\n        elif edge_type == 'references':\n            color = '#f54242'  # Red\n        elif edge_type == 'defines':\n            color = '#42f55a'  # Green\n        else:\n            color = '#999999'  # Gray\n        \n        # Add edge with details in hover\n        title = f\"{edge_type}\"\n        if edge_data.get('line_number'):\n            title += f\" (line {edge_data['line_number']})\"\n            \n        net.add_edge(source, target, title=title, color=color)\n    \n    # Enable physics and navigation\n    net.toggle_physics(True)\n    net.show_buttons(filter_=['physics'])\n    \n    # Save the visualization\n    net.save_graph(output_html)\n    print(f\"Symbol graph visualization saved to {output_html}\")\n    \n    return output_html\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T11:49:49.875434Z","iopub.execute_input":"2025-03-01T11:49:49.875758Z","iopub.status.idle":"2025-03-01T11:49:49.893874Z","shell.execute_reply.started":"2025-03-01T11:49:49.875735Z","shell.execute_reply":"2025-03-01T11:49:49.893013Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"### Visualize graph","metadata":{}},{"cell_type":"code","source":"# Visualize the entire codebase graph (with node limit for performance)\n#visualize_codebase_graph(\"flask\", limit_nodes=200)\n\n# Visualize a specific symbol (like \"has_level_handler\")\nvisualizer = CodebaseVisualizer(\"flask\")\nvisualizer.parse_files()\nG = visualizer.build_graph()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T11:54:20.555647Z","iopub.execute_input":"2025-03-01T11:54:20.555980Z","iopub.status.idle":"2025-03-01T11:54:22.195505Z","shell.execute_reply.started":"2025-03-01T11:54:20.555958Z","shell.execute_reply":"2025-03-01T11:54:22.194779Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"### Make Database\n","metadata":{}},{"cell_type":"code","source":"\ndb = ArangoClient(hosts=\"https://d2eeb8083350.arangodb.cloud:8529\").db(username=\"root\", password=\"cUZ0YaNdcwfUTw6VjRny\", verify=True)\n\nprint(db)\n","metadata":{"execution":{"iopub.status.busy":"2025-03-01T11:54:26.519967Z","iopub.execute_input":"2025-03-01T11:54:26.520322Z","iopub.status.idle":"2025-03-01T11:54:26.947993Z","shell.execute_reply.started":"2025-03-01T11:54:26.520291Z","shell.execute_reply":"2025-03-01T11:54:26.947235Z"},"trusted":true},"outputs":[{"name":"stdout","text":"<StandardDatabase _system>\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"G_adb = nxadb.Graph(\n    name=\"FlaskRepv1\",\n    db=db,\n    incoming_graph_data=G,\n    write_batch_size=50000, # feel free to modify\n    overwrite_graph=True\n)\n\nprint(G_adb)","metadata":{"execution":{"iopub.status.busy":"2025-03-01T11:54:28.634148Z","iopub.execute_input":"2025-03-01T11:54:28.634464Z","iopub.status.idle":"2025-03-01T11:54:33.261601Z","shell.execute_reply.started":"2025-03-01T11:54:28.634441Z","shell.execute_reply":"2025-03-01T11:54:33.260568Z"},"trusted":true},"outputs":[{"name":"stderr","text":"[11:54:28 +0000] [INFO]: Overwriting graph 'FlaskRepv1'\n[11:54:29 +0000] [INFO]: Graph 'FlaskRepv1' exists.\n[11:54:29 +0000] [INFO]: Default node type set to 'FlaskRepv1_node'\n[2025/03/01 11:54:30 +0000] [31] [INFO] - adbnx_adapter: Instantiated ADBNX_Adapter with database '_system'\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8160699790804ffc9c70b794b5cc0e59"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18d8696d014746898fec990f2870f9ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"},"metadata":{}},{"name":"stderr","text":"[2025/03/01 11:54:32 +0000] [31] [INFO] - adbnx_adapter: Created ArangoDB 'FlaskRepv1' Graph\n","output_type":"stream"},{"name":"stdout","text":"Graph named 'FlaskRepv1' with 2975 nodes and 8652 edges\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"### Run from loaded database","metadata":{}},{"cell_type":"code","source":"db = ArangoClient(hosts=\"https://d2eeb8083350.arangodb.cloud:8529\").db(username=\"root\", password=\"cUZ0YaNdcwfUTw6VjRny\", verify=True)\n\nprint(db)","metadata":{"execution":{"iopub.execute_input":"2025-02-23T12:32:04.625116Z","iopub.status.busy":"2025-02-23T12:32:04.624787Z","iopub.status.idle":"2025-02-23T12:32:05.367964Z","shell.execute_reply":"2025-02-23T12:32:05.367236Z","shell.execute_reply.started":"2025-02-23T12:32:04.625091Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"G_adb = nxadb.Graph(\n    name=\"FlaskRepv1\",\n    db=db,\n    #incoming_graph_data=G,\n    #write_batch_size=50000 # feel free to modify\n)\n\nprint(G_adb)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"arango_graph = ArangoGraph(db)","metadata":{"execution":{"iopub.status.busy":"2025-03-01T11:54:42.405100Z","iopub.execute_input":"2025-03-01T11:54:42.405436Z","iopub.status.idle":"2025-03-01T11:54:44.079645Z","shell.execute_reply.started":"2025-03-01T11:54:42.405409Z","shell.execute_reply":"2025-03-01T11:54:44.078967Z"},"trusted":true},"outputs":[],"execution_count":18},{"cell_type":"code","source":"os.environ[\"MISTRAL_API_KEY\"]=\"jJAuJZkjVcy2ynUhan375sHNviHiBeJU\"","metadata":{"execution":{"iopub.status.busy":"2025-03-01T11:54:45.273322Z","iopub.execute_input":"2025-03-01T11:54:45.273634Z","iopub.status.idle":"2025-03-01T11:54:45.277326Z","shell.execute_reply.started":"2025-03-01T11:54:45.273613Z","shell.execute_reply":"2025-03-01T11:54:45.276439Z"},"trusted":true},"outputs":[],"execution_count":19},{"cell_type":"code","source":"from langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(\n    model=\"mistral-large-latest\",\n    temperature=0,\n    max_retries=2,\n    # other params...\n)\nllm.invoke(\"Hello\")","metadata":{"execution":{"iopub.status.busy":"2025-03-01T11:54:47.697456Z","iopub.execute_input":"2025-03-01T11:54:47.697764Z","iopub.status.idle":"2025-03-01T11:54:48.626344Z","shell.execute_reply.started":"2025-03-01T11:54:47.697743Z","shell.execute_reply":"2025-03-01T11:54:48.625612Z"},"trusted":true},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"AIMessage(content='Hello! How can I assist you today?', additional_kwargs={}, response_metadata={'token_usage': {'prompt_tokens': 4, 'total_tokens': 13, 'completion_tokens': 9}, 'model': 'mistral-large-latest', 'finish_reason': 'stop'}, id='run-46510b8d-0909-4f86-8810-fb3e65d489db-0', usage_metadata={'input_tokens': 4, 'output_tokens': 9, 'total_tokens': 13})"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"\n# @tool\n# def text_to_nx_algorithm_to_text(query):\n#     \"\"\"This tool is available to invoke a NetworkX Algorithm on\n#     the ArangoDB Graph. You are responsible for accepting the\n#     Natural Language Query, establishing which algorithm needs to\n#     be executed, executing the algorithm, and translating the results back\n#     to Natural Language, with respect to the original query.\n\n#     If the query (e.g traversals, shortest path, etc.) can be solved using the Arango Query Language, then do not use\n#     this tool.\n#     \"\"\"\n#     llm = ChatMistralAI(\n#         model=\"mistral-large-latest\",\n#         temperature=0,\n#         max_retries=2,\n#         # other params...\n#     )\n#     ######################\n#     print(\"1) Generating NetworkX code\")\n\n#     text_to_nx = llm.invoke(f\"\"\"\n#     I have a NetworkX Graph called `G_adb`. It has the following schema: {arango_graph.schema}\n\n#     I have the following graph analysis query: {query}.\n\n#     Generate the Python Code required to answer the query using the `G_adb` object.\n    \n#     It should give code so that is gives ALL the necessary contents that use this part of the code in terms of ALL nested imports. Consider the nested directories and sub-directory informations as well.\n\n#     Be very precise on the NetworkX algorithm you select to answer this query. Think step by step.\n\n#     Only assume that networkx is installed, and other base python dependencies.\n\n#     Always set the last variable as `FINAL_RESULT`, which represents the answer to the original query.\n\n#     Only provide python code that I can directly execute via `exec()`. Do not provide any instructions.\n\n#     Make sure that `FINAL_RESULT` stores a short & consice answer. Avoid setting this variable to a long sequence.\n\n#     Your code:\n#     \"\"\").content\n\n#     text_to_nx_cleaned = re.sub(r\"^```python\\n|```$\", \"\", text_to_nx, flags=re.MULTILINE).strip()\n    \n#     print('-'*10)\n#     print(text_to_nx_cleaned)\n#     print('-'*10)\n\n#     ######################\n\n#     print(\"\\n2) Executing NetworkX code\")\n#     global_vars = {\"G_adb\": G_adb, \"nx\": nx}\n#     local_vars = {}\n\n#     try:\n#         exec(text_to_nx_cleaned, global_vars, local_vars)\n#         text_to_nx_final = text_to_nx\n#     except Exception as e:\n#         print(f\"EXEC ERROR: {e}\")\n#         return f\"EXEC ERROR: {e}\"\n\n#         # TODO: Consider experimenting with a code corrector!\n#         attempt = 1\n#         MAX_ATTEMPTS = 3\n\n#         # while attempt <= MAX_ATTEMPTS\n#             # ...\n\n#     print('-'*10)\n#     FINAL_RESULT = local_vars[\"FINAL_RESULT\"]\n#     print(f\"FINAL_RESULT: {FINAL_RESULT}\")\n#     print('-'*10)\n\n#     ######################\n\n#     print(\"3) Formulating final answer\")\n\n#     nx_to_text = llm.invoke(f\"\"\"\n#         I have a NetworkX Graph called `G_adb`. It has the following schema: {arango_graph.schema}\n\n#         I have the following graph analysis query: {query}.\n\n#         I have executed the following python code to help me answer my query:\n\n#         ---\n#         {text_to_nx_final}\n#         ---\n\n#         The `FINAL_RESULT` variable is set to the following: {FINAL_RESULT}.\n\n#         Based on my original Query and FINAL_RESULT, generate a short and concise response to\n#         answer my query.\n        \n#         Your response:\n#     \"\"\").content\n\n#     return nx_to_text","metadata":{"execution":{"iopub.execute_input":"2025-03-01T04:43:31.747340Z","iopub.status.busy":"2025-03-01T04:43:31.747045Z","iopub.status.idle":"2025-03-01T04:43:31.757820Z","shell.execute_reply":"2025-03-01T04:43:31.756946Z","shell.execute_reply.started":"2025-03-01T04:43:31.747317Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Analysing the imported Graph via the prompt","metadata":{}},{"cell_type":"code","source":"import networkx as nx\nfrom arango import ArangoClient\nimport re\n\ndef analyze_networkx_graph(G, query_text):\n    \"\"\"\n    Process a natural language query directly against a NetworkX graph\n    \n    Args:\n        G: NetworkX graph object\n        query_text: Natural language query about the codebase\n    \n    Returns:\n        Dictionary with analysis results in a human-readable format\n    \"\"\"\n    # Clean the query to extract the actual symbol being sought\n    clean_query = query_text.lower().strip()\n    \n    # Common English words to filter out\n    common_words = {\"what\", \"is\", \"the\", \"use\", \"of\", \"in\", \"where\", \"how\", \"why\", \"when\", \n                    \"who\", \"which\", \"does\", \"do\", \"are\", \"can\", \"could\", \"would\", \"should\", \n                    \"function\", \"method\", \"class\", \"variable\", \"codebase\", \"code\", \"used\", \n                    \"defined\", \"implemented\", \"called\", \"referenced\"}\n    \n    # Find all words that could be symbols\n    potential_symbols = []\n    for word in clean_query.split():\n        word = word.strip(\".,?!()[]{}'\\\"\\n\\t\")\n        if len(word) > 2 and word.lower() not in common_words:\n            potential_symbols.append(word)\n    \n    # Also look for multi-word symbols with underscores\n    for i in range(len(clean_query.split()) - 1):\n        compound = '_'.join(clean_query.split()[i:i+2])\n        if '_' in compound and compound not in potential_symbols:\n            potential_symbols.append(compound)\n    \n    # Extract exact symbol if passed directly\n    if query_text.strip() and len(query_text.strip().split()) == 1 and '_' in query_text:\n        # User likely just passed the symbol name directly\n        potential_symbols = [query_text.strip()]\n    \n    # Find matches in the graph\n    candidates = []\n    \n    # First, look for exact node IDs or node names\n    for symbol in potential_symbols:\n        for node_id, node_data in G.nodes(data=True):\n            # Check if this is a symbol node\n            if node_data.get('type') == 'symbol':\n                # Check the node ID\n                if isinstance(node_id, str) and symbol.lower() in node_id.lower():\n                    candidates.append((node_id, symbol, 1.0))  # 1.0 = high confidence\n                \n                # Check if symbol appears in the context (code snippet)\n                context = node_data.get('context', '')\n                if context and symbol.lower() in context.lower():\n                    # Higher confidence if it appears as a variable assignment\n                    patterns = [\n                        f\"self.{symbol}\", \n                        f\"{symbol} =\", \n                        f\"def {symbol}\", \n                        f\"class {symbol}\"\n                    ]\n                    score = 0.8  # Base score\n                    for pattern in patterns:\n                        if pattern.lower() in context.lower():\n                            score = 0.9  # Higher confidence\n                            break\n                    candidates.append((node_id, symbol, score))\n    \n    # Second, check for symbol references in edge contexts\n    if not candidates:\n        for source, target, edge_data in G.edges(data=True):\n            edge_type = edge_data.get('edge_type')\n            if edge_type in ['references', 'defines']:\n                context = edge_data.get('context', '')\n                for symbol in potential_symbols:\n                    if context and symbol.lower() in context.lower():\n                        patterns = [\n                            f\"self.{symbol}\", \n                            f\"{symbol} =\", \n                            f\"def {symbol}\", \n                            f\"class {symbol}\"\n                        ]\n                        score = 0.7  # Base score for edges\n                        for pattern in patterns:\n                            if pattern.lower() in context.lower():\n                                score = 0.8  # Higher confidence\n                                break\n                        candidates.append((target, symbol, score))\n    \n    # Third, search in snippets\n    if not candidates:\n        for node_id, node_data in G.nodes(data=True):\n            if node_data.get('type') == 'snippet':\n                code_snippet = node_data.get('code_snippet', '')\n                for symbol in potential_symbols:\n                    if code_snippet and symbol.lower() in code_snippet.lower():\n                        patterns = [\n                            f\"self.{symbol}\", \n                            f\"{symbol} =\", \n                            f\"def {symbol}\", \n                            f\"class {symbol}\"\n                        ]\n                        score = 0.6  # Base score for snippets\n                        for pattern in patterns:\n                            if pattern.lower() in code_snippet.lower():\n                                score = 0.7  # Higher confidence\n                                break\n                        # Find the file this snippet belongs to\n                        file_nodes = []\n                        for source, target, edge_data in G.in_edges(node_id, data=True):\n                            if edge_data.get('edge_type') == 'contains_snippet':\n                                file_nodes.append(source)\n                        \n                        # Create a pseudo symbol node ID using the file\n                        if file_nodes:\n                            pseudo_id = f\"{file_nodes[0]}::{symbol}\"\n                            candidates.append((pseudo_id, symbol, score))\n                        else:\n                            candidates.append((node_id, symbol, score))\n    \n    # No matches found\n    if not candidates:\n        return {\"response\": f\"Could not find a matching symbol in the codebase for '{query_text}'. Please try rephrasing your query with a specific function, class, or variable name.\"}\n    \n    # Sort candidates by confidence score\n    candidates.sort(key=lambda x: x[2], reverse=True)\n    \n    # Choose the best candidate\n    node_id, symbol_name, _ = candidates[0]\n    \n    # Find all usages of the identified symbol\n    symbol_usages = find_symbol_usage_nx(G, node_id, symbol_name)\n    \n    # Generate response\n    if \"error\" in symbol_usages:\n        # Try a broader search if the specific node wasn't found\n        broader_usages = search_symbol_in_contexts(G, symbol_name)\n        if broader_usages:\n            return {\"response\": broader_usages}\n        return {\"response\": symbol_usages[\"error\"]}\n    \n    # Collect information about the symbol\n    definition_files = symbol_usages.get(\"defined_in\", [])\n    usage_info = symbol_usages.get(\"usages\", {})\n    symbol_type = symbol_usages.get(\"symbol_type\", \"variable\")  # Default to variable\n    \n    # Build human-readable response\n    response = f\"Symbol: '{symbol_name}' (Type: {symbol_type})\\n\\n\"\n    response += f\"Defined in: {', '.join(definition_files) if definition_files else 'No definition location found'}\\n\\n\"\n    \n    if symbol_usages.get(\"docstring\"):\n        response += f\"Documentation:\\n{symbol_usages['docstring']}\\n\\n\"\n    \n    response += \"Used in the following locations:\\n\"\n    \n    if not usage_info:\n        response += \"\\nNo usage information found for this symbol in graph nodes.\\n\"\n        \n        # Try a broader search in contexts\n        broader_usages = search_symbol_in_contexts(G, symbol_name)\n        if broader_usages:\n            response += \"\\nHowever, found these mentions in code snippets:\\n\\n\"\n            response += broader_usages\n    else:\n        for file, usages in usage_info.items():\n            response += f\"\\nFile: {file}\\n\"\n            for usage in usages:\n                line = usage.get('line', 'unknown line')\n                context = usage.get('context', 'No context available')\n                response += f\"- Line {line}: {context}\\n\"\n    \n    # Add related symbols if available\n    if symbol_usages.get(\"related_symbols\"):\n        response += \"\\nRelated symbols:\\n\"\n        for related in symbol_usages[\"related_symbols\"][:5]:  # Limit to top 5\n            response += f\"- {related}\\n\"\n    \n    # Add graph statistics\n    response += f\"\\nAnalysis performed on graph with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\"\n    \n    return {\"response\": response}\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T11:56:13.148384Z","iopub.execute_input":"2025-03-01T11:56:13.148710Z","iopub.status.idle":"2025-03-01T11:56:13.164746Z","shell.execute_reply.started":"2025-03-01T11:56:13.148689Z","shell.execute_reply":"2025-03-01T11:56:13.163879Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"### Searching for the symbol in both context, networkx format and file","metadata":{}},{"cell_type":"code","source":"def search_symbol_in_contexts(G, symbol_name):\n    \"\"\"\n    Search for a symbol in all contexts (code snippets) in the graph\n    \n    Args:\n        G: NetworkX graph\n        symbol_name: Name of the symbol to search\n    \n    Returns:\n        String with usage information\n    \"\"\"\n    response = \"\"\n    found = False\n    \n    # Look in snippet nodes\n    for node_id, node_data in G.nodes(data=True):\n        if node_data.get('type') == 'snippet':\n            code_snippet = node_data.get('code_snippet', '')\n            if code_snippet and symbol_name.lower() in code_snippet.lower():\n                found = True\n                file_name = \"Unknown\"\n                # Find which file this snippet belongs to\n                for source, target, edge_data in G.in_edges(node_id, data=True):\n                    if edge_data.get('edge_type') == 'contains_snippet':\n                        file_name = source\n                        break\n                \n                start_line = node_data.get('start_line', 'unknown')\n                response += f\"\\nFile: {file_name} (Lines {start_line}-{node_data.get('end_line', 'unknown')})\\n\"\n                \n                # Extract the lines containing the symbol\n                lines = code_snippet.split('\\n')\n                for i, line in enumerate(lines):\n                    if symbol_name.lower() in line.lower():\n                        line_num = int(start_line) + i if isinstance(start_line, int) else \"?\"\n                        response += f\"- Line {line_num}: {line.strip()}\\n\"\n    \n    # Look in edge contexts\n    for source, target, edge_data in G.edges(data=True):\n        context = edge_data.get('context', '')\n        if context and symbol_name.lower() in context.lower():\n            found = True\n            edge_type = edge_data.get('edge_type', 'unknown')\n            line_num = edge_data.get('line_number', 'unknown')\n            \n            # For references or defines edges, source is usually the file\n            file_name = source if edge_type in ['references', 'defines'] else \"Unknown\"\n            \n            response += f\"\\nFile: {file_name} (Line {line_num})\\n\"\n            response += f\"- {context.strip()}\\n\"\n    \n    if found:\n        return response\n    else:\n        return \"\"\n\ndef find_symbol_usage_nx(graph, node_id, symbol_name):\n    \"\"\"\n    Find all usages of a specific symbol across the codebase using NetworkX graph\n    \n    Args:\n        graph: The NetworkX graph object\n        node_id: The node ID of the symbol node in the graph\n        symbol_name: The name of the symbol for display purposes\n    \n    Returns:\n        Dictionary with files and line numbers where the symbol is used\n    \"\"\"\n    # Check if this is a valid node\n    if node_id not in graph:\n        # This could be a pseudo node ID we created for snippet matches\n        if isinstance(node_id, str) and '::' in node_id:\n            file_path = node_id.split('::')[0]\n            # Return information based on file path and symbol name\n            return {\n                \"symbol\": symbol_name,\n                \"symbol_type\": \"variable\",  # Assuming variable as default\n                \"defined_in\": [file_path],\n                \"docstring\": \"\",\n                \"usages\": search_symbol_in_file(graph, file_path, symbol_name),\n                \"related_symbols\": []\n            }\n        return {\"error\": f\"Symbol '{symbol_name}' not found in the codebase as a specific node\"}\n    \n    # Get symbol node data\n    node_data = graph.nodes[node_id]\n    \n    # Extract symbol type and other metadata\n    symbol_type = node_data.get('symbol_type', 'variable')  # Default to variable\n    docstring = node_data.get('docstring', '')\n    \n    # Find definition locations\n    definitions = []\n    for source, target, edge_data in graph.in_edges(node_id, data=True):\n        if edge_data.get('edge_type') == 'defines':\n            # Source should be a file node\n            definitions.append(source)\n    \n    # If no definitions found through edges, extract from node ID\n    if not definitions and isinstance(node_id, str) and '::' in node_id:\n        file_path = node_id.split('::')[0]\n        definitions.append(file_path)\n    \n    # Find all references to the symbol\n    usages = {}\n    for source, target, edge_data in graph.in_edges(node_id, data=True):\n        edge_type = edge_data.get('edge_type')\n        \n        # Consider references\n        if edge_type == 'references':\n            # Source should be a file node\n            file_name = source\n            \n            # Extract line info and context\n            line_info = edge_data.get('line_number', 'unknown line')\n            context = edge_data.get('context', 'No context available')\n            \n            if file_name not in usages:\n                usages[file_name] = []\n            \n            usages[file_name].append({\n                'line': line_info,\n                'context': context\n            })\n    \n    # Find related symbols (e.g., symbols in the same file)\n    related_symbols = []\n    for def_file in definitions:\n        for source, target, edge_data in graph.out_edges(def_file, data=True):\n            if edge_data.get('edge_type') == 'defines' and target != node_id:\n                # Extract symbol name from target node ID\n                if isinstance(target, str) and '::' in target:\n                    related_symbol = target.split('::')[1]\n                    related_symbols.append(related_symbol)\n    \n    return {\n        \"symbol\": symbol_name,\n        \"symbol_type\": symbol_type,\n        \"defined_in\": definitions,\n        \"docstring\": docstring,\n        \"usages\": usages,\n        \"related_symbols\": related_symbols\n    }\n\ndef search_symbol_in_file(graph, file_path, symbol_name):\n    \"\"\"\n    Search for uses of a symbol within a specific file\n    \n    Args:\n        graph: NetworkX graph\n        file_path: Path of the file to search in\n        symbol_name: Name of the symbol to search for\n    \n    Returns:\n        Dictionary with usage information\n    \"\"\"\n    usages = {}\n    \n    # Look for snippet nodes from this file\n    for node_id, node_data in graph.nodes(data=True):\n        if node_data.get('type') == 'snippet':\n            # Check if this snippet belongs to the file\n            belongs_to_file = False\n            for source, target, edge_data in graph.in_edges(node_id, data=True):\n                if edge_data.get('edge_type') == 'contains_snippet' and source == file_path:\n                    belongs_to_file = True\n                    break\n            \n            if belongs_to_file:\n                code_snippet = node_data.get('code_snippet', '')\n                if code_snippet and symbol_name.lower() in code_snippet.lower():\n                    start_line = node_data.get('start_line', 'unknown')\n                    \n                    if file_path not in usages:\n                        usages[file_path] = []\n                    \n                    # Extract the lines containing the symbol\n                    lines = code_snippet.split('\\n')\n                    for i, line in enumerate(lines):\n                        if symbol_name.lower() in line.lower():\n                            line_num = int(start_line) + i if isinstance(start_line, int) else \"unknown\"\n                            usages[file_path].append({\n                                'line': line_num,\n                                'context': line.strip()\n                            })\n    \n    return usages","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T11:56:17.150480Z","iopub.execute_input":"2025-03-01T11:56:17.150825Z","iopub.status.idle":"2025-03-01T11:56:17.164834Z","shell.execute_reply.started":"2025-03-01T11:56:17.150798Z","shell.execute_reply":"2025-03-01T11:56:17.163909Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"### The text LLM file","metadata":{}},{"cell_type":"code","source":"def text_to_nx_algorithm_to_text(query_text, graph=None, db_connection=None, graph_name=None):\n    \"\"\"\n    Universal entry point for processing natural language queries against a code graph\n    \n    Args:\n        query_text: Natural language query about the codebase\n        graph: NetworkX graph object (if already available)\n        db_connection: ArangoDB connection (if graph should be loaded from ArangoDB)\n        graph_name: Name of the graph in ArangoDB (if db_connection is provided)\n    \n    Returns:\n        Dictionary with analysis results in a human-readable format\n    \"\"\"\n    # Case 1: NetworkX graph is directly provided\n    if graph is not None:\n        if isinstance(graph, nx.Graph):\n            return analyze_networkx_graph(graph, query_text)\n        else:\n            return {\"response\": \"Provided graph object is not a valid NetworkX graph\"}\n    \n    # Case 2: ArangoDB connection is provided but no graph\n    elif db_connection is not None and graph_name is None:\n        try:\n            # Try to list available graphs\n            try:\n                available_graphs = db_connection.graphs()\n                if available_graphs:\n                    graph_name = available_graphs[0][\"name\"]\n                else:\n                    return {\"response\": \"No graphs found in the ArangoDB database\"}\n            except:\n                # Different API for networkx-arangodb\n                try:\n                    import networkx_arangodb as nxadb\n                    # Try a different approach for networkx-arangodb\n                    available_graphs = [g for g in db_connection.graphs()]\n                    if available_graphs:\n                        graph_name = available_graphs[0]\n                    else:\n                        return {\"response\": \"No graphs found in the ArangoDB database\"}\n                except:\n                    return {\"response\": \"Could not retrieve graph list from ArangoDB\"}\n        except Exception as e:\n            return {\"response\": f\"Error retrieving graphs from ArangoDB: {str(e)}\"}\n    \n    # Case 3: ArangoDB connection and graph name are provided\n    if db_connection is not None and graph_name is not None:\n        try:\n            # Try to load the graph using standard ArangoDB driver\n            try:\n                # Check if graph_name is a string\n                if not isinstance(graph_name, str):\n                    if hasattr(graph_name, 'name'):\n                        # It might be a graph object with a name attribute\n                        graph_name = graph_name.name\n                    else:\n                        return {\"response\": \"Graph name must be a string or an object with a name attribute\"}\n                \n                # Try to get the graph from ArangoDB\n                arango_graph = db_connection.graph(graph_name)\n                # Convert to NetworkX using our custom function\n                nx_graph = convert_arango_to_networkx(db_connection, graph_name)\n                return analyze_networkx_graph(nx_graph, query_text)\n            except Exception as e1:\n                # If standard approach fails, try networkx-arangodb\n                try:\n                    import networkx_arangodb as nxadb\n                    graph = nxadb.Graph(name=graph_name, db=db_connection)\n                    nx_graph = graph.to_networkx()\n                    return analyze_networkx_graph(nx_graph, query_text)\n                except Exception as e2:\n                    return {\"response\": f\"Error loading graph from ArangoDB: {str(e1)}\\nAlternative method error: {str(e2)}\"}\n        except Exception as e:\n            return {\"response\": f\"Error accessing ArangoDB: {str(e)}\"}\n    \n    # Case 4: No graph info provided at all\n    return {\"response\": \"Please provide either a NetworkX graph or ArangoDB connection details\"}\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T11:56:20.296554Z","iopub.execute_input":"2025-03-01T11:56:20.296881Z","iopub.status.idle":"2025-03-01T11:56:20.304595Z","shell.execute_reply.started":"2025-03-01T11:56:20.296856Z","shell.execute_reply":"2025-03-01T11:56:20.303580Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"### Converting it to NETWORKX, can be removed for effiency and maintained in arango","metadata":{}},{"cell_type":"code","source":"def convert_arango_to_networkx(db, graph_name):\n    \"\"\"\n    Convert an ArangoDB graph to NetworkX format\n    \n    Args:\n        db: ArangoDB database connection\n        graph_name: Name of the graph in ArangoDB\n    \n    Returns:\n        NetworkX graph object\n    \"\"\"\n    G = nx.DiGraph()\n    \n    try:\n        # Get graph from ArangoDB\n        arango_graph = db.graph(graph_name)\n        \n        # Get graph properties\n        try:\n            graph_properties = arango_graph.properties()\n            edge_definitions = graph_properties.get('edgeDefinitions', [])\n            \n            # Process each vertex collection\n            vertex_collections = set()\n            for edge_def in edge_definitions:\n                vertex_collections.update(edge_def.get('from', []))\n                vertex_collections.update(edge_def.get('to', []))\n                \n            # Add nodes from vertex collections\n            for collection_name in vertex_collections:\n                try:\n                    collection = db.collection(collection_name)\n                    for doc in collection:\n                        node_id = doc['_id']\n                        G.add_node(node_id, **doc)\n                except Exception as e:\n                    print(f\"Error with vertex collection {collection_name}: {str(e)}\")\n            \n            # Process edge collections\n            for edge_def in edge_definitions:\n                edge_collection_name = edge_def.get('collection')\n                if edge_collection_name:\n                    try:\n                        collection = db.collection(edge_collection_name)\n                        for doc in collection:\n                            from_id = doc['_from']\n                            to_id = doc['_to']\n                            G.add_edge(from_id, to_id, **doc)\n                    except Exception as e:\n                        print(f\"Error with edge collection {edge_collection_name}: {str(e)}\")\n                        \n        except Exception as e:\n            # Alternative approach: try to infer collections from naming convention\n            print(f\"Error getting graph properties, trying alternate approach: {str(e)}\")\n            \n            # Common naming patterns for ArangoDB collections\n            node_collection_pattern = re.compile(f\"{re.escape(graph_name)}_node\")\n            edge_collection_pattern = re.compile(f\"{re.escape(graph_name)}_.*_to_.*\")\n            \n            # Try to find matching collections\n            for collection_name in db.collections():\n                if node_collection_pattern.match(collection_name):\n                    try:\n                        collection = db.collection(collection_name)\n                        for doc in collection:\n                            node_id = doc['_id']\n                            G.add_node(node_id, **doc)\n                    except Exception as e:\n                        print(f\"Error with inferred vertex collection {collection_name}: {str(e)}\")\n                        \n                elif edge_collection_pattern.match(collection_name):\n                    try:\n                        collection = db.collection(collection_name)\n                        for doc in collection:\n                            from_id = doc['_from']\n                            to_id = doc['_to']\n                            G.add_edge(from_id, to_id, **doc)\n                    except Exception as e:\n                        print(f\"Error with inferred edge collection {collection_name}: {str(e)}\")\n            \n    except Exception as e:\n        print(f\"Error accessing ArangoDB graph: {str(e)}\")\n    \n    return G","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T11:56:24.256777Z","iopub.execute_input":"2025-03-01T11:56:24.257102Z","iopub.status.idle":"2025-03-01T11:56:24.265398Z","shell.execute_reply.started":"2025-03-01T11:56:24.257079Z","shell.execute_reply":"2025-03-01T11:56:24.264465Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"result = text_to_nx_algorithm_to_text(\n    \"What is the use of has_level_handler in the codebase?\",\n    graph=G,  # Your NetworkX graph\n    db_connection=db\n)\nprint(result['response'])","metadata":{"execution":{"iopub.status.busy":"2025-03-01T11:59:05.742479Z","iopub.execute_input":"2025-03-01T11:59:05.742863Z","iopub.status.idle":"2025-03-01T11:59:05.770985Z","shell.execute_reply.started":"2025-03-01T11:59:05.742833Z","shell.execute_reply":"2025-03-01T11:59:05.770030Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Symbol: 'has_level_handler' (Type: function)\n\nDefined in: src/flask/logging.py\n\nDocumentation:\nCheck if there is a handler in the logging chain that will handle the\ngiven logger's :meth:`effective level <~logging.Logger.getEffectiveLevel>`.\n\nUsed in the following locations:\n\nFile: tests/test_logging.py\n- Line 83:     logger.propagate = True\n\n    handler.setLevel(logging.ERROR)\n    assert not has_level_handler(logger)\n\n\ndef test_log_view_exception(app, client):\n\nRelated symbols:\n- wsgi_errors_stream\n- default_handler\n- create_logger\n- level\n- current\n\nAnalysis performed on graph with 2975 nodes and 8652 edges\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"print(G_adb)","metadata":{"execution":{"iopub.execute_input":"2025-03-01T04:47:01.096433Z","iopub.status.busy":"2025-03-01T04:47:01.096097Z","iopub.status.idle":"2025-03-01T04:47:02.298827Z","shell.execute_reply":"2025-03-01T04:47:02.298082Z","shell.execute_reply.started":"2025-03-01T04:47:01.096408Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"arango_graph.schema","metadata":{"execution":{"iopub.execute_input":"2025-03-01T04:47:03.420389Z","iopub.status.busy":"2025-03-01T04:47:03.420081Z","iopub.status.idle":"2025-03-01T04:47:03.426586Z","shell.execute_reply":"2025-03-01T04:47:03.425816Z","shell.execute_reply.started":"2025-03-01T04:47:03.420367Z"},"trusted":true},"outputs":[],"execution_count":null}]}