{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scopium : Expanding the scope of your codebase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features of Scopium:\n",
    "- Automatic chunking and storing of the entire codebase based on relationship between files(imports, directory levels, symbol types)  \n",
    "- Efficient retrieving system performed with a hybrid approach in mind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset - The codebase needed by the user\n",
    "- When the root directory of the codebase is given as the argument, it converts it to a networkx graph capturing all the mentioned relationship between codes. \n",
    "- This is then loaded to an arangoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step0 - Installs and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nx-arangodb\n",
    "!pip install nx-cugraph-cu12 --extra-index-url https://pypi.nvidia.com # Requires CUDA-capable GPU\n",
    "!pip install --upgrade langchain langchain-community langchain-openai langgraph langchain_mistralai\n",
    "!pip install networkx==3.4\n",
    "!pip install tree-sitter\n",
    "!git clone https://github.com/tree-sitter/tree-sitter-cpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import ast\n",
    "from typing import Dict, Set, List, Tuple, Optional,Any\n",
    "import json\n",
    "from arango import ArangoClient\n",
    "import nx_arangodb as nxadb\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.graphs import ArangoGraph\n",
    "from langchain_community.chains.graph_qa.arangodb import ArangoGraphQAChain\n",
    "from langchain_core.tools import tool\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the graph - \n",
    "- This code takes the root directory of the codebase as the input. \n",
    "- It then builds the graph based on the mentioned features and storing it in appropriate nodes and edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global data structures\n",
    "def initialize_data_structures():\n",
    "    data = {\n",
    "        'root_dir': '',\n",
    "        'graph': nx.DiGraph(),\n",
    "        'file_contents': {},  # file -> content\n",
    "        'import_relations': {},  # file -> [(module, line_no)]\n",
    "        'module_symbols': {},  # file -> {symbol -> {type, line_no, context}}\n",
    "        'symbol_references': {},  # symbol -> [(file, line_no, context)]\n",
    "        'file_index': {},  # Maps files to indices\n",
    "        'current_index': 0,\n",
    "        'directories': set(),\n",
    "        'symbol_index': {},  # symbol -> [{file, type, line_no, context}]\n",
    "        'supported_languages': [\"python\", \"cpp\", \"java\", \"go\"],\n",
    "        'language_extensions': {\n",
    "            \"python\": [\".py\"],\n",
    "            \"cpp\": [\".c\", \".cpp\", \".h\", \".hpp\", \".cc\", \".cxx\", \".hxx\"],\n",
    "            \"java\": [\".java\"],\n",
    "            \"go\": [\".go\"]\n",
    "        }\n",
    "    }\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Core utility functions for code processing\n",
    "- Functions for indexing, chunking, context extraction, and language detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_index(data):\n",
    "    \"\"\"Get next available index for file indexing.\"\"\"\n",
    "    data['current_index'] += 1\n",
    "    return data['current_index']\n",
    "\n",
    "def chunk_code(code, lines_per_chunk=20):\n",
    "    \"\"\"\n",
    "    Chunk the given code into snippets.\n",
    "    Returns a list of dictionaries with 'code_snippet', 'start_line', and 'end_line'.\n",
    "    \"\"\"\n",
    "    lines = code.splitlines()\n",
    "    chunks = []\n",
    "    for i in range(0, len(lines), lines_per_chunk):\n",
    "        chunk_lines = lines[i:i + lines_per_chunk]\n",
    "        chunk = {\n",
    "            'code_snippet': '\\n'.join(chunk_lines),\n",
    "            'start_line': i + 1,\n",
    "            'end_line': i + len(chunk_lines)\n",
    "        }\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "def get_context_around_line(data, file_path, line_no, context_lines=3):\n",
    "    \"\"\"Extract context around a specific line in a file.\"\"\"\n",
    "    if file_path not in data['file_contents']:\n",
    "        return \"\"\n",
    "    \n",
    "    lines = data['file_contents'][file_path].splitlines()\n",
    "    start = max(0, line_no - context_lines - 1)\n",
    "    end = min(len(lines), line_no + context_lines)\n",
    "    \n",
    "    context = \"\\n\".join(lines[start:end])\n",
    "    return context\n",
    "\n",
    "def detect_language(data, file_path):\n",
    "    \"\"\"Detect the programming language of a file based on its extension.\"\"\"\n",
    "    _, ext = os.path.splitext(file_path)\n",
    "    ext = ext.lower()\n",
    "    \n",
    "    for language, extensions in data['language_extensions'].items():\n",
    "        if ext in extensions:\n",
    "            return language\n",
    "            \n",
    "    return \"unknown\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python AST analysis functions\n",
    "- Extract source code from Python AST nodes\n",
    "- Analyze Python files for imports and symbol definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_python_node_source(source, node):\n",
    "    \"\"\"Extract the source code for a Python AST node.\"\"\"\n",
    "    try:\n",
    "        lines = source.splitlines()\n",
    "        if hasattr(node, 'lineno') and hasattr(node, 'end_lineno'):\n",
    "            start = node.lineno - 1\n",
    "            end = getattr(node, 'end_lineno', start + 1)\n",
    "            return '\\n'.join(lines[start:end])\n",
    "        return \"\"\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def analyze_python_file(data, file_path, content):\n",
    "    \"\"\"Analyze a Python file for imports and symbols.\"\"\"\n",
    "    try:\n",
    "        tree = ast.parse(content)\n",
    "        imports = []\n",
    "        symbols = {}\n",
    "\n",
    "        for node in ast.walk(tree):\n",
    "            # Track imports\n",
    "            if isinstance(node, (ast.Import, ast.ImportFrom)):\n",
    "                if isinstance(node, ast.Import):\n",
    "                    for name in node.names:\n",
    "                        imports.append((name.name, node.lineno))\n",
    "                else:  # ImportFrom\n",
    "                    module = node.module if node.module else ''\n",
    "                    for name in node.names:\n",
    "                        imports.append((f\"{module}.{name.name}\" if module else name.name, node.lineno))\n",
    "\n",
    "            # Track defined symbols with line numbers and context\n",
    "            elif isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.Assign)):\n",
    "                if isinstance(node, (ast.FunctionDef, ast.ClassDef)):\n",
    "                    symbol_name = node.name\n",
    "                    symbol_type = 'class' if isinstance(node, ast.ClassDef) else 'function'\n",
    "                    line_no = node.lineno\n",
    "                    context = extract_python_node_source(content, node)\n",
    "                    \n",
    "                    symbols[symbol_name] = {\n",
    "                        'type': symbol_type,\n",
    "                        'line_no': line_no,\n",
    "                        'context': context,\n",
    "                        'docstring': ast.get_docstring(node)\n",
    "                    }\n",
    "                elif isinstance(node, ast.Assign):\n",
    "                    # Handle variable assignments\n",
    "                    for target in node.targets:\n",
    "                        if isinstance(target, ast.Name):\n",
    "                            symbol_name = target.id\n",
    "                            line_no = node.lineno\n",
    "                            context = extract_python_node_source(content, node)\n",
    "                            \n",
    "                            symbols[symbol_name] = {\n",
    "                                'type': 'variable',\n",
    "                                'line_no': line_no,\n",
    "                                'context': context\n",
    "                            }\n",
    "\n",
    "        data['import_relations'][file_path] = imports\n",
    "        data['module_symbols'][file_path] = symbols\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing Python file {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Language-specific code analyzers\n",
    "- Extracts imports, symbols, and structure from C++, Java, and Go files\n",
    "- Uses regex patterns to identify language constructs like classes, functions, and namespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_cpp_file(data, file_path, content):\n",
    "    \"\"\"Analyze a C/C++ file for includes and symbols.\"\"\"\n",
    "    imports = []\n",
    "    symbols = {}\n",
    "    \n",
    "    # Process content line by line\n",
    "    lines = content.splitlines()\n",
    "    \n",
    "    # Regular expressions for C/C++ code analysis\n",
    "    include_pattern = re.compile(r'#include\\s+[<\"]([^>\"]+)[>\"]')\n",
    "    class_pattern = re.compile(r'(?:class|struct)\\s+(\\w+)')\n",
    "    function_pattern = re.compile(r'(\\w+)\\s*\\([^)]*\\)\\s*(?:const|override|final|noexcept)?\\s*(?:{|;)')\n",
    "    namespace_pattern = re.compile(r'namespace\\s+(\\w+)')\n",
    "    \n",
    "    for line_no, line in enumerate(lines, 1):\n",
    "        # Find include statements\n",
    "        include_match = include_pattern.search(line)\n",
    "        if include_match:\n",
    "            imports.append((include_match.group(1), line_no))\n",
    "        \n",
    "        # Find class/struct definitions\n",
    "        class_match = class_pattern.search(line)\n",
    "        if class_match:\n",
    "            class_name = class_match.group(1)\n",
    "            context = get_context_around_line(data, file_path, line_no)\n",
    "            symbols[class_name] = {\n",
    "                'type': 'class',\n",
    "                'line_no': line_no,\n",
    "                'context': context\n",
    "            }\n",
    "        \n",
    "        # Find function definitions (simplified)\n",
    "        function_match = function_pattern.search(line)\n",
    "        if function_match and not line.strip().startswith('#') and not line.strip().startswith('//'):\n",
    "            function_name = function_match.group(1)\n",
    "            # Skip some common keywords that might be mistaken for functions\n",
    "            if function_name not in ['if', 'while', 'for', 'switch', 'return']:\n",
    "                context = get_context_around_line(data, file_path, line_no)\n",
    "                symbols[function_name] = {\n",
    "                    'type': 'function',\n",
    "                    'line_no': line_no,\n",
    "                    'context': context\n",
    "                }\n",
    "        \n",
    "        # Find namespace definitions\n",
    "        namespace_match = namespace_pattern.search(line)\n",
    "        if namespace_match:\n",
    "            namespace_name = namespace_match.group(1)\n",
    "            context = get_context_around_line(data, file_path, line_no)\n",
    "            symbols[namespace_name] = {\n",
    "                'type': 'namespace',\n",
    "                'line_no': line_no,\n",
    "                'context': context\n",
    "            }\n",
    "    \n",
    "    data['import_relations'][file_path] = imports\n",
    "    data['module_symbols'][file_path] = symbols\n",
    "\n",
    "def analyze_java_file(data, file_path, content):\n",
    "    \"\"\"Analyze a Java file for imports and symbols.\"\"\"\n",
    "    imports = []\n",
    "    symbols = {}\n",
    "    \n",
    "    # Process content line by line\n",
    "    lines = content.splitlines()\n",
    "    \n",
    "    # Regular expressions for Java code analysis\n",
    "    package_pattern = re.compile(r'package\\s+([\\w.]+)')\n",
    "    import_pattern = re.compile(r'import\\s+([\\w.]+(?:\\.\\*)?)')\n",
    "    class_pattern = re.compile(r'(?:public|private|protected)?\\s*(?:abstract|final)?\\s*class\\s+(\\w+)')\n",
    "    interface_pattern = re.compile(r'(?:public|private|protected)?\\s*interface\\s+(\\w+)')\n",
    "    method_pattern = re.compile(r'(?:public|private|protected)?\\s*(?:static|final|abstract)?\\s*(?:[\\w<>[\\],\\s]+)\\s+(\\w+)\\s*\\([^)]*\\)')\n",
    "    \n",
    "    for line_no, line in enumerate(lines, 1):\n",
    "        # Find package declaration\n",
    "        package_match = package_pattern.search(line)\n",
    "        if package_match:\n",
    "            package_name = package_match.group(1)\n",
    "            imports.append((package_name, line_no))\n",
    "        \n",
    "        # Find import statements\n",
    "        import_match = import_pattern.search(line)\n",
    "        if import_match:\n",
    "            import_name = import_match.group(1)\n",
    "            imports.append((import_name, line_no))\n",
    "        \n",
    "        # Find class definitions\n",
    "        class_match = class_pattern.search(line)\n",
    "        if class_match:\n",
    "            class_name = class_match.group(1)\n",
    "            context = get_context_around_line(data, file_path, line_no)\n",
    "            symbols[class_name] = {\n",
    "                'type': 'class',\n",
    "                'line_no': line_no,\n",
    "                'context': context\n",
    "            }\n",
    "        \n",
    "        # Find interface definitions\n",
    "        interface_match = interface_pattern.search(line)\n",
    "        if interface_match:\n",
    "            interface_name = interface_match.group(1)\n",
    "            context = get_context_around_line(data, file_path, line_no)\n",
    "            symbols[interface_name] = {\n",
    "                'type': 'interface',\n",
    "                'line_no': line_no,\n",
    "                'context': context\n",
    "            }\n",
    "        \n",
    "        # Find method definitions\n",
    "        method_match = method_pattern.search(line)\n",
    "        if method_match:\n",
    "            method_name = method_match.group(1)\n",
    "            # Skip some common keywords that might be mistaken for methods\n",
    "            if method_name not in ['if', 'while', 'for', 'switch', 'return']:\n",
    "                context = get_context_around_line(data, file_path, line_no)\n",
    "                symbols[method_name] = {\n",
    "                    'type': 'method',\n",
    "                    'line_no': line_no,\n",
    "                    'context': context\n",
    "                }\n",
    "    \n",
    "    data['import_relations'][file_path] = imports\n",
    "    data['module_symbols'][file_path] = symbols\n",
    "\n",
    "def analyze_go_file(data, file_path, content):\n",
    "    \"\"\"Analyze a Go file for imports and symbols.\"\"\"\n",
    "    imports = []\n",
    "    symbols = {}\n",
    "    \n",
    "    # Process content line by line\n",
    "    lines = content.splitlines()\n",
    "    \n",
    "    # Regular expressions for Go code analysis\n",
    "    package_pattern = re.compile(r'package\\s+(\\w+)')\n",
    "    import_single_pattern = re.compile(r'import\\s+\"([^\"]+)\"')\n",
    "    import_multi_start_pattern = re.compile(r'import\\s+\\(')\n",
    "    import_multi_line_pattern = re.compile(r'\\s*\"([^\"]+)\"')\n",
    "    func_pattern = re.compile(r'func\\s+(?:\\([^)]+\\)\\s+)?(\\w+)')\n",
    "    struct_pattern = re.compile(r'type\\s+(\\w+)\\s+struct')\n",
    "    interface_pattern = re.compile(r'type\\s+(\\w+)\\s+interface')\n",
    "    \n",
    "    in_import_block = False\n",
    "    \n",
    "    for line_no, line in enumerate(lines, 1):\n",
    "        # Find package declaration\n",
    "        package_match = package_pattern.search(line)\n",
    "        if package_match:\n",
    "            package_name = package_match.group(1)\n",
    "            imports.append((f\"package {package_name}\", line_no))\n",
    "        \n",
    "        # Handle single-line imports\n",
    "        import_match = import_single_pattern.search(line)\n",
    "        if import_match:\n",
    "            import_name = import_match.group(1)\n",
    "            imports.append((import_name, line_no))\n",
    "        \n",
    "        # Handle multi-line imports\n",
    "        if import_multi_start_pattern.search(line):\n",
    "            in_import_block = True\n",
    "            continue\n",
    "        \n",
    "        if in_import_block:\n",
    "            if line.strip() == ')':\n",
    "                in_import_block = False\n",
    "                continue\n",
    "                \n",
    "            import_line_match = import_multi_line_pattern.search(line)\n",
    "            if import_line_match:\n",
    "                import_name = import_line_match.group(1)\n",
    "                imports.append((import_name, line_no))\n",
    "        \n",
    "        # Find function definitions\n",
    "        func_match = func_pattern.search(line)\n",
    "        if func_match:\n",
    "            func_name = func_match.group(1)\n",
    "            context = get_context_around_line(data, file_path, line_no)\n",
    "            symbols[func_name] = {\n",
    "                'type': 'function',\n",
    "                'line_no': line_no,\n",
    "                'context': context\n",
    "            }\n",
    "        \n",
    "        # Find struct definitions\n",
    "        struct_match = struct_pattern.search(line)\n",
    "        if struct_match:\n",
    "            struct_name = struct_match.group(1)\n",
    "            context = get_context_around_line(data, file_path, line_no)\n",
    "            symbols[struct_name] = {\n",
    "                'type': 'struct',\n",
    "                'line_no': line_no,\n",
    "                'context': context\n",
    "            }\n",
    "        \n",
    "        # Find interface definitions\n",
    "        interface_match = interface_pattern.search(line)\n",
    "        if interface_match:\n",
    "            interface_name = interface_match.group(1)\n",
    "            context = get_context_around_line(data, file_path, line_no)\n",
    "            symbols[interface_name] = {\n",
    "                'type': 'interface',\n",
    "                'line_no': line_no,\n",
    "                'context': context\n",
    "            }\n",
    "    \n",
    "    data['import_relations'][file_path] = imports\n",
    "    data['module_symbols'][file_path] = symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysize the file\n",
    "- Calls the respective language function based on the codebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_file(data, file_path, content, language):\n",
    "    \"\"\"Analyze a file for imports and symbols with line numbers and context.\"\"\"\n",
    "    if language == \"python\":\n",
    "        analyze_python_file(data, file_path, content)\n",
    "    elif language == \"cpp\":\n",
    "        analyze_cpp_file(data, file_path, content)\n",
    "    elif language == \"java\":\n",
    "        analyze_java_file(data, file_path, content)\n",
    "    elif language == \"go\":\n",
    "        analyze_go_file(data, file_path, content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Symbol reference analyzers\n",
    "- Identifies variable and function references across Python, Go, C++, and Java files\n",
    "- Uses AST parsing for Python and regex pattern matching for other languages\n",
    "- Tracks references with file location and surrounding context for better understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_references_in_python_file(data, file_path, content):\n",
    "    \"\"\"Find references to symbols in a Python file.\"\"\"\n",
    "    try:\n",
    "        tree = ast.parse(content)\n",
    "        \n",
    "        for node in ast.walk(tree):\n",
    "            # Find variable references\n",
    "            if isinstance(node, ast.Name) and isinstance(node.ctx, ast.Load):\n",
    "                symbol_name = node.id\n",
    "                line_no = node.lineno\n",
    "                \n",
    "                # Track reference with context\n",
    "                if symbol_name not in data['symbol_references']:\n",
    "                    data['symbol_references'][symbol_name] = []\n",
    "                \n",
    "                context = get_context_around_line(data, file_path, line_no)\n",
    "                data['symbol_references'][symbol_name].append((file_path, line_no, context))\n",
    "            \n",
    "            # Find attribute references (e.g., obj.method())\n",
    "            elif isinstance(node, ast.Attribute) and isinstance(node.ctx, ast.Load):\n",
    "                attr_name = node.attr\n",
    "                line_no = node.lineno\n",
    "                \n",
    "                if attr_name not in data['symbol_references']:\n",
    "                    data['symbol_references'][attr_name] = []\n",
    "                \n",
    "                context = get_context_around_line(data, file_path, line_no)\n",
    "                data['symbol_references'][attr_name].append((file_path, line_no, context))\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error finding references in Python file {file_path}: {e}\")\n",
    "\n",
    "def find_references_in_go_file(data, file_path, content):\n",
    "    \"\"\"Find references to symbols in a Go file with optimized performance.\"\"\"\n",
    "    # Get all symbol names from all files to check for references\n",
    "    all_symbols = set()\n",
    "    for symbols_dict in data['module_symbols'].values():\n",
    "        all_symbols.update(symbols_dict.keys())\n",
    "    \n",
    "    # Skip if no symbols to check or file is empty\n",
    "    if not all_symbols or not content:\n",
    "        return\n",
    "    \n",
    "    # Pre-compile all regex patterns\n",
    "    symbol_patterns = {}\n",
    "    for symbol_name in all_symbols:\n",
    "        # Only create patterns for symbols with reasonable length (avoid single-character symbols)\n",
    "        if len(symbol_name) > 2:\n",
    "            symbol_patterns[symbol_name] = re.compile(r'\\b' + re.escape(symbol_name) + r'\\b')\n",
    "    \n",
    "    # Process content line by line\n",
    "    lines = content.splitlines()\n",
    "    \n",
    "    # Skip definition lines for this file\n",
    "    definition_lines = {}\n",
    "    if file_path in data['module_symbols']:\n",
    "        for symbol, details in data['module_symbols'][file_path].items():\n",
    "            definition_lines[details['line_no']] = symbol\n",
    "    \n",
    "    for line_no, line in enumerate(lines, 1):\n",
    "        # Skip comment lines and import/package declarations\n",
    "        if (line.strip().startswith(\"//\") or \n",
    "            line.strip().startswith(\"/*\") or \n",
    "            line.strip().startswith(\"import \") or \n",
    "            line.strip().startswith(\"package \")):\n",
    "            continue\n",
    "        \n",
    "        # Skip if this line is a symbol definition\n",
    "        if line_no in definition_lines:\n",
    "            continue\n",
    "        \n",
    "        # Check for symbol references\n",
    "        for symbol_name, pattern in symbol_patterns.items():\n",
    "            if pattern.search(line):\n",
    "                # Skip if this is a definition line for this symbol\n",
    "                if (file_path in data['module_symbols'] and \n",
    "                    symbol_name in data['module_symbols'][file_path] and \n",
    "                    data['module_symbols'][file_path][symbol_name]['line_no'] == line_no):\n",
    "                    continue\n",
    "                \n",
    "                if symbol_name not in data['symbol_references']:\n",
    "                    data['symbol_references'][symbol_name] = []\n",
    "                \n",
    "                context = get_context_around_line(data, file_path, line_no)\n",
    "                data['symbol_references'][symbol_name].append((file_path, line_no, context))\n",
    "\n",
    "\n",
    "def find_references_in_cpp_file(data, file_path, content):\n",
    "    \"\"\"Find references to symbols in a C/C++ file with optimized performance.\"\"\"\n",
    "    # Get all symbol names from all files to check for references\n",
    "    all_symbols = set()\n",
    "    for symbols_dict in data['module_symbols'].values():\n",
    "        all_symbols.update(symbols_dict.keys())\n",
    "    \n",
    "    # Skip if no symbols to check or file is empty\n",
    "    if not all_symbols or not content:\n",
    "        return\n",
    "    \n",
    "    # Pre-compile all regex patterns for symbols with meaningful length\n",
    "    symbol_patterns = {}\n",
    "    for symbol_name in all_symbols:\n",
    "        # Skip very short symbols that would cause many false positives\n",
    "        if len(symbol_name) > 2:\n",
    "            symbol_patterns[symbol_name] = re.compile(r'\\b' + re.escape(symbol_name) + r'\\b')\n",
    "    \n",
    "    # Process content line by line\n",
    "    lines = content.splitlines()\n",
    "    \n",
    "    # Skip definition lines for this file\n",
    "    definition_lines = {}\n",
    "    if file_path in data['module_symbols']:\n",
    "        for symbol, details in data['module_symbols'][file_path].items():\n",
    "            definition_lines[details['line_no']] = symbol\n",
    "    \n",
    "    for line_no, line in enumerate(lines, 1):\n",
    "        # Skip comment lines and preprocessor directives\n",
    "        if (line.strip().startswith(\"//\") or \n",
    "            line.strip().startswith(\"/*\") or \n",
    "            line.strip().startswith(\"#\")):\n",
    "            continue\n",
    "        \n",
    "        # Skip if this line is a symbol definition\n",
    "        if line_no in definition_lines:\n",
    "            continue\n",
    "        \n",
    "        # Check for symbol references\n",
    "        for symbol_name, pattern in symbol_patterns.items():\n",
    "            if pattern.search(line):\n",
    "                # Skip if this is a definition line for this symbol\n",
    "                if (file_path in data['module_symbols'] and \n",
    "                    symbol_name in data['module_symbols'][file_path] and \n",
    "                    data['module_symbols'][file_path][symbol_name]['line_no'] == line_no):\n",
    "                    continue\n",
    "                \n",
    "                if symbol_name not in data['symbol_references']:\n",
    "                    data['symbol_references'][symbol_name] = []\n",
    "                \n",
    "                context = get_context_around_line(data, file_path, line_no)\n",
    "                data['symbol_references'][symbol_name].append((file_path, line_no, context))\n",
    "\n",
    "\n",
    "def find_references_in_java_file(data, file_path, content):\n",
    "    \"\"\"Find references to symbols in a Java file with optimized performance.\"\"\"\n",
    "    # Get all symbol names from all files to check for references\n",
    "    all_symbols = set()\n",
    "    for symbols_dict in data['module_symbols'].values():\n",
    "        all_symbols.update(symbols_dict.keys())\n",
    "    \n",
    "    # Skip if no symbols to check or file is empty\n",
    "    if not all_symbols or not content:\n",
    "        return\n",
    "    \n",
    "    # Pre-compile all regex patterns for symbols with meaningful length\n",
    "    symbol_patterns = {}\n",
    "    for symbol_name in all_symbols:\n",
    "        # Skip very short symbols that would cause many false positives\n",
    "        if len(symbol_name) > 2:\n",
    "            symbol_patterns[symbol_name] = re.compile(r'\\b' + re.escape(symbol_name) + r'\\b')\n",
    "    \n",
    "    # Process content line by line\n",
    "    lines = content.splitlines()\n",
    "    \n",
    "    # Skip definition lines for this file\n",
    "    definition_lines = {}\n",
    "    if file_path in data['module_symbols']:\n",
    "        for symbol, details in data['module_symbols'][file_path].items():\n",
    "            definition_lines[details['line_no']] = symbol\n",
    "    \n",
    "    for line_no, line in enumerate(lines, 1):\n",
    "        # Skip comment lines, imports, and package declarations\n",
    "        if (line.strip().startswith(\"//\") or \n",
    "            line.strip().startswith(\"/*\") or \n",
    "            line.strip().startswith(\"import \") or \n",
    "            line.strip().startswith(\"package \")):\n",
    "            continue\n",
    "        \n",
    "        # Skip if this line is a symbol definition\n",
    "        if line_no in definition_lines:\n",
    "            continue\n",
    "        \n",
    "        # Check for symbol references\n",
    "        for symbol_name, pattern in symbol_patterns.items():\n",
    "            if pattern.search(line):\n",
    "                # Skip if this is a definition line for this symbol\n",
    "                if (file_path in data['module_symbols'] and \n",
    "                    symbol_name in data['module_symbols'][file_path] and \n",
    "                    data['module_symbols'][file_path][symbol_name]['line_no'] == line_no):\n",
    "                    continue\n",
    "                \n",
    "                if symbol_name not in data['symbol_references']:\n",
    "                    data['symbol_references'][symbol_name] = []\n",
    "                \n",
    "                context = get_context_around_line(data, file_path, line_no)\n",
    "                data['symbol_references'][symbol_name].append((file_path, line_no, context))\n",
    "\n",
    "def find_references_in_file(data, file_path, content, language):\n",
    "    \"\"\"Find references to symbols in a file based on its language.\"\"\"\n",
    "    if language == \"python\":\n",
    "        find_references_in_python_file(data, file_path, content)\n",
    "    elif language == \"cpp\":\n",
    "        find_references_in_cpp_file(data, file_path, content)\n",
    "    elif language == \"java\":\n",
    "        find_references_in_java_file(data, file_path, content)\n",
    "    elif language == \"go\":\n",
    "        find_references_in_go_file(data, file_path, content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph building and indexing\n",
    "- Constructs a comprehensive symbol index of all definitions and references\n",
    "- Parses all files in the codebase to build directory and file relationships\n",
    "- Creates a graph structure with nodes for files/directories and edges for relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_symbol_index(data):\n",
    "    \"\"\"Build a comprehensive index of all symbols and where they're defined/used.\"\"\"\n",
    "    # Initialize the symbol index\n",
    "    data['symbol_index'] = {}\n",
    "    \n",
    "    # First, add all symbol definitions\n",
    "    for file_path, symbols in data['module_symbols'].items():\n",
    "        for symbol_name, details in symbols.items():\n",
    "            if symbol_name not in data['symbol_index']:\n",
    "                data['symbol_index'][symbol_name] = []\n",
    "            \n",
    "            data['symbol_index'][symbol_name].append({\n",
    "                'file': file_path,\n",
    "                'type': 'definition',\n",
    "                'symbol_type': details['type'],\n",
    "                'line_no': details['line_no'],\n",
    "                'context': details.get('context', ''),\n",
    "                'docstring': details.get('docstring', '')\n",
    "            })\n",
    "    \n",
    "    # Then, add all references\n",
    "    for symbol_name, references in data['symbol_references'].items():\n",
    "        if symbol_name not in data['symbol_index']:\n",
    "            data['symbol_index'][symbol_name] = []\n",
    "        \n",
    "        for file_path, line_no, context in references:\n",
    "            # Avoid duplicating references if they're already in definitions\n",
    "            if not any(ref['file'] == file_path and ref['line_no'] == line_no and ref['type'] == 'definition' \n",
    "                      for ref in data['symbol_index'].get(symbol_name, [])):\n",
    "                data['symbol_index'][symbol_name].append({\n",
    "                    'file': file_path,\n",
    "                    'type': 'reference',\n",
    "                    'line_no': line_no,\n",
    "                    'context': context\n",
    "                })\n",
    "\n",
    "def parse_files(data):\n",
    "    \"\"\"Parse all files in the directory and build relationships.\"\"\"\n",
    "    # First pass: Index all files and create directory nodes\n",
    "    for root, dirs, files in os.walk(data['root_dir']):\n",
    "        # Add directory node\n",
    "        rel_dir = os.path.relpath(root, data['root_dir'])\n",
    "        if rel_dir != '.':\n",
    "            data['directories'].add(rel_dir)\n",
    "            data['graph'].add_node(rel_dir, type='directory')\n",
    "            \n",
    "            # Add edge from parent directory to this directory (if not root)\n",
    "            parent_dir = os.path.dirname(rel_dir)\n",
    "            if parent_dir and parent_dir != '.':\n",
    "                data['graph'].add_edge(parent_dir, rel_dir, edge_type='contains_directory')\n",
    "\n",
    "        # Index files of supported languages\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            rel_path = os.path.relpath(file_path, data['root_dir'])\n",
    "            file_language = detect_language(data, file_path)\n",
    "            \n",
    "            if file_language in data['supported_languages']:\n",
    "                data['file_index'][rel_path] = get_next_index(data)\n",
    "                \n",
    "                # Add node for this file\n",
    "                data['graph'].add_node(rel_path, type='file', file_index=data['file_index'][rel_path], language=file_language)\n",
    "                \n",
    "                # Connect file to its directory\n",
    "                if rel_dir != '.':\n",
    "                    data['graph'].add_edge(rel_dir, rel_path, edge_type='contains_file')\n",
    "                \n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                        content = f.read()\n",
    "                        data['file_contents'][rel_path] = content\n",
    "                        analyze_file(data, rel_path, content, file_language)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error parsing {file_path}: {e}\")\n",
    "    \n",
    "    # Second pass: Find symbol references across files\n",
    "    for file_path, content in data['file_contents'].items():\n",
    "        file_language = detect_language(data, file_path)\n",
    "        find_references_in_file(data, file_path, content, file_language)\n",
    "    \n",
    "    # Build the symbol index after all analyses\n",
    "    build_symbol_index(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph construction\n",
    "- Builds NetworkX graph with enhanced node and edge information\n",
    "- Creates nodes for directories, files, code snippets, and symbols\n",
    "- Establishes relationships between files (imports, references) with line numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(data):\n",
    "    \"\"\"Build the NetworkX graph with enhanced node and edge information.\"\"\"\n",
    "    # We've already added basic file and directory nodes during parsing\n",
    "    # Now add more detailed connections and data\n",
    "    \n",
    "    # Add nodes for all directories (if not already added)\n",
    "    for directory in data['directories']:\n",
    "        if not data['graph'].has_node(directory):\n",
    "            data['graph'].add_node(directory, type='directory')\n",
    "        \n",
    "        # Ensure parent directories exist and are connected\n",
    "        parts = directory.split(os.sep)\n",
    "        for i in range(1, len(parts)):\n",
    "            parent_path = os.sep.join(parts[:i])\n",
    "            if parent_path and not data['graph'].has_node(parent_path):\n",
    "                data['graph'].add_node(parent_path, type='directory')\n",
    "                data['directories'].add(parent_path)\n",
    "            \n",
    "            # Connect parent to child directory\n",
    "            if parent_path:\n",
    "                child_path = os.sep.join(parts[:i+1])\n",
    "                data['graph'].add_edge(parent_path, child_path, edge_type='contains_directory')\n",
    "    \n",
    "    # Add nodes for all files with indices and code snippet nodes\n",
    "    for file_path, file_idx in data['file_index'].items():\n",
    "        language = detect_language(data, file_path)\n",
    "        \n",
    "        # Update file node if it exists, create it otherwise\n",
    "        if data['graph'].has_node(file_path):\n",
    "            data['graph'].nodes[file_path].update({\n",
    "                'file_index': file_idx,\n",
    "                'directory': os.path.dirname(file_path),\n",
    "                'language': language\n",
    "            })\n",
    "        else:\n",
    "            data['graph'].add_node(file_path, \n",
    "                               type='file',\n",
    "                               file_index=file_idx,\n",
    "                               directory=os.path.dirname(file_path),\n",
    "                               language=language)\n",
    "        \n",
    "        # Connect file to its directory\n",
    "        directory = os.path.dirname(file_path)\n",
    "        if directory:\n",
    "            # Make sure the directory node exists\n",
    "            if not data['graph'].has_node(directory):\n",
    "                data['graph'].add_node(directory, type='directory')\n",
    "                data['directories'].add(directory)\n",
    "            \n",
    "            # Add edge from directory to file if it doesn't exist\n",
    "            if not data['graph'].has_edge(directory, file_path):\n",
    "                data['graph'].add_edge(directory, file_path, edge_type='contains_file')\n",
    "        \n",
    "        # Create snippet nodes for the entire file\n",
    "        if file_path in data['file_contents']:\n",
    "            chunks = chunk_code(data['file_contents'][file_path])\n",
    "            for idx, chunk_info in enumerate(chunks):\n",
    "                snippet_node = f\"{file_path}::snippet::{idx}\"\n",
    "                data['graph'].add_node(snippet_node,\n",
    "                                   type='snippet',\n",
    "                                   code_snippet=chunk_info['code_snippet'],\n",
    "                                   start_line=chunk_info['start_line'],\n",
    "                                   end_line=chunk_info['end_line'],\n",
    "                                   language=language)\n",
    "                # Connect file node to snippet node\n",
    "                data['graph'].add_edge(file_path, snippet_node, \n",
    "                                   edge_type='contains_snippet',\n",
    "                                   start_line=chunk_info['start_line'],\n",
    "                                   end_line=chunk_info['end_line'])\n",
    "\n",
    "        # Add nodes for symbols in this file\n",
    "        for symbol, details in data['module_symbols'].get(file_path, {}).items():\n",
    "            symbol_node = f\"{file_path}::{symbol}\"\n",
    "            data['graph'].add_node(symbol_node, \n",
    "                               type='symbol',\n",
    "                               symbol_type=details['type'],\n",
    "                               line_number=details['line_no'],\n",
    "                               context=details.get('context', ''),\n",
    "                               docstring=details.get('docstring', ''))\n",
    "            data['graph'].add_edge(file_path, symbol_node, \n",
    "                               edge_type='defines',\n",
    "                               line_number=details['line_no'])\n",
    "\n",
    "    # Add edges for imports with line numbers\n",
    "    for file_path, imports in data['import_relations'].items():\n",
    "        for imp, line_no in imports:\n",
    "            # Look for matching files or symbols\n",
    "            for target_file, symbols in data['module_symbols'].items():\n",
    "                if imp in symbols:\n",
    "                    data['graph'].add_edge(file_path, \n",
    "                                       f\"{target_file}::{imp}\",\n",
    "                                       edge_type='import',\n",
    "                                       line_number=line_no)\n",
    "                # For Python, handle module imports\n",
    "                elif detect_language(data, file_path) == \"python\" and target_file.replace('.py', '').endswith(imp):\n",
    "                    data['graph'].add_edge(file_path, \n",
    "                                       target_file,\n",
    "                                       edge_type='import',\n",
    "                                       line_number=line_no)\n",
    "                # For Java, handle package imports\n",
    "                elif detect_language(data, file_path) == \"java\" and imp.startswith(os.path.splitext(os.path.basename(target_file))[0]):\n",
    "                    data['graph'].add_edge(file_path, \n",
    "                                       target_file,\n",
    "                                       edge_type='import',\n",
    "                                       line_number=line_no)\n",
    "    \n",
    "    # Add edges for symbol references\n",
    "    for symbol, references in data['symbol_references'].items():\n",
    "        for file_path, line_no, context in references:\n",
    "            # Find symbol nodes that match this reference\n",
    "            for target_file, symbols in data['module_symbols'].items():\n",
    "                if symbol in symbols:\n",
    "                    # Create reference edge\n",
    "                    data['graph'].add_edge(file_path, \n",
    "                                       f\"{target_file}::{symbol}\",\n",
    "                                       edge_type='references',\n",
    "                                       line_number=line_no,\n",
    "                                       context=context)\n",
    "    \n",
    "    return data['graph']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph validation and statistics\n",
    "- Validates graph for consistency between data structures and graph nodes/edges\n",
    "- Generates comprehensive statistics on files, directories, symbols, and connections\n",
    "- Identifies and reports issues like missing nodes or incomplete relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_graph_and_data(data, G):\n",
    "    \"\"\"Validate the graph and data structures for consistency and coverage.\"\"\"\n",
    "    report = {\n",
    "        'stats': {\n",
    "            'files': len(data['file_index']),\n",
    "            'directories': len(data['directories']),\n",
    "            'symbols': len(data['symbol_index']),\n",
    "            'nodes': len(G.nodes()),\n",
    "            'edges': len(G.edges())\n",
    "        },\n",
    "        'issues': []\n",
    "    }\n",
    "    \n",
    "    # Check that all files in file_index have corresponding nodes\n",
    "    for file_path in data['file_index']:\n",
    "        if not G.has_node(file_path):\n",
    "            report['issues'].append(f\"File {file_path} in index but missing from graph\")\n",
    "    \n",
    "    # Check that all directories have nodes\n",
    "    for directory in data['directories']:\n",
    "        if not G.has_node(directory):\n",
    "            report['issues'].append(f\"Directory {directory} in data but missing from graph\")\n",
    "    \n",
    "    # Check symbol connections\n",
    "    for symbol, entries in data['symbol_index'].items():\n",
    "        definition_files = [entry['file'] for entry in entries if entry['type'] == 'definition']\n",
    "        for def_file in definition_files:\n",
    "            symbol_node = f\"{def_file}::{symbol}\"\n",
    "            if not G.has_node(symbol_node):\n",
    "                report['issues'].append(f\"Symbol {symbol} defined in {def_file} but node missing from graph\")\n",
    "    \n",
    "    # Count symbols by type\n",
    "    symbol_types = {}\n",
    "    for entries in data['symbol_index'].values():\n",
    "        for entry in entries:\n",
    "            if entry['type'] == 'definition' and 'symbol_type' in entry:\n",
    "                symbol_type = entry['symbol_type']\n",
    "                if symbol_type not in symbol_types:\n",
    "                    symbol_types[symbol_type] = 0\n",
    "                symbol_types[symbol_type] += 1\n",
    "    \n",
    "    report['stats']['symbol_types'] = symbol_types\n",
    "    \n",
    "    # Count edge types\n",
    "    edge_types = {}\n",
    "    for _, _, attrs in G.edges(data=True):\n",
    "        edge_type = attrs.get('edge_type', 'unknown')\n",
    "        if edge_type not in edge_types:\n",
    "            edge_types[edge_type] = 0\n",
    "        edge_types[edge_type] += 1\n",
    "    \n",
    "    report['stats']['edge_types'] = edge_types\n",
    "    \n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main function and program execution\n",
    "- Initializes data structures and processes codebase starting from root directory\n",
    "- Parses files, builds graph representation, and validates resulting structures\n",
    "- Outputs statistics about parsed files, graph nodes/edges, and validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_graph_builder(root_directory):\n",
    "    # Initialize all data structures\n",
    "    data = initialize_data_structures()\n",
    "    data['root_dir'] = root_directory\n",
    "    \n",
    "    print(f\"Starting to parse files in {root_directory}...\")\n",
    "    \n",
    "    # Parse all the files in the directory\n",
    "    parse_files(data)\n",
    "    print(f\"Parsed {len(data['file_index'])} files\")\n",
    "    \n",
    "    # Build the graph representation\n",
    "    G = build_graph(data)\n",
    "    print(f\"Graph has {len(G.nodes())} nodes and {len(G.edges())} edges\")\n",
    "    \n",
    "    # Validate the graph and data\n",
    "    report = validate_graph_and_data(data, G)\n",
    "    print(json.dumps(report, indent=2))\n",
    "    \n",
    "    return data, G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"flask\"\n",
    "\n",
    "data, G = main_graph_builder(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Database\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "- Rename the .env.temp with your creditials to write into the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "ARANGO_HOST = os.getenv(\"ARANGO_HOST\")\n",
    "ARANGO_USERNAME = os.getenv(\"ARANGO_USERNAME\")\n",
    "ARANGO_PASSWORD = os.getenv(\"ARANGO_PASSWORD\")\n",
    "ARANGO_VERIFY = os.getenv(\"ARANGO_VERIFY\") == \"True\"\n",
    "GRAPH_NAME = os.getenv(\"GRAPH_NAME\")\n",
    "WRITE_BATCH_SIZE = int(os.getenv(\"WRITE_BATCH_SIZE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to ArangoDB using the credentials from the environment variables\n",
    "client = ArangoClient(hosts=ARANGO_HOST)\n",
    "db = client.db(username=ARANGO_USERNAME, password=ARANGO_PASSWORD, verify=ARANGO_VERIFY)\n",
    "print(\"Database connection:\", db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the nxadb Graph object with initial graph data\n",
    "G_adb = nxadb.Graph(\n",
    "    name=GRAPH_NAME,\n",
    "    db=db,\n",
    "    incoming_graph_data=G,\n",
    "    write_batch_size=WRITE_BATCH_SIZE,\n",
    "    overwrite_graph=True\n",
    ")\n",
    "\n",
    "print(\"Graph object with data:\", G_adb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run from loaded database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database connection: <StandardDatabase _system>\n"
     ]
    }
   ],
   "source": [
    "# Connect to ArangoDB using the credentials from the environment variables\n",
    "client = ArangoClient(hosts=ARANGO_HOST)\n",
    "db = client.db(username=ARANGO_USERNAME, password=ARANGO_PASSWORD, verify=ARANGO_VERIFY)\n",
    "print(\"Database connection:\", db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10:46:50 +0530] [INFO]: Graph 'FlaskRepv1' exists.\n",
      "[10:46:51 +0530] [INFO]: Default node type set to 'FlaskRepv1_node'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph object type: <class 'nx_arangodb.classes.graph.Graph'>\n"
     ]
    }
   ],
   "source": [
    "G_adb_loaded = nxadb.Graph(\n",
    "    name=GRAPH_NAME,\n",
    "    db=db,\n",
    ")\n",
    "print(\"Graph object type:\", type(G_adb_loaded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import traceback\n",
    "from typing import Dict, Optional, List, Any\n",
    "from arango import ArangoClient\n",
    "from mistralai.client import MistralClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = None\n",
    "client = None\n",
    "mistral_client = None\n",
    "model = None\n",
    "graph_name = None\n",
    "node_collection = None\n",
    "edge_collection = None\n",
    "files = {}\n",
    "snippets = {}\n",
    "symbols = {}\n",
    "db_schema = {}\n",
    "node_types = {}\n",
    "symbol_name_index = {}\n",
    "file_to_snippets = {}\n",
    "file_to_symbols = {}\n",
    "snippet_to_symbols = {}\n",
    "conversation_history = []\n",
    "type_field = None\n",
    "path_field = None\n",
    "edge_type_field = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discover_graph_structure():\n",
    "    \"\"\"Dynamically discover the graph structure in ArangoDB with improved directory detection\"\"\"\n",
    "    global node_collection, edge_collection\n",
    "    \n",
    "    try:\n",
    "        # Get graph object\n",
    "        graph = db.graph(graph_name)\n",
    "        graph_info = graph.properties()\n",
    "        \n",
    "        # Get the edge collection name from graph properties\n",
    "        edge_definitions = graph_info.get('edgeDefinitions', [])\n",
    "        \n",
    "        # If no edge definitions exist, set defaults and retry\n",
    "        if not edge_definitions:\n",
    "            print(f\"No edge definitions found, using default naming pattern\")\n",
    "            node_collection = f\"{graph_name}_node\"\n",
    "            edge_collection = f\"{graph_name}_node_to_{graph_name}_node\"\n",
    "            print(f\"Using default collections: Nodes={node_collection}, Edges={edge_collection}\")\n",
    "            # Validate the schema to understand the field names\n",
    "            validate_schema()\n",
    "            return\n",
    "        \n",
    "        # Get the edge collection\n",
    "        edge_def = edge_definitions[0]\n",
    "        edge_collection = edge_def.get('collection')\n",
    "        \n",
    "        # Get node collection\n",
    "        from_collections = edge_def.get('from', [])\n",
    "        if not from_collections:\n",
    "            # No 'from' collections, use defaults\n",
    "            node_collection = f\"{graph_name}_nodes\"\n",
    "            print(f\"No 'from' collections found, using default node collection: {node_collection}\")\n",
    "        else:\n",
    "            node_collection = from_collections[0]\n",
    "        \n",
    "        print(f\"Using collections: Nodes={node_collection}, Edges={edge_collection}\")\n",
    "        \n",
    "        # Validate the schema to understand the field names\n",
    "        validate_schema()\n",
    "    except Exception as e:\n",
    "        print(f\"Error discovering graph structure: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_schema():\n",
    "    \"\"\"Validate the schema and identify the key field names used in this database\"\"\"\n",
    "    global type_field, path_field, edge_type_field\n",
    "    \n",
    "    try:\n",
    "        # Sample nodes to understand the schema\n",
    "        aql = f\"\"\"\n",
    "        FOR v IN {node_collection}\n",
    "        LIMIT 10\n",
    "        RETURN v\n",
    "        \"\"\"\n",
    "        cursor = db.aql.execute(aql)\n",
    "        sample_nodes = [doc for doc in cursor]\n",
    "        \n",
    "        if not sample_nodes:\n",
    "            raise ValueError(f\"No nodes found in collection {node_collection}\")\n",
    "        \n",
    "        # Identify the type field\n",
    "        type_field_candidates = ['type', 'ast_type', 'node_type']\n",
    "        type_field = None\n",
    "        \n",
    "        for field in type_field_candidates:\n",
    "            for node in sample_nodes:\n",
    "                if field in node:\n",
    "                    type_field = field\n",
    "                    print(f\"Found type field: {field}\")\n",
    "                    break\n",
    "            if type_field:\n",
    "                break\n",
    "                \n",
    "        if not type_field:\n",
    "            print(\"Warning: Could not identify a type field in nodes\")\n",
    "        \n",
    "        # Identify path field\n",
    "        path_field_candidates = ['path', 'file_path', 'rel_path']\n",
    "        path_field = None\n",
    "        \n",
    "        for field in path_field_candidates:\n",
    "            for node in sample_nodes:\n",
    "                if field in node:\n",
    "                    path_field = field\n",
    "                    print(f\"Found path field: {field}\")\n",
    "                    break\n",
    "            if path_field:\n",
    "                break\n",
    "        \n",
    "        # Sample edges to understand relationship types\n",
    "        aql = f\"\"\"\n",
    "        FOR e IN {edge_collection}\n",
    "        LIMIT 10\n",
    "        RETURN e\n",
    "        \"\"\"\n",
    "        cursor = db.aql.execute(aql)\n",
    "        sample_edges = [doc for doc in cursor]\n",
    "        \n",
    "        # Identify edge type field\n",
    "        edge_type_field_candidates = ['edge_type', 'relation', 'relationship', 'type']\n",
    "        edge_type_field = None\n",
    "        \n",
    "        for field in edge_type_field_candidates:\n",
    "            for edge in sample_edges:\n",
    "                if field in edge:\n",
    "                    edge_type_field = field\n",
    "                    print(f\"Found edge type field: {field}\")\n",
    "                    break\n",
    "            if edge_type_field:\n",
    "                break\n",
    "        \n",
    "        print(f\"Schema validation complete: type_field={type_field}, path_field={path_field}, edge_type_field={edge_type_field}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error validating schema: {str(e)}\")\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_node_types():\n",
    "    \"\"\"Validate that all necessary node types are accessible in the graph\"\"\"\n",
    "    try:\n",
    "        # Check for directory nodes specifically\n",
    "        aql = f\"\"\"\n",
    "        FOR v IN {node_collection}\n",
    "            FILTER v.type == 'directory'\n",
    "            LIMIT 1\n",
    "            RETURN v\n",
    "        \"\"\"\n",
    "        cursor = db.aql.execute(aql)\n",
    "        directories = [doc for doc in cursor]\n",
    "        \n",
    "        if not directories:\n",
    "            print(\"Warning: No directory nodes found in the collection.\")\n",
    "            # Try alternative fields\n",
    "            alternative_fields = ['ast_type', 'node_type']\n",
    "            for field in alternative_fields:\n",
    "                aql = f\"\"\"\n",
    "                FOR v IN {node_collection}\n",
    "                    FILTER v.{field} == 'directory' OR v.{field} == 'Directory'\n",
    "                    LIMIT 1\n",
    "                    RETURN v\n",
    "                \"\"\"\n",
    "                cursor = db.aql.execute(aql)\n",
    "                alternative_dirs = [doc for doc in cursor]\n",
    "                if alternative_dirs:\n",
    "                    print(f\"Found directory nodes using alternate field: {field}\")\n",
    "                    break\n",
    "        else:\n",
    "            print(f\"Found directory nodes successfully\")\n",
    "            \n",
    "        # Also check for edges that connect directories\n",
    "        aql = f\"\"\"\n",
    "        FOR e IN {edge_collection}\n",
    "            FILTER e.edge_type == 'contains_directory'\n",
    "            LIMIT 1\n",
    "            RETURN e\n",
    "        \"\"\"\n",
    "        cursor = db.aql.execute(aql)\n",
    "        dir_edges = [doc for doc in cursor]\n",
    "        \n",
    "        if not dir_edges:\n",
    "            print(\"Warning: No 'contains_directory' edges found in the edge collection.\")\n",
    "            # Try alternative edge types\n",
    "            alt_edge_types = ['contains', 'has_directory', 'parent']\n",
    "            for edge_type in alt_edge_types:\n",
    "                aql = f\"\"\"\n",
    "                FOR e IN {edge_collection}\n",
    "                    FILTER e.edge_type == '{edge_type}' OR e.relation == '{edge_type}' OR e.relationship == '{edge_type}'\n",
    "                    FOR v1 IN {node_collection}\n",
    "                        FILTER v1._id == e._from\n",
    "                        FOR v2 IN {node_collection}\n",
    "                            FILTER v2._id == e._to\n",
    "                            FILTER (v1.type == 'directory' OR v2.type == 'directory')\n",
    "                            LIMIT 1\n",
    "                            RETURN e\n",
    "                \"\"\"\n",
    "                cursor = db.aql.execute(aql)\n",
    "                alt_dir_edges = [doc for doc in cursor]\n",
    "                if alt_dir_edges:\n",
    "                    print(f\"Found directory edges using alternate edge type: {edge_type}\")\n",
    "                    break\n",
    "        else:\n",
    "            print(f\"Found directory edge relationships successfully\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error validating node types: {str(e)}\")\n",
    "        # Not raising the exception here to allow the process to continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_db_schema() -> Dict:\n",
    "    \"\"\"Get detailed schema information with better type understanding\"\"\"\n",
    "    try:\n",
    "        # Basic schema information\n",
    "        collections = db.collections()\n",
    "        collection_names = [c['name'] for c in collections if not c['name'].startswith('_')]\n",
    "        \n",
    "        # Get graphs\n",
    "        graphs = db.graphs()\n",
    "        graph_names = [g['name'] for g in graphs]\n",
    "        graph_details = []\n",
    "        \n",
    "        for graph_name in graph_names:\n",
    "            graph = db.graph(graph_name)\n",
    "            graph_info = graph.properties()\n",
    "            \n",
    "            # Get edge definitions for better understanding of relationships\n",
    "            edge_definitions = graph_info.get('edgeDefinitions', [])\n",
    "            enhanced_edge_defs = []\n",
    "            \n",
    "            for edge_def in edge_definitions:\n",
    "                collection = edge_def.get('collection', '')\n",
    "                from_collections = edge_def.get('from', [])\n",
    "                to_collections = edge_def.get('to', [])\n",
    "                \n",
    "                # Sample some edges to understand relationship types\n",
    "                edge_samples = []\n",
    "                if collection:\n",
    "                    try:\n",
    "                        cursor = db.aql.execute(\n",
    "                            f\"FOR e IN {collection} LIMIT 5 RETURN e\"\n",
    "                        )\n",
    "                        edge_samples = [edge for edge in cursor]\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error sampling edges from {collection}: {str(e)}\")\n",
    "                \n",
    "                # Extract edge types if they exist\n",
    "                edge_types = set()\n",
    "                for edge in edge_samples:\n",
    "                    if 'edge_type' in edge:\n",
    "                        edge_types.add(edge['edge_type'])\n",
    "                \n",
    "                enhanced_edge_defs.append({\n",
    "                    'collection': collection,\n",
    "                    'from_collections': from_collections,\n",
    "                    'to_collections': to_collections,\n",
    "                    'edge_types': list(edge_types),\n",
    "                    'sample_count': len(edge_samples),\n",
    "                })\n",
    "            \n",
    "            graph_details.append({\n",
    "                'name': graph_info.get('name'),\n",
    "                'edge_definitions': enhanced_edge_defs,\n",
    "                'orphan_collections': graph_info.get('orphanCollections', [])\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            \"Graph Schema\": graph_details,\n",
    "            \"Collection Schema\": [c for c in collection_names],\n",
    "            \"Node Types\": {},  # Will be filled by analyze_node_types\n",
    "            \"Type Relationships\": []  # Will be filled later\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting enhanced schema: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_node_types():\n",
    "    \"\"\"Analyze and cache the node types in the database using the detected schema fields\"\"\"\n",
    "    node_types_dict = {}\n",
    "    try:\n",
    "        # Use the detected type field\n",
    "        if not type_field:\n",
    "            print(\"No type field detected, trying to infer node types from other properties\")\n",
    "            # Fallback logic to infer types\n",
    "            return infer_node_types()\n",
    "        \n",
    "        # Query distinct node types\n",
    "        aql = f\"\"\"\n",
    "        FOR v IN {node_collection}\n",
    "            FILTER HAS(v, \"{type_field}\")\n",
    "            COLLECT type = v.{type_field} WITH COUNT INTO count\n",
    "            RETURN {{\n",
    "                \"type\": type,\n",
    "                \"count\": count\n",
    "            }}\n",
    "        \"\"\"\n",
    "        cursor = db.aql.execute(aql)\n",
    "        type_counts = [doc for doc in cursor]\n",
    "        \n",
    "        # For each node type, get a sample and analyze structure\n",
    "        for type_info in type_counts:\n",
    "            node_type = type_info.get('type')\n",
    "            count = type_info.get('count', 0)\n",
    "            \n",
    "            if not node_type:\n",
    "                continue\n",
    "            \n",
    "            # Get a sample for this node type\n",
    "            aql = f\"\"\"\n",
    "            FOR v IN {node_collection}\n",
    "                FILTER v.{type_field} == '{node_type}'\n",
    "                LIMIT 1\n",
    "                RETURN v\n",
    "            \"\"\"\n",
    "            cursor = db.aql.execute(aql)\n",
    "            samples = [doc for doc in cursor]\n",
    "            \n",
    "            if not samples:\n",
    "                continue\n",
    "            \n",
    "            sample = samples[0]\n",
    "            \n",
    "            # Normalize the node type name\n",
    "            normalized_type = node_type\n",
    "            \n",
    "            # Add to node types dictionary\n",
    "            node_types_dict[normalized_type] = {\n",
    "                'count': count,\n",
    "                'field': type_field,\n",
    "                'sample_structure': list(sample.keys()),\n",
    "                'sample': sample\n",
    "            }\n",
    "            \n",
    "            print(f\"Type: {node_type}, Count: {count}\")\n",
    "        \n",
    "        # Update the db_schema with node types\n",
    "        global db_schema\n",
    "        db_schema[\"Node Types\"] = node_types_dict\n",
    "        \n",
    "        # Special handling for directories and files if not found\n",
    "        for important_type in ['directory', 'file']:\n",
    "            if important_type not in node_types_dict:\n",
    "                detect_special_type(important_type, node_types_dict)\n",
    "                \n",
    "        return node_types_dict\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing node types: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_node_types():\n",
    "    \"\"\"Fallback method to infer node types when type field is not detected\"\"\"\n",
    "    # This is a placeholder for the function to be implemented later\n",
    "    print(\"infer_node_types function needs to be implemented\")\n",
    "    return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_special_type(type_name, node_types_dict):\n",
    "    \"\"\"Try to detect special types like directories and files if they weren't found by regular means\"\"\"\n",
    "    try:\n",
    "        # Different detection strategies based on type\n",
    "        if type_name == 'directory':\n",
    "            # Look for nodes with directory-like properties\n",
    "            indicators = ['path', 'directory', 'dir_name', 'folder']\n",
    "            filter_conditions = []\n",
    "            \n",
    "            for indicator in indicators:\n",
    "                filter_conditions.append(f'HAS(v, \"{indicator}\")')\n",
    "                \n",
    "            if path_field:\n",
    "                # Add condition that path doesn't end with file extension\n",
    "                filter_conditions.append(f'NOT REGEX_TEST(v.{path_field}, \"\\\\.[a-zA-Z0-9]+$\")')\n",
    "            \n",
    "            filter_str = \" OR \".join(filter_conditions)\n",
    "            \n",
    "            aql = f\"\"\"\n",
    "            FOR v IN {node_collection}\n",
    "                FILTER {filter_str}\n",
    "                LIMIT 100\n",
    "                RETURN v\n",
    "            \"\"\"\n",
    "            \n",
    "        elif type_name == 'file':\n",
    "            # Look for nodes with file-like properties\n",
    "            indicators = ['file', 'file_name', 'filename']\n",
    "            filter_conditions = []\n",
    "            \n",
    "            for indicator in indicators:\n",
    "                filter_conditions.append(f'HAS(v, \"{indicator}\")')\n",
    "                \n",
    "            if path_field:\n",
    "                # Add condition that path ends with file extension\n",
    "                filter_conditions.append(f'REGEX_TEST(v.{path_field}, \"\\\\.[a-zA-Z0-9]+$\")')\n",
    "            \n",
    "            filter_str = \" OR \".join(filter_conditions)\n",
    "            \n",
    "            aql = f\"\"\"\n",
    "            FOR v IN {node_collection}\n",
    "                FILTER {filter_str}\n",
    "                LIMIT 100\n",
    "                RETURN v\n",
    "            \"\"\"\n",
    "        \n",
    "        cursor = db.aql.execute(aql)\n",
    "        detected_nodes = [doc for doc in cursor]\n",
    "        \n",
    "        if detected_nodes:\n",
    "            print(f\"Detected {len(detected_nodes)} potential {type_name} nodes\")\n",
    "            \n",
    "            # Use the first node as a sample\n",
    "            sample = detected_nodes[0]\n",
    "            \n",
    "            node_types_dict[type_name] = {\n",
    "                'count': len(detected_nodes),\n",
    "                'field': 'inferred',\n",
    "                'sample_structure': list(sample.keys()),\n",
    "                'sample': sample\n",
    "            }\n",
    "            \n",
    "            print(f\"Added inferred {type_name} type to node types\")\n",
    "        else:\n",
    "            print(f\"Could not detect any {type_name} nodes\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error detecting {type_name} nodes: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_type_relationships(node_types_dict):\n",
    "    \"\"\"Analyze relationships between different node types\"\"\"\n",
    "    type_relationships = []\n",
    "    try:\n",
    "        node_type_keys = list(node_types_dict.keys())\n",
    "        \n",
    "        # For each node type pair, check if there are edges between them\n",
    "        for from_type in node_type_keys:\n",
    "            for to_type in node_type_keys:\n",
    "                aql = f\"\"\"\n",
    "                FOR v1 IN {node_collection}\n",
    "                    FILTER v1.type == '{from_type}'\n",
    "                    LIMIT 1\n",
    "                    FOR v2 IN {node_collection}\n",
    "                        FILTER v2.type == '{to_type}'\n",
    "                        LIMIT 1\n",
    "                        FOR e IN {edge_collection}\n",
    "                            FILTER e._from == v1._id AND e._to == v2._id\n",
    "                            RETURN DISTINCT {{\n",
    "                                \"from_type\": '{from_type}',\n",
    "                                \"to_type\": '{to_type}',\n",
    "                                \"edge_type\": e.edge_type\n",
    "                            }}\n",
    "                \"\"\"\n",
    "                cursor = db.aql.execute(aql)\n",
    "                relationships = [doc for doc in cursor]\n",
    "                \n",
    "                for rel in relationships:\n",
    "                    type_relationships.append(rel)\n",
    "        \n",
    "        # Update db_schema with type relationships\n",
    "        global db_schema\n",
    "        db_schema[\"Type Relationships\"] = type_relationships\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing type relationships: {str(e)}\")\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_directory_structure() -> Dict:\n",
    "    \"\"\"\n",
    "    Build a hierarchical representation of the directory structure\n",
    "    Returns:\n",
    "        Dictionary representing the directory tree\n",
    "    \"\"\"\n",
    "    directory_tree = {}\n",
    "    \n",
    "    try:\n",
    "        # First, identify all directory nodes\n",
    "        directory_field = 'type'\n",
    "        if 'directory' in node_types:\n",
    "            directory_field = node_types['directory'].get('field', 'type')\n",
    "            \n",
    "        # Get all directory nodes\n",
    "        aql = f\"\"\"\n",
    "        FOR v IN {node_collection}\n",
    "            FILTER v.{directory_field} == 'directory'\n",
    "            RETURN {{\n",
    "                \"key\": v._key,\n",
    "                \"path\": v.path,\n",
    "                \"name\": v.name\n",
    "            }}\n",
    "        \"\"\"\n",
    "        cursor = db.aql.execute(aql)\n",
    "        directories = [doc for doc in cursor]\n",
    "        \n",
    "        # If no explicit directory nodes found, try to extract directories from file paths\n",
    "        if not directories:\n",
    "            # Extract directories from file paths\n",
    "            all_directories = set()\n",
    "            for file_info in files.values():\n",
    "                file_path = file_info.get(\"file_path\", \"\")\n",
    "                if file_path:\n",
    "                    # Extract all parent directories\n",
    "                    parts = file_path.split('/')\n",
    "                    for i in range(1, len(parts)):\n",
    "                        dir_path = '/'.join(parts[:i])\n",
    "                        if dir_path:\n",
    "                            all_directories.add(dir_path)\n",
    "            \n",
    "            # Create synthetic directory nodes\n",
    "            directories = [{\"path\": dir_path, \"name\": dir_path.split('/')[-1]} for dir_path in all_directories]\n",
    "            \n",
    "        # Build directory tree\n",
    "        for directory in directories:\n",
    "            path = directory.get(\"path\", \"\")\n",
    "            if not path:\n",
    "                continue\n",
    "                \n",
    "            # Add to tree\n",
    "            current = directory_tree\n",
    "            parts = path.split('/')\n",
    "            for i, part in enumerate(parts):\n",
    "                if not part:\n",
    "                    continue\n",
    "                    \n",
    "                if part not in current:\n",
    "                    current[part] = {\"files\": [], \"dirs\": {}}\n",
    "                    \n",
    "                if i == len(parts) - 1:\n",
    "                    # This is the target directory, add its key\n",
    "                    current[part][\"key\"] = directory.get(\"key\")\n",
    "                else:\n",
    "                    current = current[part][\"dirs\"]\n",
    "                    \n",
    "        # Add files to their respective directories\n",
    "        for file_key, file_info in files.items():\n",
    "            file_path = file_info.get(\"file_path\", \"\")\n",
    "            if not file_path:\n",
    "                continue\n",
    "                \n",
    "            # Determine directory path and file name\n",
    "            parts = file_path.split('/')\n",
    "            file_name = parts[-1]\n",
    "            dir_path = '/'.join(parts[:-1])\n",
    "            \n",
    "            # Find directory in tree\n",
    "            current = directory_tree\n",
    "            if dir_path:\n",
    "                found = True\n",
    "                for part in dir_path.split('/'):\n",
    "                    if not part:\n",
    "                        continue\n",
    "                    if part in current:\n",
    "                        current = current[part][\"dirs\"]\n",
    "                    else:\n",
    "                        # Directory not found in tree, create it\n",
    "                        found = False\n",
    "                        break\n",
    "                        \n",
    "                if not found:\n",
    "                    # Create missing directory path\n",
    "                    current = directory_tree\n",
    "                    for part in dir_path.split('/'):\n",
    "                        if not part:\n",
    "                            continue\n",
    "                        if part not in current:\n",
    "                            current[part] = {\"files\": [], \"dirs\": {}}\n",
    "                        current = current[part][\"dirs\"]\n",
    "            \n",
    "            # Find parent directory and add file\n",
    "            parent = directory_tree\n",
    "            for part in parts[:-1]:\n",
    "                if not part:\n",
    "                    continue\n",
    "                if part not in parent:\n",
    "                    parent[part] = {\"files\": [], \"dirs\": {}}\n",
    "                parent = parent[part][\"dirs\"]\n",
    "            \n",
    "            # Add file to parent directory if parent exists and is a valid dictionary\n",
    "            if parts[-2] in parent and isinstance(parent[parts[-2]], dict):\n",
    "                parent[parts[-2]][\"files\"].append({\n",
    "                    \"key\": file_key,\n",
    "                    \"name\": file_name,\n",
    "                    \"path\": file_path,\n",
    "                    \"language\": file_info.get(\"language\", \"\")\n",
    "                })\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error building directory structure: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        \n",
    "    return directory_tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_cache():\n",
    "    \"\"\"Initialize cache of files, code snippets, and symbols using detected schema fields\"\"\"\n",
    "    global files, snippets, symbols, symbol_name_index\n",
    "    \n",
    "    try:\n",
    "        # Initialize file cache\n",
    "        if 'file' in node_types:\n",
    "            # Determine best field for file info\n",
    "            field_info = node_types['file']\n",
    "            \n",
    "            path_field = None\n",
    "            name_field = None\n",
    "            \n",
    "            # Try to find the best fields for path and name\n",
    "            sample = field_info.get('sample', {})\n",
    "            for field in sample:\n",
    "                lower_field = field.lower()\n",
    "                if 'path' in lower_field and not path_field:\n",
    "                    path_field = field\n",
    "                elif ('name' in lower_field or 'file' in lower_field) and 'path' not in lower_field and not name_field:\n",
    "                    name_field = field\n",
    "            \n",
    "            # Use detected fields or defaults\n",
    "            path_field = path_field or 'path'\n",
    "            name_field = name_field or 'file_name'\n",
    "            type_field = field_info.get('field') or 'type'\n",
    "            \n",
    "            aql = f\"\"\"\n",
    "            FOR v IN {node_collection}\n",
    "                FILTER v.{type_field} == 'file'\n",
    "                RETURN v\n",
    "            \"\"\"\n",
    "            cursor = db.aql.execute(aql)\n",
    "            \n",
    "            # Process each file\n",
    "            for doc in cursor:\n",
    "                file_key = doc.get('_key')\n",
    "                file_path = doc.get(path_field, \"\")\n",
    "                file_name = doc.get(name_field, \"\")\n",
    "                \n",
    "                if not file_path and not file_name:\n",
    "                    continue\n",
    "                \n",
    "                if not file_path and file_name:\n",
    "                    # Try to construct a path\n",
    "                    for key in doc:\n",
    "                        if 'dir' in key.lower() or 'folder' in key.lower():\n",
    "                            directory = doc.get(key, \"\")\n",
    "                            file_path = f\"{directory}/{file_name}\" if directory else file_name\n",
    "                            break\n",
    "                \n",
    "                language = \"\"\n",
    "                # Try to detect language from extension\n",
    "                if file_path:\n",
    "                    ext = file_path.split('.')[-1].lower() if '.' in file_path else \"\"\n",
    "                    if ext == 'py':\n",
    "                        language = 'python'\n",
    "                    elif ext in ['js', 'ts']:\n",
    "                        language = 'javascript'\n",
    "                    elif ext in ['java']:\n",
    "                        language = 'java'\n",
    "                    elif ext in ['c', 'cpp', 'h', 'hpp']:\n",
    "                        language = 'c/c++'\n",
    "                \n",
    "                files[file_key] = {\n",
    "                    \"key\": file_key,\n",
    "                    \"file_name\": file_name,\n",
    "                    \"file_path\": file_path,\n",
    "                    \"language\": language\n",
    "                }\n",
    "            \n",
    "            print(f\"Cached {len(files)} files\")\n",
    "            \n",
    "        # Initialize snippet cache\n",
    "        if 'snippet' in node_types:\n",
    "            # Determine best fields for snippet info\n",
    "            field_info = node_types['snippet']\n",
    "            \n",
    "            content_field = None\n",
    "            name_field = None\n",
    "            \n",
    "            # Try to find the best fields for content and name\n",
    "            sample = field_info.get('sample', {})\n",
    "            for field in sample:\n",
    "                lower_field = field.lower()\n",
    "                if ('content' in lower_field or 'code' in lower_field) and not content_field:\n",
    "                    content_field = field\n",
    "                elif ('name' in lower_field or 'title' in lower_field) and not name_field:\n",
    "                    name_field = field\n",
    "            \n",
    "            # Use detected fields or defaults\n",
    "            content_field = content_field or 'content'\n",
    "            name_field = name_field or 'snippet_name'\n",
    "            type_field = field_info.get('field') or 'type'\n",
    "            \n",
    "            aql = f\"\"\"\n",
    "            FOR v IN {node_collection}\n",
    "                FILTER v.{type_field} == 'snippet'\n",
    "                RETURN v\n",
    "            \"\"\"\n",
    "            cursor = db.aql.execute(aql)\n",
    "            \n",
    "            # Process each snippet\n",
    "            for doc in cursor:\n",
    "                snippet_key = doc.get('_key')\n",
    "                content = doc.get(content_field, \"\")\n",
    "                snippet_name = doc.get(name_field, \"\")\n",
    "                \n",
    "                if not content:\n",
    "                    continue\n",
    "                \n",
    "                # Try to determine file relationship\n",
    "                file_key = None\n",
    "                for key in doc:\n",
    "                    if 'file' in key.lower() and key != name_field:\n",
    "                        file_key = doc.get(key)\n",
    "                        break\n",
    "                \n",
    "                # Try to determine language\n",
    "                language = \"\"\n",
    "                for key in doc:\n",
    "                    if 'lang' in key.lower():\n",
    "                        language = doc.get(key, \"\")\n",
    "                        break\n",
    "                \n",
    "                if not language and file_key in files:\n",
    "                    language = files[file_key].get('language', \"\")\n",
    "                \n",
    "                snippets[snippet_key] = {\n",
    "                    \"key\": snippet_key,\n",
    "                    \"snippet_name\": snippet_name,\n",
    "                    \"content\": content,\n",
    "                    \"file_key\": file_key,\n",
    "                    \"language\": language\n",
    "                }\n",
    "            \n",
    "            print(f\"Cached {len(snippets)} code snippets\")\n",
    "        \n",
    "            # Initialize symbol cache\n",
    "            if 'symbol' in node_types:  # This is checking for an exact match with 'symbol'\n",
    "                # Determine best fields for symbol info\n",
    "                field_info = node_types['symbol']\n",
    "                \n",
    "                name_field = None\n",
    "                type_name_field = None\n",
    "                \n",
    "                # Try to find the best fields for symbol name and symbol type\n",
    "                sample = field_info.get('sample', {})\n",
    "                for field in sample:\n",
    "                    lower_field = field.lower()\n",
    "                    if 'name' in lower_field and not name_field:\n",
    "                        name_field = field\n",
    "                    elif ('type' in lower_field and 'name' in lower_field) and not type_name_field:\n",
    "                        type_name_field = field\n",
    "                \n",
    "                # Add fallback detection for symbol name field\n",
    "                if not name_field and 'context' in sample:\n",
    "                    name_field = 'context'\n",
    "                    print(f\"Using 'context' as fallback for symbol name field\")\n",
    "                \n",
    "                # Use detected fields or defaults\n",
    "                name_field = name_field or 'symbol_name'\n",
    "                type_name_field = type_name_field or 'symbol_type'\n",
    "                type_field = field_info.get('field') or 'type'\n",
    "                \n",
    "                print(f\"Using name_field: {name_field}, type_field: {type_field}\")\n",
    "                \n",
    "                aql = f\"\"\"\n",
    "                FOR v IN {node_collection}\n",
    "                    FILTER v.{type_field} == 'symbol'\n",
    "                    RETURN v\n",
    "                \"\"\"\n",
    "                print(f\"Symbol query: {aql}\")\n",
    "                cursor = db.aql.execute(aql)\n",
    "                sample_symbols = [doc for doc in cursor]\n",
    "                print(f\"Sample symbol count: {len(sample_symbols)}\")\n",
    "                \n",
    "                if sample_symbols:\n",
    "                    print(f\"Sample symbol fields: {list(sample_symbols[0].keys())}\")\n",
    "                    print(f\"Sample symbol name value: {sample_symbols[0].get(name_field, 'NOT FOUND')}\")\n",
    "                    print(f\"Sample symbol type value: {sample_symbols[0].get(type_name_field, 'NOT FOUND')}\")\n",
    "\n",
    "                # Re-execute the query\n",
    "                cursor = db.aql.execute(aql)\n",
    "                \n",
    "                # Process counter\n",
    "                processed_count = 0\n",
    "                \n",
    "                # Process each symbol\n",
    "                for doc in cursor:\n",
    "                    symbol_key = doc.get('_key')\n",
    "                    symbol_name = doc.get(name_field, \"\")\n",
    "                    symbol_type = doc.get(type_name_field, \"\")\n",
    "                    \n",
    "                    if not symbol_name:\n",
    "                        # Try context as a fallback\n",
    "                        symbol_name = doc.get('context', \"\")\n",
    "                        if not symbol_name:\n",
    "                            continue\n",
    "                    \n",
    "                    # Try to determine file relationship\n",
    "                    file_key = None\n",
    "                    for key in doc:\n",
    "                        if 'file' in key.lower() and key != name_field:\n",
    "                            file_key = doc.get(key)\n",
    "                            break\n",
    "                    \n",
    "                    # Try to determine snippet relationship\n",
    "                    snippet_key = None\n",
    "                    for key in doc:\n",
    "                        if 'snippet' in key.lower():\n",
    "                            snippet_key = doc.get(key)\n",
    "                            break\n",
    "                    \n",
    "                    # Try to get definition and documentation\n",
    "                    definition = \"\"\n",
    "                    documentation = doc.get('docstring', \"\")  # Try the known docstring field first\n",
    "                    \n",
    "                    for key in doc:\n",
    "                        lower_key = key.lower()\n",
    "                        if 'def' in lower_key or 'decl' in lower_key:\n",
    "                            definition = doc.get(key, \"\")\n",
    "                        elif ('doc' in lower_key or 'comment' in lower_key) and not documentation:\n",
    "                            documentation = doc.get(key, \"\")\n",
    "                    \n",
    "                    symbols[symbol_key] = {\n",
    "                        \"key\": symbol_key,\n",
    "                        \"symbol_name\": symbol_name,\n",
    "                        \"symbol_type\": symbol_type,\n",
    "                        \"file_key\": file_key,\n",
    "                        \"snippet_key\": snippet_key,\n",
    "                        \"definition\": definition,\n",
    "                        \"documentation\": documentation\n",
    "                    }\n",
    "                    \n",
    "                    # Index by name for quick lookups\n",
    "                    if symbol_name:\n",
    "                        if symbol_name not in symbol_name_index:\n",
    "                            symbol_name_index[symbol_name] = []\n",
    "                        symbol_name_index[symbol_name].append(symbol_key)\n",
    "                    \n",
    "                    processed_count += 1\n",
    "                    if processed_count % 200 == 0:\n",
    "                        print(f\"Processed {processed_count} symbols so far\")\n",
    "                \n",
    "                print(f\"Cached {len(symbols)} symbols\")\n",
    "            \n",
    "        # Build relationship indexes for faster traversal\n",
    "        build_relationship_indexes()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing cache: {str(e)}\")\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_relationship_indexes():\n",
    "    \"\"\"Build indexes for quick relationship lookup between files, snippets and symbols\"\"\"\n",
    "    global file_to_snippets, file_to_symbols, snippet_to_symbols\n",
    "    \n",
    "    try:\n",
    "        # Build file -> snippets index\n",
    "        for snippet_key, snippet in snippets.items():\n",
    "            file_key = snippet.get('file_key')\n",
    "            if file_key:\n",
    "                if file_key not in file_to_snippets:\n",
    "                    file_to_snippets[file_key] = []\n",
    "                file_to_snippets[file_key].append(snippet_key)\n",
    "        \n",
    "        # Build file -> symbols index\n",
    "        for symbol_key, symbol in symbols.items():\n",
    "            file_key = symbol.get('file_key')\n",
    "            if file_key:\n",
    "                if file_key not in file_to_symbols:\n",
    "                    file_to_symbols[file_key] = []\n",
    "                file_to_symbols[file_key].append(symbol_key)\n",
    "        \n",
    "        # Build snippet -> symbols index\n",
    "        for symbol_key, symbol in symbols.items():\n",
    "            snippet_key = symbol.get('snippet_key')\n",
    "            if snippet_key:\n",
    "                if snippet_key not in snippet_to_symbols:\n",
    "                    snippet_to_symbols[snippet_key] = []\n",
    "                snippet_to_symbols[snippet_key].append(symbol_key)\n",
    "        \n",
    "        print(\"Built relationship indexes for files, snippets, and symbols\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error building relationship indexes: {str(e)}\")\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_by_key(file_key: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Helper method to retrieve file node by key\n",
    "    \n",
    "    Args:\n",
    "        file_key: The key of the file node\n",
    "        \n",
    "    Returns:\n",
    "        Dict containing file information\n",
    "    \"\"\"\n",
    "    if file_key in files:\n",
    "        return files[file_key]\n",
    "    \n",
    "    try:\n",
    "        aql = f\"\"\"\n",
    "        FOR file IN {node_collection}\n",
    "            FILTER file._key == '{file_key}' AND file.type == 'file'\n",
    "            RETURN {{\n",
    "                \"key\": file._key,\n",
    "                \"directory\": file.directory,\n",
    "                \"file_name\": file.file_name,\n",
    "                \"file_path\": file.path || (file.directory + '/' + file.file_name),\n",
    "                \"language\": file.language\n",
    "            }}\n",
    "        \"\"\"\n",
    "        cursor = db.aql.execute(aql)\n",
    "        found_files = [doc for doc in cursor]\n",
    "        \n",
    "        if found_files:\n",
    "            files[file_key] = found_files[0]\n",
    "            return found_files[0]\n",
    "        \n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving file by key: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_symbol_occurrences(symbol_name: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Find all occurrences of a symbol using both the symbol nodes and code snippets\n",
    "    \n",
    "    Args:\n",
    "        symbol_name: The name of the symbol to find\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries containing symbol occurrences\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    try:\n",
    "        # Look for symbol nodes\n",
    "        if 'symbol' in node_types:\n",
    "            aql = f\"\"\"\n",
    "            FOR symbol IN {node_collection}\n",
    "                FILTER symbol.type == 'symbol' AND symbol.name == '{symbol_name}'\n",
    "                LET file = (\n",
    "                    FOR edge IN {edge_collection}\n",
    "                        FILTER edge._to == symbol._id\n",
    "                        FOR file IN {node_collection}\n",
    "                            FILTER file._id == edge._from AND file.type == 'file'\n",
    "                            RETURN {{\n",
    "                                \"key\": file._key,\n",
    "                                \"directory\": file.directory,\n",
    "                                \"file_name\": file.file_name,\n",
    "                                \"file_path\": file.path || (file.directory + '/' + file.file_name),\n",
    "                                \"language\": file.language\n",
    "                            }}\n",
    "                )\n",
    "                RETURN {{\n",
    "                    \"type\": \"symbol\",\n",
    "                    \"name\": symbol.name,\n",
    "                    \"symbol_type\": symbol.symbol_type,\n",
    "                    \"line_number\": symbol.line_number,\n",
    "                    \"context\": symbol.context,\n",
    "                    \"docstring\": symbol.docstring,\n",
    "                    \"file\": LENGTH(file) > 0 ? file[0] : null\n",
    "                }}\n",
    "            \"\"\"\n",
    "            cursor = db.aql.execute(aql)\n",
    "            symbol_results = [doc for doc in cursor]\n",
    "            results.extend(symbol_results)\n",
    "        \n",
    "        # Look for symbol occurrences in code snippets\n",
    "        if 'snippet' in node_types:\n",
    "            # Determine the best attribute for code based on the sample\n",
    "            code_field = 'code_snippet'\n",
    "            snippet_sample = node_types.get('snippet', {}).get('sample', {})\n",
    "            \n",
    "            if 'code_snippet' in snippet_sample:\n",
    "                code_field = 'code_snippet'\n",
    "            elif 'code' in snippet_sample:\n",
    "                code_field = 'code'\n",
    "            elif 'snippet' in snippet_sample:\n",
    "                code_field = 'snippet'\n",
    "            \n",
    "            aql = f\"\"\"\n",
    "            FOR snippet IN {node_collection}\n",
    "                FILTER snippet.type == 'snippet' AND snippet.{code_field} LIKE '%{symbol_name}%'\n",
    "                LET file = (\n",
    "                    FOR edge IN {edge_collection}\n",
    "                        FILTER edge._to == snippet._id\n",
    "                        FOR file IN {node_collection}\n",
    "                            FILTER file._id == edge._from AND file.type == 'file'\n",
    "                            RETURN {{\n",
    "                                \"key\": file._key,\n",
    "                                \"directory\": file.directory,\n",
    "                                \"file_name\": file.file_name,\n",
    "                                \"file_path\": file.path || (file.directory + '/' + file.file_name),\n",
    "                                \"language\": file.language\n",
    "                            }}\n",
    "                )\n",
    "                RETURN {{\n",
    "                    \"type\": \"snippet\",\n",
    "                    \"code\": snippet.{code_field},\n",
    "                    \"start_line\": snippet.start_line,\n",
    "                    \"end_line\": snippet.end_line,\n",
    "                    \"file\": LENGTH(file) > 0 ? file[0] : null\n",
    "                }}\n",
    "            \"\"\"\n",
    "            cursor = db.aql.execute(aql)\n",
    "            snippet_results = [doc for doc in cursor]\n",
    "            results.extend(snippet_results)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error finding symbol occurrences: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_by_name(name: str, symbol_type: Optional[str] = None) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Find function/class snippets by name with improved matching across all files\n",
    "    \n",
    "    Args:\n",
    "        name: The name of the function/class to find\n",
    "        symbol_type: Optional filter for symbol type (e.g., 'function', 'class')\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries containing matching symbols and snippets\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    try:\n",
    "        # Look for symbol nodes first\n",
    "        if 'symbol' in node_types:\n",
    "            type_filter = f\" AND symbol.symbol_type == '{symbol_type}'\" if symbol_type else \"\"\n",
    "            \n",
    "            aql = f\"\"\"\n",
    "            FOR symbol IN {node_collection}\n",
    "                FILTER symbol.type == 'symbol' AND symbol.name == '{name}'{type_filter}\n",
    "                LET file = (\n",
    "                    FOR edge IN {edge_collection}\n",
    "                        FILTER edge._to == symbol._id\n",
    "                        FOR file IN {node_collection}\n",
    "                            FILTER file._id == edge._from AND file.type == 'file'\n",
    "                            RETURN {{\n",
    "                                \"key\": file._key,\n",
    "                                \"directory\": file.directory,\n",
    "                                \"file_name\": file.file_name,\n",
    "                                \"file_path\": file.path || (file.directory + '/' + file.file_name),\n",
    "                                \"language\": file.language\n",
    "                            }}\n",
    "                )\n",
    "                LET snippet = (\n",
    "                    FOR edge IN {edge_collection}\n",
    "                        FILTER edge._from == symbol._id\n",
    "                        FOR snippet IN {node_collection}\n",
    "                            FILTER snippet._id == edge._to AND snippet.type == 'snippet'\n",
    "                            RETURN snippet\n",
    "                )\n",
    "                RETURN {{\n",
    "                    \"type\": \"symbol\",\n",
    "                    \"name\": symbol.name,\n",
    "                    \"symbol_type\": symbol.symbol_type,\n",
    "                    \"line_number\": symbol.line_number,\n",
    "                    \"context\": symbol.context,\n",
    "                    \"docstring\": symbol.docstring,\n",
    "                    \"file\": LENGTH(file) > 0 ? file[0] : null,\n",
    "                    \"snippet\": LENGTH(snippet) > 0 ? snippet[0] : null\n",
    "                }}\n",
    "            \"\"\"\n",
    "            cursor = db.aql.execute(aql)\n",
    "            symbol_results = [doc for doc in cursor]\n",
    "            results.extend(symbol_results)\n",
    "        \n",
    "        # If no symbols found or symbol cache is empty, try fuzzy matching in snippets\n",
    "        if not results and 'snippet' in node_types:\n",
    "            # Determine the best attribute for code based on the sample\n",
    "            code_field = 'code_snippet'\n",
    "            snippet_sample = node_types.get('snippet', {}).get('sample', {})\n",
    "            \n",
    "            if 'code_snippet' in snippet_sample:\n",
    "                code_field = 'code_snippet'\n",
    "            elif 'code' in snippet_sample:\n",
    "                code_field = 'code'\n",
    "            elif 'snippet' in snippet_sample:\n",
    "                code_field = 'snippet'\n",
    "            \n",
    "            # Common patterns for function/class definitions in different languages\n",
    "            patterns = []\n",
    "            \n",
    "            if not symbol_type or symbol_type == 'function':\n",
    "                patterns.extend([\n",
    "                    f\"function {name}\",  # JavaScript\n",
    "                    f\"def {name}\",       # Python\n",
    "                    f\"{name} = function\", # JavaScript\n",
    "                    f\"const {name} = \", # JavaScript arrow function\n",
    "                    f\"let {name} = \",   # JavaScript arrow function\n",
    "                    f\"var {name} = \",   # JavaScript arrow function\n",
    "                    f\"{name}\\\\(\",       # C/C++/Java method\n",
    "                    f\"func {name}\",     # Go\n",
    "                ])\n",
    "            \n",
    "            if not symbol_type or symbol_type == 'class':\n",
    "                patterns.extend([\n",
    "                    f\"class {name}\",     # Python/JavaScript/Java\n",
    "                    f\"interface {name}\", # TypeScript/Java\n",
    "                    f\"struct {name}\",    # C/C++/Go\n",
    "                    f\"type {name} struct\", # Go\n",
    "                ])\n",
    "            \n",
    "            # Create LIKE conditions for each pattern\n",
    "            like_conditions = [f\"snippet.{code_field} LIKE '%{pattern}%'\" for pattern in patterns]\n",
    "            like_filter = \" OR \".join(like_conditions)\n",
    "            \n",
    "            aql = f\"\"\"\n",
    "            FOR snippet IN {node_collection}\n",
    "                FILTER snippet.type == 'snippet' AND ({like_filter})\n",
    "                LET file = (\n",
    "                    FOR edge IN {edge_collection}\n",
    "                        FILTER edge._to == snippet._id\n",
    "                        FOR file IN {node_collection}\n",
    "                            FILTER file._id == edge._from AND file.type == 'file'\n",
    "                            RETURN {{\n",
    "                                \"key\": file._key,\n",
    "                                \"directory\": file.directory,\n",
    "                                \"file_name\": file.file_name,\n",
    "                                \"file_path\": file.path || (file.directory + '/' + file.file_name),\n",
    "                                \"language\": file.language\n",
    "                            }}\n",
    "                )\n",
    "                RETURN {{\n",
    "                    \"type\": \"snippet\",\n",
    "                    \"code\": snippet.{code_field},\n",
    "                    \"start_line\": snippet.start_line,\n",
    "                    \"end_line\": snippet.end_line,\n",
    "                    \"file\": LENGTH(file) > 0 ? file[0] : null\n",
    "                }}\n",
    "            \"\"\"\n",
    "            cursor = db.aql.execute(aql)\n",
    "            snippet_results = [doc for doc in cursor]\n",
    "            results.extend(snippet_results)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error finding by name: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_symbol(name: str, symbol_type: Optional[str] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Query about a specific function/class and get an analysis in JSON format.\n",
    "    Will return all implementations across different files.\n",
    "    \n",
    "    Args:\n",
    "        name: The name of the function/class to analyze\n",
    "        symbol_type: Optional filter for symbol type (e.g., 'function', 'class')\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing analysis of the symbol\n",
    "    \"\"\"\n",
    "    # First, find all occurrences\n",
    "    occurrences = find_by_name(name, symbol_type)\n",
    "    \n",
    "    if not occurrences:\n",
    "        return {\"error\": f\"No {symbol_type or 'symbol'} named '{name}' found in the codebase\"}\n",
    "    \n",
    "    # Extract code snippets and organize by file\n",
    "    implementations_by_file = {}\n",
    "    for occurrence in occurrences:\n",
    "        file_info = occurrence.get(\"file\", {})\n",
    "        file_path = file_info.get(\"file_path\", \"unknown_path\")\n",
    "        \n",
    "        if file_path not in implementations_by_file:\n",
    "            implementations_by_file[file_path] = {\n",
    "                \"file_info\": file_info,\n",
    "                \"implementations\": []\n",
    "            }\n",
    "        \n",
    "        if occurrence.get(\"type\") == \"symbol\":\n",
    "            # For symbol occurrence, get its snippet\n",
    "            snippet = occurrence.get(\"snippet\", {})\n",
    "            implementations_by_file[file_path][\"implementations\"].append({\n",
    "                \"type\": occurrence.get(\"symbol_type\", \"unknown\"),\n",
    "                \"name\": occurrence.get(\"name\", name),\n",
    "                \"line_number\": occurrence.get(\"line_number\"),\n",
    "                \"docstring\": occurrence.get(\"docstring\", \"\"),\n",
    "                \"context\": occurrence.get(\"context\", \"\"),\n",
    "                \"code\": snippet.get(\"code_snippet\", snippet.get(\"code\", snippet.get(\"snippet\", \"\")))\n",
    "            })\n",
    "        elif occurrence.get(\"type\") == \"snippet\":\n",
    "            # For snippet occurrence\n",
    "            implementations_by_file[file_path][\"implementations\"].append({\n",
    "                \"type\": symbol_type or \"unknown\",\n",
    "                \"name\": name,\n",
    "                \"line_number\": occurrence.get(\"start_line\"),\n",
    "                \"code\": occurrence.get(\"code\", \"\")\n",
    "            })\n",
    "    \n",
    "    # Use Mistral LLM to analyze the symbol\n",
    "    symbol_analysis = analyze_with_llm(name, symbol_type, implementations_by_file)\n",
    "    \n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"type\": symbol_type or \"unknown\",\n",
    "        \"implementations_count\": len(occurrences),\n",
    "        \"files_count\": len(implementations_by_file),\n",
    "        \"implementations_by_file\": implementations_by_file,\n",
    "        \"analysis\": symbol_analysis\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_with_llm(name: str, symbol_type: Optional[str], implementations: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Use Mistral API to analyze a symbol based on its implementations\n",
    "    \n",
    "    Args:\n",
    "        name: The name of the symbol to analyze\n",
    "        symbol_type: The type of the symbol (function, class, etc.)\n",
    "        implementations: Dictionary with implementations by file\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with LLM analysis\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract all code snippets from implementations\n",
    "        all_code = []\n",
    "        for file_path, file_data in implementations.items():\n",
    "            for implementation in file_data[\"implementations\"]:\n",
    "                code = implementation.get(\"code\", \"\")\n",
    "                docstring = implementation.get(\"docstring\", \"\")\n",
    "                if code:\n",
    "                    all_code.append(f\"File: {file_path}\\n{code}\")\n",
    "                if docstring:\n",
    "                    all_code.append(f\"Docstring: {docstring}\")\n",
    "        \n",
    "        # Join all code with separators\n",
    "        code_text = \"\\n\\n\" + \"-\" * 40 + \"\\n\\n\".join(all_code)\n",
    "        \n",
    "        # Create a prompt for the LLM\n",
    "        prompt = f\"\"\"\n",
    "        Please analyze this {symbol_type or 'symbol'} named '{name}' from a codebase:\n",
    "        \n",
    "        {code_text}\n",
    "        \n",
    "        Provide a JSON response with the following fields:\n",
    "        1. purpose: A clear description of what this {symbol_type or 'symbol'} does\n",
    "        2. parameters: List of parameters with their types and purpose (if applicable)\n",
    "        3. return_value: What this {symbol_type or 'symbol'} returns (if applicable)\n",
    "        4. dependencies: Other functions/classes/modules it depends on\n",
    "        5. usage_pattern: How this {symbol_type or 'symbol'} is typically used\n",
    "        6. edge_cases: Potential edge cases or error handling\n",
    "        7. complexity: Analysis of time/space complexity (if applicable)\n",
    "        8. suggestions: Any improvements or best practices that could be applied\n",
    "        \n",
    "        Format your response as a valid JSON object without any extra text or markdown.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create message for the LLM\n",
    "        messages = [\n",
    "            ChatMessage(role=\"user\", content=prompt)\n",
    "        ]\n",
    "        \n",
    "        # Get completion from Mistral\n",
    "        chat_response = mistral_client.chat(\n",
    "            model=model,\n",
    "            messages=messages\n",
    "        )\n",
    "        \n",
    "        # Extract the content from the response\n",
    "        content = chat_response.choices[0].message.content\n",
    "        \n",
    "        # Try to parse the response as JSON\n",
    "        try:\n",
    "            analysis = json.loads(content)\n",
    "            return analysis\n",
    "        except json.JSONDecodeError:\n",
    "            # If JSON parsing fails, return the raw text\n",
    "            return {\"raw_analysis\": content}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing with LLM: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return {\"error\": str(e)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_error(error_message: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze a specific error message in the codebase and suggest solutions\n",
    "    \n",
    "    Args:\n",
    "        error_message: The error message to analyze\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing error analysis and potential solutions\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # First, search for similar error patterns in the code\n",
    "        # Split error message into keywords\n",
    "        keywords = error_message.lower().split()\n",
    "        keywords = [kw for kw in keywords if len(kw) > 3]  # Filter out short words\n",
    "        \n",
    "        # Create LIKE conditions for each keyword\n",
    "        code_field = 'code_snippet'\n",
    "        snippet_sample = node_types.get('snippet', {}).get('sample', {})\n",
    "        \n",
    "        if 'code_snippet' in snippet_sample:\n",
    "            code_field = 'code_snippet'\n",
    "        elif 'code' in snippet_sample:\n",
    "            code_field = 'code'\n",
    "        elif 'snippet' in snippet_sample:\n",
    "            code_field = 'snippet'\n",
    "            \n",
    "        # Find code snippets that might contain error handling for similar errors\n",
    "        related_snippets = []\n",
    "        \n",
    "        for keyword in keywords:\n",
    "            aql = f\"\"\"\n",
    "            FOR snippet IN {node_collection}\n",
    "                FILTER snippet.type == 'snippet' \n",
    "                AND (\n",
    "                    snippet.{code_field} LIKE '%error%' \n",
    "                    AND snippet.{code_field} LIKE '%{keyword}%'\n",
    "                )\n",
    "                LET file = (\n",
    "                    FOR edge IN {edge_collection}\n",
    "                        FILTER edge._to == snippet._id\n",
    "                        FOR file IN {node_collection}\n",
    "                            FILTER file._id == edge._from AND file.type == 'file'\n",
    "                            RETURN {{\n",
    "                                \"key\": file._key,\n",
    "                                \"file_path\": file.path || (file.directory + '/' + file.file_name)\n",
    "                            }}\n",
    "                )\n",
    "                RETURN {{\n",
    "                    \"code\": snippet.{code_field},\n",
    "                    \"start_line\": snippet.start_line,\n",
    "                    \"end_line\": snippet.end_line,\n",
    "                    \"file\": LENGTH(file) > 0 ? file[0] : null\n",
    "                }}\n",
    "            \"\"\"\n",
    "            cursor = db.aql.execute(aql)\n",
    "            for doc in cursor:\n",
    "                if doc not in related_snippets:\n",
    "                    related_snippets.append(doc)\n",
    "        \n",
    "        # Format snippets for LLM\n",
    "        snippets_text = \"\"\n",
    "        for i, snippet in enumerate(related_snippets):\n",
    "            file_info = snippet.get(\"file\", {})\n",
    "            file_path = file_info.get(\"file_path\", \"unknown\")\n",
    "            code = snippet.get(\"code\", \"\")\n",
    "            \n",
    "            snippets_text += f\"\\nSnippet {i+1} from {file_path}:\\n{code}\\n\"\n",
    "        \n",
    "        # Create a prompt for the LLM\n",
    "        prompt = f\"\"\"\n",
    "        Please analyze this error message from a codebase:\n",
    "        \n",
    "        ```\n",
    "        {error_message}\n",
    "        ```\n",
    "        \n",
    "        I found these potentially related code snippets from the codebase:\n",
    "        {snippets_text if snippets_text else \"No directly related snippets found.\"}\n",
    "        \n",
    "        Provide a JSON response with the following fields:\n",
    "        1. error_type: Classification of this error\n",
    "        2. likely_causes: List of potential causes for this error\n",
    "        3. affected_components: Which parts of the code might be affected\n",
    "        4. solution_suggestions: Specific recommendations to fix this error\n",
    "        5. preventive_measures: How to prevent this type of error in the future\n",
    "        \n",
    "        Format your response as a valid JSON object without any extra text or markdown.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create message for the LLM\n",
    "        messages = [\n",
    "            ChatMessage(role=\"user\", content=prompt)\n",
    "        ]\n",
    "        \n",
    "        # Get completion from Mistral\n",
    "        chat_response = mistral_client.chat(\n",
    "            model=model,\n",
    "            messages=messages\n",
    "        )\n",
    "        \n",
    "        # Extract the content from the response\n",
    "        content = chat_response.choices[0].message.content\n",
    "        \n",
    "        # Try to parse the response as JSON\n",
    "        try:\n",
    "            analysis = json.loads(content)\n",
    "            return {\n",
    "                \"error_message\": error_message,\n",
    "                \"related_snippets_count\": len(related_snippets),\n",
    "                \"analysis\": analysis\n",
    "            }\n",
    "        except json.JSONDecodeError:\n",
    "            # If JSON parsing fails, return the raw text\n",
    "            return {\n",
    "                \"error_message\": error_message,\n",
    "                \"related_snippets_count\": len(related_snippets),\n",
    "                \"raw_analysis\": content\n",
    "            }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing error: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_database_structure() -> Dict:\n",
    "    \"\"\"\n",
    "    Answer questions about the database structure\n",
    "    Returns:\n",
    "        Dictionary containing information about the database structure\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Most of this information was already gathered during initialization\n",
    "        # Just format it in a more user-friendly way\n",
    "        # Extract node types with counts\n",
    "        node_types_info = {}\n",
    "        for node_type, info in node_types.items():\n",
    "            node_types_info[node_type] = {\n",
    "                \"count\": info.get(\"count\", 0),\n",
    "                \"properties\": info.get(\"sample_structure\", [])\n",
    "            }\n",
    "            \n",
    "        # Extract relationship types\n",
    "        relationship_types = {}\n",
    "        for rel in db_schema.get(\"Type Relationships\", []):\n",
    "            from_type = rel.get(\"from_type\", \"\")\n",
    "            to_type = rel.get(\"to_type\", \"\")\n",
    "            edge_type = rel.get(\"edge_type\", \"\")\n",
    "            key = f\"{from_type}_to_{to_type}\"\n",
    "            if key not in relationship_types:\n",
    "                relationship_types[key] = {\n",
    "                    \"from_type\": from_type,\n",
    "                    \"to_type\": to_type,\n",
    "                    \"edge_types\": []\n",
    "                }\n",
    "            if edge_type and edge_type not in relationship_types[key][\"edge_types\"]:\n",
    "                relationship_types[key][\"edge_types\"].append(edge_type)\n",
    "                \n",
    "        # Count files by language\n",
    "        languages = {}\n",
    "        for file_info in files.values():\n",
    "            language = file_info.get(\"language\", \"unknown\")\n",
    "            if language not in languages:\n",
    "                languages[language] = 0\n",
    "            languages[language] += 1\n",
    "            \n",
    "        # Build directory structure map for improved path navigation\n",
    "        directory_structure = build_directory_structure()\n",
    "            \n",
    "        return {\n",
    "            \"graph_name\": graph_name,\n",
    "            \"node_collection\": node_collection,\n",
    "            \"edge_collection\": edge_collection,\n",
    "            \"node_types\": node_types_info,\n",
    "            \"relationship_types\": list(relationship_types.values()),\n",
    "            \"file_count\": len(files),\n",
    "            \"snippet_count\": len(snippets),\n",
    "            \"symbol_count\": len(symbols),\n",
    "            \"languages\": languages,\n",
    "            \"directory_structure\": directory_structure\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting database structure: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_directory(path: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze a specific directory in the codebase\n",
    "    \n",
    "    Args:\n",
    "        path: Path to directory to analyze\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with directory analysis results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Analyzing code structure at path: {path}\")\n",
    "        \n",
    "        # Normalize path for consistent matching\n",
    "        normalized_path = path.rstrip('/')\n",
    "        \n",
    "        # First try direct path matching for directory nodes\n",
    "        print(f\"Looking for files with path pattern: {normalized_path}\")\n",
    "        \n",
    "        # Query files with matching path prefix\n",
    "        matching_files = []\n",
    "        for file_key, file_info in files.items():\n",
    "            file_path = file_info.get(\"file_path\", \"\")\n",
    "            if file_path and (file_path.startswith(f\"{normalized_path}/\") or file_path == normalized_path):\n",
    "                matching_files.append(file_info)\n",
    "        \n",
    "        # Sort files for consistent output\n",
    "        matching_files.sort(key=lambda x: x.get(\"file_path\", \"\"))\n",
    "        \n",
    "        # Print sample paths for debugging\n",
    "        print(\"Sample file paths in database:\")\n",
    "        for i, file_info in enumerate(list(files.values())[:6]):\n",
    "            print(f\"File {i+1}: {file_info.get('file_path', '')}\")\n",
    "        \n",
    "        # If no files found with direct path matching, try more flexible matching\n",
    "        if not matching_files:\n",
    "            # Try to find files that might contain the path (handle relative paths)\n",
    "            for file_key, file_info in files.items():\n",
    "                file_path = file_info.get(\"file_path\", \"\")\n",
    "                path_parts = normalized_path.split('/')\n",
    "                \n",
    "                # Check if all path parts appear in order in the file path\n",
    "                if file_path:\n",
    "                    file_parts = file_path.split('/')\n",
    "                    for i in range(len(file_parts) - len(path_parts) + 1):\n",
    "                        if file_parts[i:i+len(path_parts)] == path_parts:\n",
    "                            matching_files.append(file_info)\n",
    "                            break\n",
    "            \n",
    "            # Sort again after flexible matching\n",
    "            matching_files.sort(key=lambda x: x.get(\"file_path\", \"\"))\n",
    "        \n",
    "        # Get directory structure\n",
    "        directory_structure = get_directory_contents(normalized_path)\n",
    "        \n",
    "        # Get snippets for matching files\n",
    "        file_keys = [file_info.get(\"key\") for file_info in matching_files]\n",
    "        matching_snippets = []\n",
    "        for snippet_key, snippet_info in snippets.items():\n",
    "            if snippet_info.get(\"file_key\") in file_keys:\n",
    "                matching_snippets.append(snippet_info)\n",
    "        \n",
    "        # Get symbols for matching files\n",
    "        matching_symbols = []\n",
    "        for symbol_key, symbol_info in symbols.items():\n",
    "            if symbol_info.get(\"file_key\") in file_keys:\n",
    "                matching_symbols.append(symbol_info)\n",
    "        \n",
    "        return {\n",
    "            \"path\": normalized_path,\n",
    "            \"files\": matching_files,\n",
    "            \"file_count\": len(matching_files),\n",
    "            \"directory_structure\": directory_structure,\n",
    "            \"snippets_count\": len(matching_snippets),\n",
    "            \"symbols_count\": len(matching_symbols)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing directory: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directory_contents(path: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Get contents of a specific directory\n",
    "    \n",
    "    Args:\n",
    "        path: Path to directory\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with directory contents\n",
    "    \"\"\"\n",
    "    contents = {\"files\": [], \"subdirectories\": []}\n",
    "    \n",
    "    # Normalize path\n",
    "    normalized_path = path.rstrip('/')\n",
    "    \n",
    "    # Get files directly in this directory\n",
    "    for file_key, file_info in files.items():\n",
    "        file_path = file_info.get(\"file_path\", \"\")\n",
    "        if not file_path:\n",
    "            continue\n",
    "            \n",
    "        file_dir = '/'.join(file_path.split('/')[:-1])\n",
    "        \n",
    "        if file_dir == normalized_path:\n",
    "            contents[\"files\"].append({\n",
    "                \"key\": file_key,\n",
    "                \"name\": file_info.get(\"file_name\", \"\"),\n",
    "                \"path\": file_path,\n",
    "                \"language\": file_info.get(\"language\", \"\")\n",
    "            })\n",
    "    \n",
    "    # Get subdirectories\n",
    "    seen_subdirs = set()\n",
    "    for file_key, file_info in files.items():\n",
    "        file_path = file_info.get(\"file_path\", \"\")\n",
    "        if not file_path or not file_path.startswith(f\"{normalized_path}/\"):\n",
    "            continue\n",
    "            \n",
    "        # Get next directory level\n",
    "        remaining_path = file_path[len(normalized_path)+1:]\n",
    "        if '/' in remaining_path:\n",
    "            subdir = remaining_path.split('/')[0]\n",
    "            subdir_path = f\"{normalized_path}/{subdir}\"\n",
    "            \n",
    "            if subdir_path not in seen_subdirs:\n",
    "                seen_subdirs.add(subdir_path)\n",
    "                contents[\"subdirectories\"].append({\n",
    "                    \"name\": subdir,\n",
    "                    \"path\": subdir_path\n",
    "                })\n",
    "    \n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_code(term: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Search for code containing a specific term\n",
    "    \n",
    "    Args:\n",
    "        term: The term to search for\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries containing matching code snippets\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    try:\n",
    "        # Determine the best attribute for code based on the sample\n",
    "        code_field = 'code_snippet'\n",
    "        snippet_sample = node_types.get('snippet', {}).get('sample', {})\n",
    "        \n",
    "        if 'code_snippet' in snippet_sample:\n",
    "            code_field = 'code_snippet'\n",
    "        elif 'code' in snippet_sample:\n",
    "            code_field = 'code'\n",
    "        elif 'snippet' in snippet_sample:\n",
    "            code_field = 'snippet'\n",
    "        \n",
    "        aql = f\"\"\"\n",
    "        FOR snippet IN {node_collection}\n",
    "            FILTER snippet.type == 'snippet' AND snippet.{code_field} LIKE '%{term}%'\n",
    "            LET file = (\n",
    "                FOR edge IN {edge_collection}\n",
    "                    FILTER edge._to == snippet._id\n",
    "                    FOR file IN {node_collection}\n",
    "                        FILTER file._id == edge._from AND file.type == 'file'\n",
    "                        RETURN {{\n",
    "                            \"key\": file._key,\n",
    "                            \"directory\": file.directory,\n",
    "                            \"file_name\": file.file_name,\n",
    "                            \"file_path\": file.path || (file.directory + '/' + file.file_name),\n",
    "                            \"language\": file.language\n",
    "                        }}\n",
    "            )\n",
    "            RETURN {{\n",
    "                \"key\": snippet._key,\n",
    "                \"code\": snippet.{code_field},\n",
    "                \"start_line\": snippet.start_line,\n",
    "                \"end_line\": snippet.end_line,\n",
    "                \"file\": LENGTH(file) > 0 ? file[0] : null\n",
    "            }}\n",
    "        \"\"\"\n",
    "        cursor = db.aql.execute(aql)\n",
    "        for doc in cursor:\n",
    "            results.append(doc)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error searching code: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_code_structure(path: Optional[str] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze and visualize the structure of the code, either for a specific file or directory\n",
    "    \n",
    "    Args:\n",
    "        path: Optional path to focus the analysis on\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing code structure analysis\n",
    "    \"\"\"\n",
    "    print(f\"Analyzing code structure at path: {path}\")\n",
    "    # Print the query you're using to find files\n",
    "    print(f\"Looking for files with path pattern: {path}\")\n",
    "    # Print a few sample files from your cache for comparison\n",
    "    print(\"Sample file paths in database:\")\n",
    "    for i, (key, file_info) in enumerate(files.items()):\n",
    "        print(f\"File {i+1}: {file_info.get('file_path', 'unknown')}\")\n",
    "        if i >= 5:\n",
    "            break\n",
    "            \n",
    "    try:\n",
    "        # If path is provided, filter by that path\n",
    "        path_filter = \"\"\n",
    "        if path:\n",
    "            path_filter = f\" AND (file.path LIKE '{path}/%' OR file.path == '{path}')\"\n",
    "        \n",
    "        # First, gather file structure\n",
    "        aql = f\"\"\"\n",
    "        FOR file IN {node_collection}\n",
    "            FILTER file.type == 'file'{path_filter}\n",
    "            RETURN {{\n",
    "                \"key\": file._key,\n",
    "                \"file_path\": file.path || (file.directory + '/' + file.file_name),\n",
    "                \"language\": file.language\n",
    "            }}\n",
    "        \"\"\"\n",
    "        cursor = db.aql.execute(aql)\n",
    "        files_list = [doc for doc in cursor]\n",
    "        \n",
    "        # Group files by directory\n",
    "        directory_structure = {}\n",
    "        for file in files_list:\n",
    "            file_path = file.get(\"file_path\", \"\")\n",
    "            if not file_path:\n",
    "                continue\n",
    "            \n",
    "            # Split path and use all but the last part as directory\n",
    "            path_parts = file_path.split('/')\n",
    "            if len(path_parts) > 1:\n",
    "                directory = '/'.join(path_parts[:-1])\n",
    "                filename = path_parts[-1]\n",
    "            else:\n",
    "                directory = \".\"\n",
    "                filename = file_path\n",
    "            \n",
    "            if directory not in directory_structure:\n",
    "                directory_structure[directory] = []\n",
    "            \n",
    "            directory_structure[directory].append({\n",
    "                \"file_name\": filename,\n",
    "                \"file_path\": file_path,\n",
    "                \"key\": file.get(\"key\"),\n",
    "                \"language\": file.get(\"language\", \"unknown\")\n",
    "            })\n",
    "        \n",
    "        # Count symbols by type and file\n",
    "        symbol_counts = {}\n",
    "        if 'symbol' in node_types:\n",
    "            path_join = \"\"\n",
    "            if path:\n",
    "                path_join = f\" AND (file.path LIKE '{path}/%' OR file.path == '{path}')\"\n",
    "            \n",
    "            aql = f\"\"\"\n",
    "            FOR symbol IN {node_collection}\n",
    "                FILTER symbol.type == 'symbol'\n",
    "                LET file = (\n",
    "                    FOR edge IN {edge_collection}\n",
    "                        FILTER edge._to == symbol._id\n",
    "                        FOR file IN {node_collection}\n",
    "                            FILTER file._id == edge._from AND file.type == 'file'{path_join}\n",
    "                            RETURN file\n",
    "                )\n",
    "                FILTER LENGTH(file) > 0\n",
    "                COLLECT file_path = file[0].path || (file[0].directory + '/' + file[0].file_name),\n",
    "                        symbol_type = symbol.symbol_type WITH COUNT INTO count\n",
    "                RETURN {{\n",
    "                    \"file_path\": file_path,\n",
    "                    \"symbol_type\": symbol_type,\n",
    "                    \"count\": count\n",
    "                }}\n",
    "            \"\"\"\n",
    "            cursor = db.aql.execute(aql)\n",
    "            for doc in cursor:\n",
    "                file_path = doc.get(\"file_path\", \"\")\n",
    "                symbol_type = doc.get(\"symbol_type\", \"unknown\")\n",
    "                count = doc.get(\"count\", 0)\n",
    "                \n",
    "                if file_path not in symbol_counts:\n",
    "                    symbol_counts[file_path] = {}\n",
    "                \n",
    "                symbol_counts[file_path][symbol_type] = count\n",
    "        \n",
    "        # Prepare analysis data for LLM\n",
    "        file_count = len(files_list)\n",
    "        directory_count = len(directory_structure)\n",
    "        \n",
    "        # Prepare information for visualization\n",
    "        directory_tree = []\n",
    "        for directory, file_list in directory_structure.items():\n",
    "            directory_tree.append({\n",
    "                \"directory\": directory,\n",
    "                \"files\": file_list,\n",
    "                \"file_count\": len(file_list)\n",
    "            })\n",
    "        \n",
    "        # Sort directories by file count (descending)\n",
    "        directory_tree.sort(key=lambda x: x[\"file_count\"], reverse=True)\n",
    "        \n",
    "        # Analyze distribution of languages\n",
    "        language_counts = {}\n",
    "        for file in files_list:\n",
    "            language = file.get(\"language\", \"unknown\")\n",
    "            if language not in language_counts:\n",
    "                language_counts[language] = 0\n",
    "            language_counts[language] += 1\n",
    "        \n",
    "        # Create an analysis with Mistral\n",
    "        if files_list:\n",
    "            structure_info = {\n",
    "                \"file_count\": file_count,\n",
    "                \"directory_count\": directory_count,\n",
    "                \"top_directories\": [d[\"directory\"] for d in directory_tree[:5]],\n",
    "                \"language_distribution\": language_counts,\n",
    "                \"symbol_type_distribution\": symbol_counts\n",
    "            }\n",
    "            \n",
    "            # Create a prompt for the LLM to analyze the structure\n",
    "            prompt = f\"\"\"\n",
    "            Please analyze this codebase structure:\n",
    "            \n",
    "            {json.dumps(structure_info, indent=2)}\n",
    "            \n",
    "            Provide a JSON response with the following fields:\n",
    "            1. overview: High-level description of the codebase structure\n",
    "            2. architecture_patterns: Any architectural patterns you can identify\n",
    "            3. key_components: The most important directories/modules\n",
    "            4. language_insights: Analysis of the programming language usage\n",
    "            5. recommendations: Suggestions for organization or structure improvements\n",
    "            \n",
    "            Format your response as a valid JSON object without any extra text or markdown.\n",
    "            \"\"\"\n",
    "            \n",
    "            # Create message for the LLM\n",
    "            messages = [\n",
    "                ChatMessage(role=\"user\", content=prompt)\n",
    "            ]\n",
    "            \n",
    "            # Get completion from Mistral\n",
    "            chat_response = mistral_client.chat(\n",
    "                model=model,\n",
    "                messages=messages\n",
    "            )\n",
    "            \n",
    "            # Extract the content from the response\n",
    "            content = chat_response.choices[0].message.content\n",
    "            \n",
    "            # Try to parse the response as JSON\n",
    "            try:\n",
    "                analysis = json.loads(content)\n",
    "            except json.JSONDecodeError:\n",
    "                # If JSON parsing fails, return the raw text\n",
    "                analysis = {\"raw_analysis\": content}\n",
    "        else:\n",
    "            analysis = {\"message\": \"No files found matching the specified path\"}\n",
    "        \n",
    "        return {\n",
    "            \"path\": path or \"entire codebase\",\n",
    "            \"file_count\": file_count,\n",
    "            \"directory_count\": directory_count,\n",
    "            \"directory_structure\": directory_tree,\n",
    "            \"language_distribution\": language_counts,\n",
    "            \"symbol_distribution\": symbol_counts,\n",
    "            \"analysis\": analysis\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing code structure: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(query: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Process natural language queries about the codebase\n",
    "    \n",
    "    Args:\n",
    "        query: Natural language query about the codebase\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing the response to the query\n",
    "    \"\"\"\n",
    "    global conversation_history, mistral_client, model\n",
    "    \n",
    "    try:\n",
    "        # Save the query to conversation history\n",
    "        conversation_history.append({\"role\": \"user\", \"content\": query})\n",
    "        \n",
    "        # Get database structure for context\n",
    "        db_structure = get_database_structure()\n",
    "        \n",
    "        # Create context for the LLM\n",
    "        context = {\n",
    "            \"db_structure\": db_structure,\n",
    "            \"conversation_history\": conversation_history[-5:] if len(conversation_history) > 1 else []\n",
    "        }\n",
    "        \n",
    "        # Create a prompt for the LLM to analyze the query and decide what action to take\n",
    "        prompt = f\"\"\"\n",
    "        You are a codebase assistant that helps users find information in their codebase.\n",
    "        \n",
    "        Database Structure:\n",
    "        {json.dumps(db_structure, indent=2)}\n",
    "        \n",
    "        Available functions:\n",
    "        1. find_symbol_occurrences(symbol_name): Find all occurrences of a symbol\n",
    "        2. find_by_name(name, symbol_type): Find function/class snippets by name\n",
    "        3. analyze_symbol(name, symbol_type): Get detailed analysis of a function/class\n",
    "        4. analyze_error(error_message): Analyze an error message and suggest solutions\n",
    "        5. search_code(term): Search for code containing specific terms\n",
    "        6. analyze_code_structure(path): Analyze the structure of the code\n",
    "        7. analyze_directory(path): Analyze a specific directory in the codebase\n",
    "        \n",
    "        Conversation History:\n",
    "        {json.dumps(context[\"conversation_history\"], indent=2)}\n",
    "        \n",
    "        User Query: {query}\n",
    "        \n",
    "        First, determine what the user is asking and which function would be most appropriate to answer their query.\n",
    "        \n",
    "        Return a JSON response with:\n",
    "        1. understanding: Brief explanation of what you think the user is asking\n",
    "        2. function_to_call: The most appropriate function to call based on the query\n",
    "        3. parameters: Parameters to pass to the function\n",
    "        \n",
    "        Format your response as a valid JSON object without any extra text or markdown.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create message for the LLM\n",
    "        messages = [\n",
    "            ChatMessage(role=\"user\", content=prompt)\n",
    "        ]\n",
    "        \n",
    "        # Get completion from Mistral\n",
    "        chat_response = mistral_client.chat(\n",
    "            model=model,\n",
    "            messages=messages\n",
    "        )\n",
    "        \n",
    "        # Extract the content from the response\n",
    "        content = chat_response.choices[0].message.content\n",
    "        \n",
    "        # Try to parse the response as JSON\n",
    "        try:\n",
    "            # Clean up the content to remove markdown code blocks if present\n",
    "            cleaned_content = content\n",
    "            if content.strip().startswith(\"```\") and content.strip().endswith(\"```\"):\n",
    "                # Extract the content between the backticks\n",
    "                cleaned_content = \"\\n\".join(content.strip().split(\"\\n\")[1:-1])\n",
    "            query_analysis = json.loads(cleaned_content)\n",
    "        except json.JSONDecodeError:\n",
    "            return {\"error\": \"Failed to parse LLM response as JSON\", \"raw_response\": content}\n",
    "        \n",
    "        # Get the function to call and parameters\n",
    "        function_name = query_analysis.get(\"function_to_call\", \"\")\n",
    "        parameters = query_analysis.get(\"parameters\", {})\n",
    "        \n",
    "        # Call the appropriate function based on the analysis\n",
    "        result = None\n",
    "        if function_name == \"find_symbol_occurrences\":\n",
    "            symbol_name = parameters.get(\"symbol_name\", \"\")\n",
    "            if symbol_name:\n",
    "                result = find_symbol_occurrences(symbol_name)\n",
    "        elif function_name == \"find_by_name\":\n",
    "            name = parameters.get(\"name\", \"\")\n",
    "            symbol_type = parameters.get(\"symbol_type\")\n",
    "            if name:\n",
    "                result = find_by_name(name, symbol_type)\n",
    "        elif function_name == \"analyze_symbol\":\n",
    "            name = parameters.get(\"name\", \"\")\n",
    "            symbol_type = parameters.get(\"symbol_type\")\n",
    "            if name:\n",
    "                result = analyze_symbol(name, symbol_type)\n",
    "        elif function_name == \"analyze_error\":\n",
    "            error_message = parameters.get(\"error_message\", \"\")\n",
    "            if error_message:\n",
    "                result = analyze_error(error_message)\n",
    "        elif function_name == \"search_code\":\n",
    "            term = parameters.get(\"term\", \"\")\n",
    "            if term:\n",
    "                result = search_code(term)\n",
    "        elif function_name == \"analyze_code_structure\":\n",
    "            path = parameters.get(\"path\")\n",
    "            result = analyze_code_structure(path)\n",
    "        elif function_name == \"analyze_directory\":\n",
    "            path = parameters.get(\"path\", \"\")\n",
    "            if path:\n",
    "                result = analyze_directory(path)\n",
    "        else:\n",
    "            result = {\"error\": f\"Unknown function: {function_name}\"}\n",
    "        \n",
    "        # If result is None or empty, try to handle the query directly\n",
    "        if result is None or (isinstance(result, list) and len(result) == 0):\n",
    "            # Create a fallback prompt for the LLM\n",
    "            fallback_prompt = f\"\"\"\n",
    "            You are a codebase assistant that helps users find information in their codebase.\n",
    "            \n",
    "            Database Structure:\n",
    "            {json.dumps(db_structure, indent=2)}\n",
    "            \n",
    "            Unfortunately, I couldn't find specific information to answer the user's query:\n",
    "            \n",
    "            User Query: {query}\n",
    "            \n",
    "            Please provide a helpful response based on the general codebase structure.\n",
    "            Your response should:\n",
    "            1. Acknowledge what information is missing\n",
    "            2. Suggest alternative approaches based on the available database structure\n",
    "            3. Ask for any clarification if needed\n",
    "            \n",
    "            Format your response as a conversation, not as JSON.\n",
    "            \"\"\"\n",
    "            \n",
    "            # Create message for the LLM\n",
    "            fallback_messages = [\n",
    "                ChatMessage(role=\"user\", content=fallback_prompt)\n",
    "            ]\n",
    "            \n",
    "            # Get completion from Mistral\n",
    "            fallback_response = mistral_client.chat(\n",
    "                model=model,\n",
    "                messages=fallback_messages\n",
    "            )\n",
    "            \n",
    "            # Extract the content from the response\n",
    "            fallback_content = fallback_response.choices[0].message.content\n",
    "            \n",
    "            # Add the fallback response to conversation history\n",
    "            conversation_history.append({\"role\": \"assistant\", \"content\": fallback_content})\n",
    "            \n",
    "            return {\n",
    "                \"query\": query,\n",
    "                \"understanding\": query_analysis.get(\"understanding\", \"\"),\n",
    "                \"response_type\": \"fallback\",\n",
    "                \"response\": fallback_content\n",
    "            }\n",
    "        \n",
    "        # Generate a user-friendly explanation of the result\n",
    "        explanation_prompt = f\"\"\"\n",
    "        You are a codebase assistant that helps users find information in their codebase.\n",
    "        \n",
    "        User Query: {query}\n",
    "        \n",
    "        Understanding: {query_analysis.get(\"understanding\", \"\")}\n",
    "        \n",
    "        Result: {json.dumps(result, indent=2)}\n",
    "        \n",
    "        Please explain these results to the user in a clear, conversational way.\n",
    "        If results include code snippets, explain what the code does.\n",
    "        If there are multiple results, summarize the key findings.\n",
    "        Include specific details from the results to make your explanation concrete.\n",
    "        \n",
    "        Format your response as a conversation, not as JSON.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create message for the LLM\n",
    "        explanation_messages = [\n",
    "            ChatMessage(role=\"user\", content=explanation_prompt)\n",
    "        ]\n",
    "        \n",
    "        # Get completion from Mistral\n",
    "        explanation_response = mistral_client.chat(\n",
    "            model=model,\n",
    "            messages=explanation_messages\n",
    "        )\n",
    "        \n",
    "        # Extract the content from the response\n",
    "        explanation = explanation_response.choices[0].message.content\n",
    "        \n",
    "        # Add the explanation to conversation history\n",
    "        conversation_history.append({\"role\": \"assistant\", \"content\": explanation})\n",
    "        \n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"understanding\": query_analysis.get(\"understanding\", \"\"),\n",
    "            \"function_called\": function_name,\n",
    "            \"parameters\": parameters,\n",
    "            \"raw_result\": result,\n",
    "            \"explanation\": explanation\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing query: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return {\"error\": str(e)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_codebase(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Main conversational function that processes user queries about the codebase\n",
    "    \n",
    "    Args:\n",
    "        query: User's natural language query\n",
    "        \n",
    "    Returns:\n",
    "        String containing the response to the user\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Process the query\n",
    "        result = process_query(query)\n",
    "        \n",
    "        # If an error occurred, return an error message\n",
    "        if \"error\" in result:\n",
    "            error_message = result.get(\"error\", \"An unknown error occurred\")\n",
    "            if \"raw_response\" in result:\n",
    "                return f\"I encountered an error: {error_message}\\n\\nRaw response from LLM: {result['raw_response']}\"\n",
    "            return f\"I encountered an error: {error_message}\"\n",
    "        \n",
    "        # If the result contains an explanation, return it\n",
    "        if \"explanation\" in result:\n",
    "            return result[\"explanation\"]\n",
    "        \n",
    "        # If the result contains a response, return it\n",
    "        if \"response\" in result:\n",
    "            return result[\"response\"]\n",
    "        \n",
    "        # This is a fallback if neither explanation nor response are available\n",
    "        return \"I processed your query but couldn't generate a proper explanation. Please try rephrasing your question.\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in chat_with_codebase: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return f\"I'm sorry, I encountered an error while processing your query: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_conversation():\n",
    "    \"\"\"Reset the conversation history\"\"\"\n",
    "    global conversation_history\n",
    "    conversation_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_codebase_query(\n",
    "    db_name: str = \"_system\",\n",
    "    username: str = \"root\",\n",
    "    password: str = None,\n",
    "    host: str = None,\n",
    "    mistral_api_key: Optional[str] = None,\n",
    "    model_name: str = \"mistral-large-latest\",\n",
    "    graph: str = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Initialize the codebase query system that dynamically discovers the graph structure.\n",
    "    \n",
    "    Args:\n",
    "        db_name: ArangoDB database name\n",
    "        username: ArangoDB username\n",
    "        password: ArangoDB password\n",
    "        host: ArangoDB host URL\n",
    "        mistral_api_key: Mistral API key (if None, will try to get from environment)\n",
    "        model_name: Mistral model to use\n",
    "        graph: Graph name (if None, will try to discover the first available graph)\n",
    "    \"\"\"\n",
    "    global db, client, mistral_client, model, graph_name, files, snippets, symbols\n",
    "    global db_schema, node_types, symbol_name_index, file_to_snippets, file_to_symbols, snippet_to_symbols\n",
    "    global conversation_history, node_collection, edge_collection\n",
    "    \n",
    "    # Connect to ArangoDB\n",
    "    if not host:\n",
    "        host = os.environ.get(\"ARANGO_HOST\", \"http://localhost:8529\")\n",
    "    client = ArangoClient(hosts=host)\n",
    "    \n",
    "    if not password:\n",
    "        password = os.environ.get(\"ARANGO_PASSWORD\")\n",
    "        if not password:\n",
    "            raise ValueError(\"ArangoDB password not provided and not found in environment\")\n",
    "    \n",
    "    db = client.db(db_name, username=username, password=password)\n",
    "    \n",
    "    # Connect to Mistral API\n",
    "    if mistral_api_key is None:\n",
    "        mistral_api_key = os.environ.get(\"MISTRAL_API_KEY\")\n",
    "    if mistral_api_key is None:\n",
    "        raise ValueError(\"Mistral API key not provided and not found in environment\")\n",
    "    \n",
    "    # Initialize Mistral client\n",
    "    mistral_client = MistralClient(api_key=mistral_api_key)\n",
    "    model = model_name\n",
    "    \n",
    "    # Dynamically discover graph structure\n",
    "    graph_name = graph\n",
    "    \n",
    "    # Initialize data structures\n",
    "    files = {}\n",
    "    snippets = {}\n",
    "    symbols = {}\n",
    "    symbol_name_index = {}\n",
    "    file_to_snippets = {}\n",
    "    file_to_symbols = {}\n",
    "    snippet_to_symbols = {}\n",
    "    conversation_history = []\n",
    "    \n",
    "    # Discover graph structure\n",
    "    discover_graph_structure()\n",
    "    \n",
    "    # Get schema information\n",
    "    db_schema = get_db_schema()\n",
    "    \n",
    "    # Analyze node types\n",
    "    node_types = analyze_node_types()\n",
    "    \n",
    "    # Initialize cache\n",
    "    initialize_cache()\n",
    "    \n",
    "    print(\"Codebase query system initialized successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No edge definitions found, using default naming pattern\n",
      "Using default collections: Nodes=FlaskRepv1_node, Edges=FlaskRepv1_node_to_FlaskRepv1_node\n",
      "Found type field: type\n",
      "Found edge type field: edge_type\n",
      "Schema validation complete: type_field=type, path_field=None, edge_type_field=edge_type\n",
      "Type: directory, Count: 69\n",
      "Type: file, Count: 83\n",
      "Type: snippet, Count: 930\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[143], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFlaskRepv1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Initialize the system\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[43minitialize_codebase_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdb_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mARANGO_DB_NAME\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43musername\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mARANGO_USERNAME\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mARANGO_PASSWORD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mARANGO_HOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmistral_api_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMISTRAL_API_KEY\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMISTRAL_MODEL_NAME\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Ask a question about the codebase\u001b[39;00m\n\u001b[1;32m     22\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShow me all functions related to error handling\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[117], line 68\u001b[0m, in \u001b[0;36minitialize_codebase_query\u001b[0;34m(db_name, username, password, host, mistral_api_key, model_name, graph)\u001b[0m\n\u001b[1;32m     65\u001b[0m db_schema \u001b[38;5;241m=\u001b[39m get_db_schema()\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Analyze node types\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m node_types \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_node_types\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Initialize cache\u001b[39;00m\n\u001b[1;32m     71\u001b[0m initialize_cache()\n",
      "Cell \u001b[0;32mIn[122], line 39\u001b[0m, in \u001b[0;36manalyze_node_types\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Get a sample for this node type\u001b[39;00m\n\u001b[1;32m     33\u001b[0m aql \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124mFOR v IN \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode_collection\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124m    FILTER v.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtype_field\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m == \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124m    LIMIT 1\u001b[39m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124m    RETURN v\u001b[39m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 39\u001b[0m cursor \u001b[38;5;241m=\u001b[39m \u001b[43mdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43maql\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m samples \u001b[38;5;241m=\u001b[39m [doc \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m cursor]\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m samples:\n",
      "File \u001b[0;32m~/GitHub/scopium/.venv/lib/python3.10/site-packages/arango/aql.py:492\u001b[0m, in \u001b[0;36mAQL.execute\u001b[0;34m(self, query, count, batch_size, ttl, bind_vars, full_count, max_plans, optimizer_rules, cache, memory_limit, fail_on_warning, profile, max_transaction_size, max_warning_count, intermediate_commit_count, intermediate_commit_size, satellite_sync_wait, stream, skip_inaccessible_cols, max_runtime, fill_block_cache, allow_dirty_read, allow_retry, force_one_shard_attribute_value, use_plan_cache)\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m AQLQueryExecuteError(resp, request)\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Cursor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conn, resp\u001b[38;5;241m.\u001b[39mbody, allow_retry\u001b[38;5;241m=\u001b[39mallow_retry)\n\u001b[0;32m--> 492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_handler\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GitHub/scopium/.venv/lib/python3.10/site-packages/arango/api.py:74\u001b[0m, in \u001b[0;36mApiGroup._execute\u001b[0;34m(self, request, response_handler)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_execute\u001b[39m(\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28mself\u001b[39m, request: Request, response_handler: Callable[[Response], T]\n\u001b[1;32m     65\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Result[T]:\n\u001b[1;32m     66\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Execute an API.\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m    :param request: HTTP request.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m    :return: API execution result.\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_handler\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GitHub/scopium/.venv/lib/python3.10/site-packages/arango/executor.py:66\u001b[0m, in \u001b[0;36mDefaultApiExecutor.execute\u001b[0;34m(self, request, response_handler)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mexecute\u001b[39m(\u001b[38;5;28mself\u001b[39m, request: Request, response_handler: Callable[[Response], T]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m     58\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Execute an API request and return the result.\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m    :param request: HTTP request.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m    :return: API execution result.\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response_handler(resp)\n",
      "File \u001b[0;32m~/GitHub/scopium/.venv/lib/python3.10/site-packages/arango/connection.py:311\u001b[0m, in \u001b[0;36mBasicConnection.send_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Send an HTTP request to ArangoDB server.\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \n\u001b[1;32m    305\u001b[0m \u001b[38;5;124;03m:param request: HTTP request.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;124;03m:rtype: arango.response.Response\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    310\u001b[0m host_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_host_resolver\u001b[38;5;241m.\u001b[39mget_host_index()\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_auth\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GitHub/scopium/.venv/lib/python3.10/site-packages/arango/connection.py:156\u001b[0m, in \u001b[0;36mBaseConnection.process_request\u001b[0;34m(self, host_index, request, auth)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m tries \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_host_resolver\u001b[38;5;241m.\u001b[39mmax_tries:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 156\u001b[0m         resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_http\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m            \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sessions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mhost_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_url_prefixes\u001b[49m\u001b[43m[\u001b[49m\u001b[43mhost_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m            \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_response(resp, request\u001b[38;5;241m.\u001b[39mdeserialize)\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "File \u001b[0;32m~/GitHub/scopium/.venv/lib/python3.10/site-packages/arango/http.py:230\u001b[0m, in \u001b[0;36mDefaultHTTPClient.send_request\u001b[0;34m(self, session, method, url, headers, params, data, auth)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msend_request\u001b[39m(\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    203\u001b[0m     session: Session,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    209\u001b[0m     auth: Optional[Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    210\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Response:\n\u001b[1;32m    211\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Send an HTTP request.\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \n\u001b[1;32m    213\u001b[0m \u001b[38;5;124;03m    :param session: Requests session object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03m    :rtype: arango.response.Response\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 230\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    240\u001b[0m         method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m    241\u001b[0m         url\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    245\u001b[0m         raw_body\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mtext,\n\u001b[1;32m    246\u001b[0m     )\n",
      "File \u001b[0;32m~/GitHub/scopium/.venv/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/GitHub/scopium/.venv/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/GitHub/scopium/.venv/lib/python3.10/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/GitHub/scopium/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/GitHub/scopium/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/GitHub/scopium/.venv/lib/python3.10/site-packages/urllib3/connection.py:516\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    513\u001b[0m _shutdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    519\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/socket.py:717\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 717\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    718\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    719\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py:1307\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1304\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1305\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1306\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py:1163\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Example usage in a Jupyter notebook\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "graph = \"FlaskRepv1\"\n",
    "\n",
    "# Initialize the system\n",
    "initialize_codebase_query(\n",
    "    db_name=os.getenv(\"ARANGO_DB_NAME\"),\n",
    "    username=os.getenv(\"ARANGO_USERNAME\"),\n",
    "    password=os.getenv(\"ARANGO_PASSWORD\"),\n",
    "    host=os.getenv(\"ARANGO_HOST\"),\n",
    "    mistral_api_key=os.getenv(\"MISTRAL_API_KEY\"),\n",
    "    model_name=os.getenv(\"MISTRAL_MODEL_NAME\"),\n",
    "    graph=graph\n",
    ")\n",
    "\n",
    "# Ask a question about the codebase\n",
    "query = \"Show me all functions related to error handling\"\n",
    "response = chat_with_codebase(query)\n",
    "print(response)\n",
    "\n",
    "# Reset the conversation if needed\n",
    "reset_conversation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
