{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scopium : Expanding the scope of your codebase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features of Scopium:\n",
    "- Automatic chunking and storing of the entire codebase based on relationship between files(imports, directory levels, symbol types)  \n",
    "- Efficient retrieving system performed with a hybrid approach in mind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset - The codebase needed by the user\n",
    "- When the root directory of the codebase is given as the argument, it converts it to a networkx graph capturing all the mentioned relationship between codes. \n",
    "- This is then loaded to an arangoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step0 - Installs and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nx-arangodb\n",
    "!pip install nx-cugraph-cu12 --extra-index-url https://pypi.nvidia.com # Requires CUDA-capable GPU\n",
    "!pip install --upgrade langchain langchain-community langchain-openai langgraph langchain_mistralai\n",
    "!pip install networkx==3.4\n",
    "!pip install tree-sitter\n",
    "!git clone https://github.com/tree-sitter/tree-sitter-cpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import ast\n",
    "from typing import Dict, Set, List, Tuple, Optional,Any\n",
    "import json\n",
    "from arango import ArangoClient\n",
    "import nx_arangodb as nxadb\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.graphs import ArangoGraph\n",
    "from langchain_community.chains.graph_qa.arangodb import ArangoGraphQAChain\n",
    "from langchain_core.tools import tool\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the graph - \n",
    "- This code takes the root directory of the codebase as the input. \n",
    "- It then builds the graph based on the mentioned features and storing it in appropriate nodes and edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global data structures\n",
    "def initialize_data_structures():\n",
    "    data = {\n",
    "        'root_dir': '',\n",
    "        'graph': nx.DiGraph(),\n",
    "        'file_contents': {},  # file -> content\n",
    "        'import_relations': {},  # file -> [(module, line_no)]\n",
    "        'module_symbols': {},  # file -> {symbol -> {type, line_no, context}}\n",
    "        'symbol_references': {},  # symbol -> [(file, line_no, context)]\n",
    "        'file_index': {},  # Maps files to indices\n",
    "        'current_index': 0,\n",
    "        'directories': set(),\n",
    "        'symbol_index': {},  # symbol -> [{file, type, line_no, context}]\n",
    "        'supported_languages': [\"python\", \"cpp\", \"java\", \"go\"],\n",
    "        'language_extensions': {\n",
    "            \"python\": [\".py\"],\n",
    "            \"cpp\": [\".c\", \".cpp\", \".h\", \".hpp\", \".cc\", \".cxx\", \".hxx\"],\n",
    "            \"java\": [\".java\"],\n",
    "            \"go\": [\".go\"]\n",
    "        }\n",
    "    }\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_index(data):\n",
    "    \"\"\"Get next available index for file indexing.\"\"\"\n",
    "    data['current_index'] += 1\n",
    "    return data['current_index']\n",
    "\n",
    "def chunk_code(code, lines_per_chunk=20):\n",
    "    \"\"\"\n",
    "    Chunk the given code into snippets.\n",
    "    Returns a list of dictionaries with 'code_snippet', 'start_line', and 'end_line'.\n",
    "    \"\"\"\n",
    "    lines = code.splitlines()\n",
    "    chunks = []\n",
    "    for i in range(0, len(lines), lines_per_chunk):\n",
    "        chunk_lines = lines[i:i + lines_per_chunk]\n",
    "        chunk = {\n",
    "            'code_snippet': '\\n'.join(chunk_lines),\n",
    "            'start_line': i + 1,\n",
    "            'end_line': i + len(chunk_lines)\n",
    "        }\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "def get_context_around_line(data, file_path, line_no, context_lines=3):\n",
    "    \"\"\"Extract context around a specific line in a file.\"\"\"\n",
    "    if file_path not in data['file_contents']:\n",
    "        return \"\"\n",
    "    \n",
    "    lines = data['file_contents'][file_path].splitlines()\n",
    "    start = max(0, line_no - context_lines - 1)\n",
    "    end = min(len(lines), line_no + context_lines)\n",
    "    \n",
    "    context = \"\\n\".join(lines[start:end])\n",
    "    return context\n",
    "\n",
    "def detect_language(data, file_path):\n",
    "    \"\"\"Detect the programming language of a file based on its extension.\"\"\"\n",
    "    _, ext = os.path.splitext(file_path)\n",
    "    ext = ext.lower()\n",
    "    \n",
    "    for language, extensions in data['language_extensions'].items():\n",
    "        if ext in extensions:\n",
    "            return language\n",
    "            \n",
    "    return \"unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_python_node_source(source, node):\n",
    "    \"\"\"Extract the source code for a Python AST node.\"\"\"\n",
    "    try:\n",
    "        lines = source.splitlines()\n",
    "        if hasattr(node, 'lineno') and hasattr(node, 'end_lineno'):\n",
    "            start = node.lineno - 1\n",
    "            end = getattr(node, 'end_lineno', start + 1)\n",
    "            return '\\n'.join(lines[start:end])\n",
    "        return \"\"\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def analyze_python_file(data, file_path, content):\n",
    "    \"\"\"Analyze a Python file for imports and symbols.\"\"\"\n",
    "    try:\n",
    "        tree = ast.parse(content)\n",
    "        imports = []\n",
    "        symbols = {}\n",
    "\n",
    "        for node in ast.walk(tree):\n",
    "            # Track imports\n",
    "            if isinstance(node, (ast.Import, ast.ImportFrom)):\n",
    "                if isinstance(node, ast.Import):\n",
    "                    for name in node.names:\n",
    "                        imports.append((name.name, node.lineno))\n",
    "                else:  # ImportFrom\n",
    "                    module = node.module if node.module else ''\n",
    "                    for name in node.names:\n",
    "                        imports.append((f\"{module}.{name.name}\" if module else name.name, node.lineno))\n",
    "\n",
    "            # Track defined symbols with line numbers and context\n",
    "            elif isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.Assign)):\n",
    "                if isinstance(node, (ast.FunctionDef, ast.ClassDef)):\n",
    "                    symbol_name = node.name\n",
    "                    symbol_type = 'class' if isinstance(node, ast.ClassDef) else 'function'\n",
    "                    line_no = node.lineno\n",
    "                    context = extract_python_node_source(content, node)\n",
    "                    \n",
    "                    symbols[symbol_name] = {\n",
    "                        'type': symbol_type,\n",
    "                        'line_no': line_no,\n",
    "                        'context': context,\n",
    "                        'docstring': ast.get_docstring(node)\n",
    "                    }\n",
    "                elif isinstance(node, ast.Assign):\n",
    "                    # Handle variable assignments\n",
    "                    for target in node.targets:\n",
    "                        if isinstance(target, ast.Name):\n",
    "                            symbol_name = target.id\n",
    "                            line_no = node.lineno\n",
    "                            context = extract_python_node_source(content, node)\n",
    "                            \n",
    "                            symbols[symbol_name] = {\n",
    "                                'type': 'variable',\n",
    "                                'line_no': line_no,\n",
    "                                'context': context\n",
    "                            }\n",
    "\n",
    "        data['import_relations'][file_path] = imports\n",
    "        data['module_symbols'][file_path] = symbols\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing Python file {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_cpp_file(data, file_path, content):\n",
    "    \"\"\"Analyze a C/C++ file for includes and symbols.\"\"\"\n",
    "    imports = []\n",
    "    symbols = {}\n",
    "    \n",
    "    # Process content line by line\n",
    "    lines = content.splitlines()\n",
    "    \n",
    "    # Regular expressions for C/C++ code analysis\n",
    "    include_pattern = re.compile(r'#include\\s+[<\"]([^>\"]+)[>\"]')\n",
    "    class_pattern = re.compile(r'(?:class|struct)\\s+(\\w+)')\n",
    "    function_pattern = re.compile(r'(\\w+)\\s*\\([^)]*\\)\\s*(?:const|override|final|noexcept)?\\s*(?:{|;)')\n",
    "    namespace_pattern = re.compile(r'namespace\\s+(\\w+)')\n",
    "    \n",
    "    for line_no, line in enumerate(lines, 1):\n",
    "        # Find include statements\n",
    "        include_match = include_pattern.search(line)\n",
    "        if include_match:\n",
    "            imports.append((include_match.group(1), line_no))\n",
    "        \n",
    "        # Find class/struct definitions\n",
    "        class_match = class_pattern.search(line)\n",
    "        if class_match:\n",
    "            class_name = class_match.group(1)\n",
    "            context = get_context_around_line(data, file_path, line_no)\n",
    "            symbols[class_name] = {\n",
    "                'type': 'class',\n",
    "                'line_no': line_no,\n",
    "                'context': context\n",
    "            }\n",
    "        \n",
    "        # Find function definitions (simplified)\n",
    "        function_match = function_pattern.search(line)\n",
    "        if function_match and not line.strip().startswith('#') and not line.strip().startswith('//'):\n",
    "            function_name = function_match.group(1)\n",
    "            # Skip some common keywords that might be mistaken for functions\n",
    "            if function_name not in ['if', 'while', 'for', 'switch', 'return']:\n",
    "                context = get_context_around_line(data, file_path, line_no)\n",
    "                symbols[function_name] = {\n",
    "                    'type': 'function',\n",
    "                    'line_no': line_no,\n",
    "                    'context': context\n",
    "                }\n",
    "        \n",
    "        # Find namespace definitions\n",
    "        namespace_match = namespace_pattern.search(line)\n",
    "        if namespace_match:\n",
    "            namespace_name = namespace_match.group(1)\n",
    "            context = get_context_around_line(data, file_path, line_no)\n",
    "            symbols[namespace_name] = {\n",
    "                'type': 'namespace',\n",
    "                'line_no': line_no,\n",
    "                'context': context\n",
    "            }\n",
    "    \n",
    "    data['import_relations'][file_path] = imports\n",
    "    data['module_symbols'][file_path] = symbols\n",
    "\n",
    "def analyze_java_file(data, file_path, content):\n",
    "    \"\"\"Analyze a Java file for imports and symbols.\"\"\"\n",
    "    imports = []\n",
    "    symbols = {}\n",
    "    \n",
    "    # Process content line by line\n",
    "    lines = content.splitlines()\n",
    "    \n",
    "    # Regular expressions for Java code analysis\n",
    "    package_pattern = re.compile(r'package\\s+([\\w.]+)')\n",
    "    import_pattern = re.compile(r'import\\s+([\\w.]+(?:\\.\\*)?)')\n",
    "    class_pattern = re.compile(r'(?:public|private|protected)?\\s*(?:abstract|final)?\\s*class\\s+(\\w+)')\n",
    "    interface_pattern = re.compile(r'(?:public|private|protected)?\\s*interface\\s+(\\w+)')\n",
    "    method_pattern = re.compile(r'(?:public|private|protected)?\\s*(?:static|final|abstract)?\\s*(?:[\\w<>[\\],\\s]+)\\s+(\\w+)\\s*\\([^)]*\\)')\n",
    "    \n",
    "    for line_no, line in enumerate(lines, 1):\n",
    "        # Find package declaration\n",
    "        package_match = package_pattern.search(line)\n",
    "        if package_match:\n",
    "            package_name = package_match.group(1)\n",
    "            imports.append((package_name, line_no))\n",
    "        \n",
    "        # Find import statements\n",
    "        import_match = import_pattern.search(line)\n",
    "        if import_match:\n",
    "            import_name = import_match.group(1)\n",
    "            imports.append((import_name, line_no))\n",
    "        \n",
    "        # Find class definitions\n",
    "        class_match = class_pattern.search(line)\n",
    "        if class_match:\n",
    "            class_name = class_match.group(1)\n",
    "            context = get_context_around_line(data, file_path, line_no)\n",
    "            symbols[class_name] = {\n",
    "                'type': 'class',\n",
    "                'line_no': line_no,\n",
    "                'context': context\n",
    "            }\n",
    "        \n",
    "        # Find interface definitions\n",
    "        interface_match = interface_pattern.search(line)\n",
    "        if interface_match:\n",
    "            interface_name = interface_match.group(1)\n",
    "            context = get_context_around_line(data, file_path, line_no)\n",
    "            symbols[interface_name] = {\n",
    "                'type': 'interface',\n",
    "                'line_no': line_no,\n",
    "                'context': context\n",
    "            }\n",
    "        \n",
    "        # Find method definitions\n",
    "        method_match = method_pattern.search(line)\n",
    "        if method_match:\n",
    "            method_name = method_match.group(1)\n",
    "            # Skip some common keywords that might be mistaken for methods\n",
    "            if method_name not in ['if', 'while', 'for', 'switch', 'return']:\n",
    "                context = get_context_around_line(data, file_path, line_no)\n",
    "                symbols[method_name] = {\n",
    "                    'type': 'method',\n",
    "                    'line_no': line_no,\n",
    "                    'context': context\n",
    "                }\n",
    "    \n",
    "    data['import_relations'][file_path] = imports\n",
    "    data['module_symbols'][file_path] = symbols\n",
    "\n",
    "def analyze_go_file(data, file_path, content):\n",
    "    \"\"\"Analyze a Go file for imports and symbols.\"\"\"\n",
    "    imports = []\n",
    "    symbols = {}\n",
    "    \n",
    "    # Process content line by line\n",
    "    lines = content.splitlines()\n",
    "    \n",
    "    # Regular expressions for Go code analysis\n",
    "    package_pattern = re.compile(r'package\\s+(\\w+)')\n",
    "    import_single_pattern = re.compile(r'import\\s+\"([^\"]+)\"')\n",
    "    import_multi_start_pattern = re.compile(r'import\\s+\\(')\n",
    "    import_multi_line_pattern = re.compile(r'\\s*\"([^\"]+)\"')\n",
    "    func_pattern = re.compile(r'func\\s+(?:\\([^)]+\\)\\s+)?(\\w+)')\n",
    "    struct_pattern = re.compile(r'type\\s+(\\w+)\\s+struct')\n",
    "    interface_pattern = re.compile(r'type\\s+(\\w+)\\s+interface')\n",
    "    \n",
    "    in_import_block = False\n",
    "    \n",
    "    for line_no, line in enumerate(lines, 1):\n",
    "        # Find package declaration\n",
    "        package_match = package_pattern.search(line)\n",
    "        if package_match:\n",
    "            package_name = package_match.group(1)\n",
    "            imports.append((f\"package {package_name}\", line_no))\n",
    "        \n",
    "        # Handle single-line imports\n",
    "        import_match = import_single_pattern.search(line)\n",
    "        if import_match:\n",
    "            import_name = import_match.group(1)\n",
    "            imports.append((import_name, line_no))\n",
    "        \n",
    "        # Handle multi-line imports\n",
    "        if import_multi_start_pattern.search(line):\n",
    "            in_import_block = True\n",
    "            continue\n",
    "        \n",
    "        if in_import_block:\n",
    "            if line.strip() == ')':\n",
    "                in_import_block = False\n",
    "                continue\n",
    "                \n",
    "            import_line_match = import_multi_line_pattern.search(line)\n",
    "            if import_line_match:\n",
    "                import_name = import_line_match.group(1)\n",
    "                imports.append((import_name, line_no))\n",
    "        \n",
    "        # Find function definitions\n",
    "        func_match = func_pattern.search(line)\n",
    "        if func_match:\n",
    "            func_name = func_match.group(1)\n",
    "            context = get_context_around_line(data, file_path, line_no)\n",
    "            symbols[func_name] = {\n",
    "                'type': 'function',\n",
    "                'line_no': line_no,\n",
    "                'context': context\n",
    "            }\n",
    "        \n",
    "        # Find struct definitions\n",
    "        struct_match = struct_pattern.search(line)\n",
    "        if struct_match:\n",
    "            struct_name = struct_match.group(1)\n",
    "            context = get_context_around_line(data, file_path, line_no)\n",
    "            symbols[struct_name] = {\n",
    "                'type': 'struct',\n",
    "                'line_no': line_no,\n",
    "                'context': context\n",
    "            }\n",
    "        \n",
    "        # Find interface definitions\n",
    "        interface_match = interface_pattern.search(line)\n",
    "        if interface_match:\n",
    "            interface_name = interface_match.group(1)\n",
    "            context = get_context_around_line(data, file_path, line_no)\n",
    "            symbols[interface_name] = {\n",
    "                'type': 'interface',\n",
    "                'line_no': line_no,\n",
    "                'context': context\n",
    "            }\n",
    "    \n",
    "    data['import_relations'][file_path] = imports\n",
    "    data['module_symbols'][file_path] = symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_file(data, file_path, content, language):\n",
    "    \"\"\"Analyze a file for imports and symbols with line numbers and context.\"\"\"\n",
    "    if language == \"python\":\n",
    "        analyze_python_file(data, file_path, content)\n",
    "    elif language == \"cpp\":\n",
    "        analyze_cpp_file(data, file_path, content)\n",
    "    elif language == \"java\":\n",
    "        analyze_java_file(data, file_path, content)\n",
    "    elif language == \"go\":\n",
    "        analyze_go_file(data, file_path, content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_references_in_python_file(data, file_path, content):\n",
    "    \"\"\"Find references to symbols in a Python file.\"\"\"\n",
    "    try:\n",
    "        tree = ast.parse(content)\n",
    "        \n",
    "        for node in ast.walk(tree):\n",
    "            # Find variable references\n",
    "            if isinstance(node, ast.Name) and isinstance(node.ctx, ast.Load):\n",
    "                symbol_name = node.id\n",
    "                line_no = node.lineno\n",
    "                \n",
    "                # Track reference with context\n",
    "                if symbol_name not in data['symbol_references']:\n",
    "                    data['symbol_references'][symbol_name] = []\n",
    "                \n",
    "                context = get_context_around_line(data, file_path, line_no)\n",
    "                data['symbol_references'][symbol_name].append((file_path, line_no, context))\n",
    "            \n",
    "            # Find attribute references (e.g., obj.method())\n",
    "            elif isinstance(node, ast.Attribute) and isinstance(node.ctx, ast.Load):\n",
    "                attr_name = node.attr\n",
    "                line_no = node.lineno\n",
    "                \n",
    "                if attr_name not in data['symbol_references']:\n",
    "                    data['symbol_references'][attr_name] = []\n",
    "                \n",
    "                context = get_context_around_line(data, file_path, line_no)\n",
    "                data['symbol_references'][attr_name].append((file_path, line_no, context))\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error finding references in Python file {file_path}: {e}\")\n",
    "\n",
    "def find_references_in_go_file(data, file_path, content):\n",
    "    \"\"\"Find references to symbols in a Go file with optimized performance.\"\"\"\n",
    "    # Get all symbol names from all files to check for references\n",
    "    all_symbols = set()\n",
    "    for symbols_dict in data['module_symbols'].values():\n",
    "        all_symbols.update(symbols_dict.keys())\n",
    "    \n",
    "    # Skip if no symbols to check or file is empty\n",
    "    if not all_symbols or not content:\n",
    "        return\n",
    "    \n",
    "    # Pre-compile all regex patterns\n",
    "    symbol_patterns = {}\n",
    "    for symbol_name in all_symbols:\n",
    "        # Only create patterns for symbols with reasonable length (avoid single-character symbols)\n",
    "        if len(symbol_name) > 2:\n",
    "            symbol_patterns[symbol_name] = re.compile(r'\\b' + re.escape(symbol_name) + r'\\b')\n",
    "    \n",
    "    # Process content line by line\n",
    "    lines = content.splitlines()\n",
    "    \n",
    "    # Skip definition lines for this file\n",
    "    definition_lines = {}\n",
    "    if file_path in data['module_symbols']:\n",
    "        for symbol, details in data['module_symbols'][file_path].items():\n",
    "            definition_lines[details['line_no']] = symbol\n",
    "    \n",
    "    for line_no, line in enumerate(lines, 1):\n",
    "        # Skip comment lines and import/package declarations\n",
    "        if (line.strip().startswith(\"//\") or \n",
    "            line.strip().startswith(\"/*\") or \n",
    "            line.strip().startswith(\"import \") or \n",
    "            line.strip().startswith(\"package \")):\n",
    "            continue\n",
    "        \n",
    "        # Skip if this line is a symbol definition\n",
    "        if line_no in definition_lines:\n",
    "            continue\n",
    "        \n",
    "        # Check for symbol references\n",
    "        for symbol_name, pattern in symbol_patterns.items():\n",
    "            if pattern.search(line):\n",
    "                # Skip if this is a definition line for this symbol\n",
    "                if (file_path in data['module_symbols'] and \n",
    "                    symbol_name in data['module_symbols'][file_path] and \n",
    "                    data['module_symbols'][file_path][symbol_name]['line_no'] == line_no):\n",
    "                    continue\n",
    "                \n",
    "                if symbol_name not in data['symbol_references']:\n",
    "                    data['symbol_references'][symbol_name] = []\n",
    "                \n",
    "                context = get_context_around_line(data, file_path, line_no)\n",
    "                data['symbol_references'][symbol_name].append((file_path, line_no, context))\n",
    "\n",
    "\n",
    "def find_references_in_cpp_file(data, file_path, content):\n",
    "    \"\"\"Find references to symbols in a C/C++ file with optimized performance.\"\"\"\n",
    "    # Get all symbol names from all files to check for references\n",
    "    all_symbols = set()\n",
    "    for symbols_dict in data['module_symbols'].values():\n",
    "        all_symbols.update(symbols_dict.keys())\n",
    "    \n",
    "    # Skip if no symbols to check or file is empty\n",
    "    if not all_symbols or not content:\n",
    "        return\n",
    "    \n",
    "    # Pre-compile all regex patterns for symbols with meaningful length\n",
    "    symbol_patterns = {}\n",
    "    for symbol_name in all_symbols:\n",
    "        # Skip very short symbols that would cause many false positives\n",
    "        if len(symbol_name) > 2:\n",
    "            symbol_patterns[symbol_name] = re.compile(r'\\b' + re.escape(symbol_name) + r'\\b')\n",
    "    \n",
    "    # Process content line by line\n",
    "    lines = content.splitlines()\n",
    "    \n",
    "    # Skip definition lines for this file\n",
    "    definition_lines = {}\n",
    "    if file_path in data['module_symbols']:\n",
    "        for symbol, details in data['module_symbols'][file_path].items():\n",
    "            definition_lines[details['line_no']] = symbol\n",
    "    \n",
    "    for line_no, line in enumerate(lines, 1):\n",
    "        # Skip comment lines and preprocessor directives\n",
    "        if (line.strip().startswith(\"//\") or \n",
    "            line.strip().startswith(\"/*\") or \n",
    "            line.strip().startswith(\"#\")):\n",
    "            continue\n",
    "        \n",
    "        # Skip if this line is a symbol definition\n",
    "        if line_no in definition_lines:\n",
    "            continue\n",
    "        \n",
    "        # Check for symbol references\n",
    "        for symbol_name, pattern in symbol_patterns.items():\n",
    "            if pattern.search(line):\n",
    "                # Skip if this is a definition line for this symbol\n",
    "                if (file_path in data['module_symbols'] and \n",
    "                    symbol_name in data['module_symbols'][file_path] and \n",
    "                    data['module_symbols'][file_path][symbol_name]['line_no'] == line_no):\n",
    "                    continue\n",
    "                \n",
    "                if symbol_name not in data['symbol_references']:\n",
    "                    data['symbol_references'][symbol_name] = []\n",
    "                \n",
    "                context = get_context_around_line(data, file_path, line_no)\n",
    "                data['symbol_references'][symbol_name].append((file_path, line_no, context))\n",
    "\n",
    "\n",
    "def find_references_in_java_file(data, file_path, content):\n",
    "    \"\"\"Find references to symbols in a Java file with optimized performance.\"\"\"\n",
    "    # Get all symbol names from all files to check for references\n",
    "    all_symbols = set()\n",
    "    for symbols_dict in data['module_symbols'].values():\n",
    "        all_symbols.update(symbols_dict.keys())\n",
    "    \n",
    "    # Skip if no symbols to check or file is empty\n",
    "    if not all_symbols or not content:\n",
    "        return\n",
    "    \n",
    "    # Pre-compile all regex patterns for symbols with meaningful length\n",
    "    symbol_patterns = {}\n",
    "    for symbol_name in all_symbols:\n",
    "        # Skip very short symbols that would cause many false positives\n",
    "        if len(symbol_name) > 2:\n",
    "            symbol_patterns[symbol_name] = re.compile(r'\\b' + re.escape(symbol_name) + r'\\b')\n",
    "    \n",
    "    # Process content line by line\n",
    "    lines = content.splitlines()\n",
    "    \n",
    "    # Skip definition lines for this file\n",
    "    definition_lines = {}\n",
    "    if file_path in data['module_symbols']:\n",
    "        for symbol, details in data['module_symbols'][file_path].items():\n",
    "            definition_lines[details['line_no']] = symbol\n",
    "    \n",
    "    for line_no, line in enumerate(lines, 1):\n",
    "        # Skip comment lines, imports, and package declarations\n",
    "        if (line.strip().startswith(\"//\") or \n",
    "            line.strip().startswith(\"/*\") or \n",
    "            line.strip().startswith(\"import \") or \n",
    "            line.strip().startswith(\"package \")):\n",
    "            continue\n",
    "        \n",
    "        # Skip if this line is a symbol definition\n",
    "        if line_no in definition_lines:\n",
    "            continue\n",
    "        \n",
    "        # Check for symbol references\n",
    "        for symbol_name, pattern in symbol_patterns.items():\n",
    "            if pattern.search(line):\n",
    "                # Skip if this is a definition line for this symbol\n",
    "                if (file_path in data['module_symbols'] and \n",
    "                    symbol_name in data['module_symbols'][file_path] and \n",
    "                    data['module_symbols'][file_path][symbol_name]['line_no'] == line_no):\n",
    "                    continue\n",
    "                \n",
    "                if symbol_name not in data['symbol_references']:\n",
    "                    data['symbol_references'][symbol_name] = []\n",
    "                \n",
    "                context = get_context_around_line(data, file_path, line_no)\n",
    "                data['symbol_references'][symbol_name].append((file_path, line_no, context))\n",
    "\n",
    "def find_references_in_file(data, file_path, content, language):\n",
    "    \"\"\"Find references to symbols in a file based on its language.\"\"\"\n",
    "    if language == \"python\":\n",
    "        find_references_in_python_file(data, file_path, content)\n",
    "    elif language == \"cpp\":\n",
    "        find_references_in_cpp_file(data, file_path, content)\n",
    "    elif language == \"java\":\n",
    "        find_references_in_java_file(data, file_path, content)\n",
    "    elif language == \"go\":\n",
    "        find_references_in_go_file(data, file_path, content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_symbol_index(data):\n",
    "    \"\"\"Build a comprehensive index of all symbols and where they're defined/used.\"\"\"\n",
    "    # Initialize the symbol index\n",
    "    data['symbol_index'] = {}\n",
    "    \n",
    "    # First, add all symbol definitions\n",
    "    for file_path, symbols in data['module_symbols'].items():\n",
    "        for symbol_name, details in symbols.items():\n",
    "            if symbol_name not in data['symbol_index']:\n",
    "                data['symbol_index'][symbol_name] = []\n",
    "            \n",
    "            data['symbol_index'][symbol_name].append({\n",
    "                'file': file_path,\n",
    "                'type': 'definition',\n",
    "                'symbol_type': details['type'],\n",
    "                'line_no': details['line_no'],\n",
    "                'context': details.get('context', ''),\n",
    "                'docstring': details.get('docstring', '')\n",
    "            })\n",
    "    \n",
    "    # Then, add all references\n",
    "    for symbol_name, references in data['symbol_references'].items():\n",
    "        if symbol_name not in data['symbol_index']:\n",
    "            data['symbol_index'][symbol_name] = []\n",
    "        \n",
    "        for file_path, line_no, context in references:\n",
    "            # Avoid duplicating references if they're already in definitions\n",
    "            if not any(ref['file'] == file_path and ref['line_no'] == line_no and ref['type'] == 'definition' \n",
    "                      for ref in data['symbol_index'].get(symbol_name, [])):\n",
    "                data['symbol_index'][symbol_name].append({\n",
    "                    'file': file_path,\n",
    "                    'type': 'reference',\n",
    "                    'line_no': line_no,\n",
    "                    'context': context\n",
    "                })\n",
    "\n",
    "def parse_files(data):\n",
    "    \"\"\"Parse all files in the directory and build relationships.\"\"\"\n",
    "    # First pass: Index all files and create directory nodes\n",
    "    for root, dirs, files in os.walk(data['root_dir']):\n",
    "        # Add directory node\n",
    "        rel_dir = os.path.relpath(root, data['root_dir'])\n",
    "        if rel_dir != '.':\n",
    "            data['directories'].add(rel_dir)\n",
    "            data['graph'].add_node(rel_dir, type='directory')\n",
    "            \n",
    "            # Add edge from parent directory to this directory (if not root)\n",
    "            parent_dir = os.path.dirname(rel_dir)\n",
    "            if parent_dir and parent_dir != '.':\n",
    "                data['graph'].add_edge(parent_dir, rel_dir, edge_type='contains_directory')\n",
    "\n",
    "        # Index files of supported languages\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            rel_path = os.path.relpath(file_path, data['root_dir'])\n",
    "            file_language = detect_language(data, file_path)\n",
    "            \n",
    "            if file_language in data['supported_languages']:\n",
    "                data['file_index'][rel_path] = get_next_index(data)\n",
    "                \n",
    "                # Add node for this file\n",
    "                data['graph'].add_node(rel_path, type='file', file_index=data['file_index'][rel_path], language=file_language)\n",
    "                \n",
    "                # Connect file to its directory\n",
    "                if rel_dir != '.':\n",
    "                    data['graph'].add_edge(rel_dir, rel_path, edge_type='contains_file')\n",
    "                \n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                        content = f.read()\n",
    "                        data['file_contents'][rel_path] = content\n",
    "                        analyze_file(data, rel_path, content, file_language)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error parsing {file_path}: {e}\")\n",
    "    \n",
    "    # Second pass: Find symbol references across files\n",
    "    for file_path, content in data['file_contents'].items():\n",
    "        file_language = detect_language(data, file_path)\n",
    "        find_references_in_file(data, file_path, content, file_language)\n",
    "    \n",
    "    # Build the symbol index after all analyses\n",
    "    build_symbol_index(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(data):\n",
    "    \"\"\"Build the NetworkX graph with enhanced node and edge information.\"\"\"\n",
    "    # We've already added basic file and directory nodes during parsing\n",
    "    # Now add more detailed connections and data\n",
    "    \n",
    "    # Add nodes for all directories (if not already added)\n",
    "    for directory in data['directories']:\n",
    "        if not data['graph'].has_node(directory):\n",
    "            data['graph'].add_node(directory, type='directory')\n",
    "        \n",
    "        # Ensure parent directories exist and are connected\n",
    "        parts = directory.split(os.sep)\n",
    "        for i in range(1, len(parts)):\n",
    "            parent_path = os.sep.join(parts[:i])\n",
    "            if parent_path and not data['graph'].has_node(parent_path):\n",
    "                data['graph'].add_node(parent_path, type='directory')\n",
    "                data['directories'].add(parent_path)\n",
    "            \n",
    "            # Connect parent to child directory\n",
    "            if parent_path:\n",
    "                child_path = os.sep.join(parts[:i+1])\n",
    "                data['graph'].add_edge(parent_path, child_path, edge_type='contains_directory')\n",
    "    \n",
    "    # Add nodes for all files with indices and code snippet nodes\n",
    "    for file_path, file_idx in data['file_index'].items():\n",
    "        language = detect_language(data, file_path)\n",
    "        \n",
    "        # Update file node if it exists, create it otherwise\n",
    "        if data['graph'].has_node(file_path):\n",
    "            data['graph'].nodes[file_path].update({\n",
    "                'file_index': file_idx,\n",
    "                'directory': os.path.dirname(file_path),\n",
    "                'language': language\n",
    "            })\n",
    "        else:\n",
    "            data['graph'].add_node(file_path, \n",
    "                               type='file',\n",
    "                               file_index=file_idx,\n",
    "                               directory=os.path.dirname(file_path),\n",
    "                               language=language)\n",
    "        \n",
    "        # Connect file to its directory\n",
    "        directory = os.path.dirname(file_path)\n",
    "        if directory:\n",
    "            # Make sure the directory node exists\n",
    "            if not data['graph'].has_node(directory):\n",
    "                data['graph'].add_node(directory, type='directory')\n",
    "                data['directories'].add(directory)\n",
    "            \n",
    "            # Add edge from directory to file if it doesn't exist\n",
    "            if not data['graph'].has_edge(directory, file_path):\n",
    "                data['graph'].add_edge(directory, file_path, edge_type='contains_file')\n",
    "        \n",
    "        # Create snippet nodes for the entire file\n",
    "        if file_path in data['file_contents']:\n",
    "            chunks = chunk_code(data['file_contents'][file_path])\n",
    "            for idx, chunk_info in enumerate(chunks):\n",
    "                snippet_node = f\"{file_path}::snippet::{idx}\"\n",
    "                data['graph'].add_node(snippet_node,\n",
    "                                   type='snippet',\n",
    "                                   code_snippet=chunk_info['code_snippet'],\n",
    "                                   start_line=chunk_info['start_line'],\n",
    "                                   end_line=chunk_info['end_line'],\n",
    "                                   language=language)\n",
    "                # Connect file node to snippet node\n",
    "                data['graph'].add_edge(file_path, snippet_node, \n",
    "                                   edge_type='contains_snippet',\n",
    "                                   start_line=chunk_info['start_line'],\n",
    "                                   end_line=chunk_info['end_line'])\n",
    "\n",
    "        # Add nodes for symbols in this file\n",
    "        for symbol, details in data['module_symbols'].get(file_path, {}).items():\n",
    "            symbol_node = f\"{file_path}::{symbol}\"\n",
    "            data['graph'].add_node(symbol_node, \n",
    "                               type='symbol',\n",
    "                               symbol_type=details['type'],\n",
    "                               line_number=details['line_no'],\n",
    "                               context=details.get('context', ''),\n",
    "                               docstring=details.get('docstring', ''))\n",
    "            data['graph'].add_edge(file_path, symbol_node, \n",
    "                               edge_type='defines',\n",
    "                               line_number=details['line_no'])\n",
    "\n",
    "    # Add edges for imports with line numbers\n",
    "    for file_path, imports in data['import_relations'].items():\n",
    "        for imp, line_no in imports:\n",
    "            # Look for matching files or symbols\n",
    "            for target_file, symbols in data['module_symbols'].items():\n",
    "                if imp in symbols:\n",
    "                    data['graph'].add_edge(file_path, \n",
    "                                       f\"{target_file}::{imp}\",\n",
    "                                       edge_type='import',\n",
    "                                       line_number=line_no)\n",
    "                # For Python, handle module imports\n",
    "                elif detect_language(data, file_path) == \"python\" and target_file.replace('.py', '').endswith(imp):\n",
    "                    data['graph'].add_edge(file_path, \n",
    "                                       target_file,\n",
    "                                       edge_type='import',\n",
    "                                       line_number=line_no)\n",
    "                # For Java, handle package imports\n",
    "                elif detect_language(data, file_path) == \"java\" and imp.startswith(os.path.splitext(os.path.basename(target_file))[0]):\n",
    "                    data['graph'].add_edge(file_path, \n",
    "                                       target_file,\n",
    "                                       edge_type='import',\n",
    "                                       line_number=line_no)\n",
    "    \n",
    "    # Add edges for symbol references\n",
    "    for symbol, references in data['symbol_references'].items():\n",
    "        for file_path, line_no, context in references:\n",
    "            # Find symbol nodes that match this reference\n",
    "            for target_file, symbols in data['module_symbols'].items():\n",
    "                if symbol in symbols:\n",
    "                    # Create reference edge\n",
    "                    data['graph'].add_edge(file_path, \n",
    "                                       f\"{target_file}::{symbol}\",\n",
    "                                       edge_type='references',\n",
    "                                       line_number=line_no,\n",
    "                                       context=context)\n",
    "    \n",
    "    return data['graph']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_graph_and_data(data, G):\n",
    "    \"\"\"Validate the graph and data structures for consistency and coverage.\"\"\"\n",
    "    report = {\n",
    "        'stats': {\n",
    "            'files': len(data['file_index']),\n",
    "            'directories': len(data['directories']),\n",
    "            'symbols': len(data['symbol_index']),\n",
    "            'nodes': len(G.nodes()),\n",
    "            'edges': len(G.edges())\n",
    "        },\n",
    "        'issues': []\n",
    "    }\n",
    "    \n",
    "    # Check that all files in file_index have corresponding nodes\n",
    "    for file_path in data['file_index']:\n",
    "        if not G.has_node(file_path):\n",
    "            report['issues'].append(f\"File {file_path} in index but missing from graph\")\n",
    "    \n",
    "    # Check that all directories have nodes\n",
    "    for directory in data['directories']:\n",
    "        if not G.has_node(directory):\n",
    "            report['issues'].append(f\"Directory {directory} in data but missing from graph\")\n",
    "    \n",
    "    # Check symbol connections\n",
    "    for symbol, entries in data['symbol_index'].items():\n",
    "        definition_files = [entry['file'] for entry in entries if entry['type'] == 'definition']\n",
    "        for def_file in definition_files:\n",
    "            symbol_node = f\"{def_file}::{symbol}\"\n",
    "            if not G.has_node(symbol_node):\n",
    "                report['issues'].append(f\"Symbol {symbol} defined in {def_file} but node missing from graph\")\n",
    "    \n",
    "    # Count symbols by type\n",
    "    symbol_types = {}\n",
    "    for entries in data['symbol_index'].values():\n",
    "        for entry in entries:\n",
    "            if entry['type'] == 'definition' and 'symbol_type' in entry:\n",
    "                symbol_type = entry['symbol_type']\n",
    "                if symbol_type not in symbol_types:\n",
    "                    symbol_types[symbol_type] = 0\n",
    "                symbol_types[symbol_type] += 1\n",
    "    \n",
    "    report['stats']['symbol_types'] = symbol_types\n",
    "    \n",
    "    # Count edge types\n",
    "    edge_types = {}\n",
    "    for _, _, attrs in G.edges(data=True):\n",
    "        edge_type = attrs.get('edge_type', 'unknown')\n",
    "        if edge_type not in edge_types:\n",
    "            edge_types[edge_type] = 0\n",
    "        edge_types[edge_type] += 1\n",
    "    \n",
    "    report['stats']['edge_types'] = edge_types\n",
    "    \n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(root_directory):\n",
    "    # Initialize all data structures\n",
    "    data = initialize_data_structures()\n",
    "    data['root_dir'] = root_directory\n",
    "    \n",
    "    print(f\"Starting to parse files in {root_directory}...\")\n",
    "    \n",
    "    # Parse all the files in the directory\n",
    "    parse_files(data)\n",
    "    print(f\"Parsed {len(data['file_index'])} files\")\n",
    "    \n",
    "    # Build the graph representation\n",
    "    G = build_graph(data)\n",
    "    print(f\"Graph has {len(G.nodes())} nodes and {len(G.edges())} edges\")\n",
    "    \n",
    "    # Validate the graph and data\n",
    "    report = validate_graph_and_data(data, G)\n",
    "    print(json.dumps(report, indent=2))\n",
    "    \n",
    "    return data, G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"flask\"\n",
    "\n",
    "data, G = main(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "db = ArangoClient(hosts=\"https://d2eeb8083350.arangodb.cloud:8529\").db(username=\"root\", password=\"cUZ0YaNdcwfUTw6VjRny\", verify=True)\n",
    "\n",
    "print(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "G_adb = nxadb.Graph(\n",
    "    name=\"FlaskRepv1\",\n",
    "    db=db,\n",
    "    incoming_graph_data=G,\n",
    "    write_batch_size=50000, # feel free to modify\n",
    "    overwrite_graph=True\n",
    ")\n",
    "\n",
    "print(G_adb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run from loaded database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<StandardDatabase _system>\n"
     ]
    }
   ],
   "source": [
    "db = ArangoClient(hosts=\"https://d2eeb8083350.arangodb.cloud:8529\").db(username=\"root\", password=\"cUZ0YaNdcwfUTw6VjRny\", verify=True)\n",
    "\n",
    "print(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20:27:38 +0530] [INFO]: Graph 'FlaskRepv1' exists.\n",
      "[20:27:39 +0530] [INFO]: Default node type set to 'FlaskRepv1_node'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nx_arangodb.classes.graph.Graph'>\n"
     ]
    }
   ],
   "source": [
    "G_adb = nxadb.Graph(\n",
    "    name=\"FlaskRepv1\",\n",
    "    db=db,\n",
    "    #incoming_graph_data=G,\n",
    "    #write_batch_size=50000 # feel free to modify\n",
    ")\n",
    "\n",
    "print(type(G_adb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arango_graph = ArangoGraph(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Graph Schema': [{'graph_name': 'CodebaseGraph', 'edge_definitions': [{'edge_collection': 'CodebaseGraph_node_to_CodebaseGraph_node', 'from_vertex_collections': ['CodebaseGraph_node'], 'to_vertex_collections': ['CodebaseGraph_node']}]}, {'graph_name': 'FlaskRepv1_node_to_FlaskRespv1_node', 'edge_definitions': [{'edge_collection': 'FlaskRepv1_node_to_FlaskRespv1_node_node_to_FlaskRepv1_node_to_FlaskRespv1_node_node', 'from_vertex_collections': ['FlaskRepv1_node_to_FlaskRespv1_node_node'], 'to_vertex_collections': ['FlaskRepv1_node_to_FlaskRespv1_node_node']}]}, {'graph_name': 'FlaskRepv1_node', 'edge_definitions': [{'edge_collection': 'FlaskRepv1_node_node_to_FlaskRepv1_node_node', 'from_vertex_collections': ['FlaskRepv1_node_node'], 'to_vertex_collections': ['FlaskRepv1_node_node']}]}, {'graph_name': 'code_graph', 'edge_definitions': [{'edge_collection': 'code_edges', 'from_vertex_collections': ['code_nodes'], 'to_vertex_collections': ['code_nodes']}]}, {'graph_name': 'FlaskRespv1', 'edge_definitions': [{'edge_collection': 'FlaskRespv1_node_to_FlaskRespv1_node', 'from_vertex_collections': ['FlaskRespv1_node'], 'to_vertex_collections': ['FlaskRespv1_node']}]}, {'graph_name': 'FlaskRepv2', 'edge_definitions': [{'edge_collection': 'FlaskRepv2_node_to_FlaskRepv2_node', 'from_vertex_collections': ['FlaskRepv2_node'], 'to_vertex_collections': ['FlaskRepv2_node']}]}, {'graph_name': 'Flaskv3', 'edge_definitions': [{'edge_collection': 'Flaskv3_edges', 'from_vertex_collections': ['Flaskv3_nodes'], 'to_vertex_collections': ['Flaskv3_nodes']}]}, {'graph_name': 'Flaskv2', 'edge_definitions': [{'edge_collection': 'Flaskv2_node_to_Flaskv2_node', 'from_vertex_collections': ['Flaskv2_node'], 'to_vertex_collections': ['Flaskv2_node']}]}, {'graph_name': 'FlaskRepv1', 'edge_definitions': [{'edge_collection': 'FlaskRepv1_node_to_FlaskRepv1_node', 'from_vertex_collections': ['FlaskRepv1_node'], 'to_vertex_collections': ['FlaskRepv1_node']}]}], 'Collection Schema': [{'collection_name': 'Flaskv2_node', 'collection_type': 'document', 'document_properties': [{'name': '_key', 'type': 'str'}, {'name': '_id', 'type': 'str'}, {'name': '_rev', 'type': 'str'}, {'name': 'ast_type', 'type': 'str'}, {'name': 'file_path', 'type': 'str'}, {'name': 'rel_path', 'type': 'str'}, {'name': 'module_name', 'type': 'str'}, {'name': 'dir_depth', 'type': 'int'}, {'name': 'imported_by_count', 'type': 'int'}, {'name': 'name', 'type': 'str'}], 'example_document': {'_key': '0', '_id': 'Flaskv2_node/0', '_rev': '_jVNoMc2---', 'ast_type': 'File', 'file_path': 'flask/src/flask/testing.py', 'rel_path': '../src/flask/testing.py', 'module_name': '...src.flask.testing', 'dir_depth': 3, 'imported_by_count': 0, 'name': 'testing.py'}}, {'collection_name': 'FlaskRepv1_node', 'collection_type': 'document', 'document_properties': [{'name': '_key', 'type': 'str'}, {'name': '_id', 'type': 'str'}, {'name': '_rev', 'type': 'str'}, {'name': 'type', 'type': 'str'}], 'example_document': {'_key': '0', '_id': 'FlaskRepv1_node/0', '_rev': '_jVrCqTK---', 'type': 'directory'}}, {'collection_name': 'Flaskv3_nodes', 'collection_type': 'document', 'document_properties': [{'name': '_key', 'type': 'str'}, {'name': '_id', 'type': 'str'}, {'name': '_rev', 'type': 'str'}, {'name': 'type', 'type': 'str'}, {'name': 'path', 'type': 'str'}, {'name': 'docstring', 'type': 'str'}, {'name': 'code_snippet', 'type': 'str'}], 'example_document': {'_key': '0', '_id': 'Flaskv3_nodes/0', '_rev': '_jVQPDwa---', 'type': 'module', 'path': 'flask/src/flask/debughelpers.py', 'docstring': '', 'code_snippet': 'from __future__ import annotations\\n\\nimport typing as t\\n\\nfrom jinja2.loaders import BaseLoader\\nfrom werkzeug.routing import RequestRedirect\\n\\nfrom .blueprints import Blueprint\\nfrom .globals import request_ctx\\nfrom .sansio.app import App\\n\\nif t.TYPE_CHECKING:\\n    from .sansio.scaffold import Scaffold\\n    from .wrappers import Request\\n\\n\\nclass UnexpectedUnicodeError(AssertionError, UnicodeError):\\n    \"\"\"Raised in places where we want some better error reporting for\\n    unexpected unicode or binary dat...'}}, {'collection_name': 'Flaskv2_node_to_Flaskv2_node', 'collection_type': 'edge', 'edge_properties': [{'name': '_key', 'type': 'str'}, {'name': '_id', 'type': 'str'}, {'name': '_from', 'type': 'str'}, {'name': '_to', 'type': 'str'}, {'name': '_rev', 'type': 'str'}, {'name': 'relation', 'type': 'str'}], 'example_edge': {'_key': '0', '_id': 'Flaskv2_node_to_Flaskv2_node/0', '_from': 'Flaskv2_node/0', '_to': 'Flaskv2_node/1', '_rev': '_jVNoNBi--h', 'relation': 'contains'}}, {'collection_name': 'Flaskv3_edges', 'collection_type': 'edge', 'edge_properties': [{'name': '_key', 'type': 'str'}, {'name': '_id', 'type': 'str'}, {'name': '_from', 'type': 'str'}, {'name': '_to', 'type': 'str'}, {'name': '_rev', 'type': 'str'}, {'name': 'relationship', 'type': 'str'}, {'name': 'weight', 'type': 'int'}], 'example_edge': {'_key': '0', '_id': 'Flaskv3_edges/0', '_from': 'Flaskv3_nodes/0', '_to': 'Flaskv3_nodes/1', '_rev': '_jVQPEBi---', 'relationship': 'contains', 'weight': 1}}, {'collection_name': 'FlaskRepv1_node_to_FlaskRepv1_node', 'collection_type': 'edge', 'edge_properties': [{'name': '_key', 'type': 'str'}, {'name': '_id', 'type': 'str'}, {'name': '_from', 'type': 'str'}, {'name': '_to', 'type': 'str'}, {'name': '_rev', 'type': 'str'}, {'name': 'edge_type', 'type': 'str'}], 'example_edge': {'_key': '0', '_id': 'FlaskRepv1_node_to_FlaskRepv1_node/0', '_from': 'FlaskRepv1_node/0', '_to': 'FlaskRepv1_node/1', '_rev': '_jVrCq5K---', 'edge_type': 'contains_directory'}}]}\n"
     ]
    }
   ],
   "source": [
    "print( arango_graph.schema )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
