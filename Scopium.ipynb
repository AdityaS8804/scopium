{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scopium : Expanding the scope of your codebase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features of Scopium:\n",
    "- Automatic chunking and storing of the entire codebase based on relationship between files(imports, directory levels, symbol types)  \n",
    "- Efficient retrieving system performed with a hybrid approach in mind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset - The codebase needed by the user\n",
    "- When the root directory of the codebase is given as the argument, it converts it to a networkx graph capturing all the mentioned relationship between codes. \n",
    "- This is then loaded to an arangoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step0 - Installs and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nx-arangodb in ./.venv/lib/python3.10/site-packages (1.3.0)\n",
      "Requirement already satisfied: networkx<=3.4,>=3.0 in ./.venv/lib/python3.10/site-packages (from nx-arangodb) (3.4)\n",
      "Requirement already satisfied: phenolrs~=0.5 in ./.venv/lib/python3.10/site-packages (from nx-arangodb) (0.5.9)\n",
      "Requirement already satisfied: python-arango~=8.1 in ./.venv/lib/python3.10/site-packages (from nx-arangodb) (8.1.6)\n",
      "Requirement already satisfied: adbnx-adapter~=5.0.5 in ./.venv/lib/python3.10/site-packages (from nx-arangodb) (5.0.6)\n",
      "Requirement already satisfied: requests>=2.27.1 in ./.venv/lib/python3.10/site-packages (from adbnx-adapter~=5.0.5->nx-arangodb) (2.32.3)\n",
      "Requirement already satisfied: rich>=12.5.1 in ./.venv/lib/python3.10/site-packages (from adbnx-adapter~=5.0.5->nx-arangodb) (13.9.4)\n",
      "Requirement already satisfied: setuptools>=45 in ./.venv/lib/python3.10/site-packages (from adbnx-adapter~=5.0.5->nx-arangodb) (75.6.0)\n",
      "Requirement already satisfied: numpy~=1.26 in ./.venv/lib/python3.10/site-packages (from phenolrs~=0.5->nx-arangodb) (1.26.4)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in ./.venv/lib/python3.10/site-packages (from python-arango~=8.1->nx-arangodb) (2.3.0)\n",
      "Requirement already satisfied: requests_toolbelt in ./.venv/lib/python3.10/site-packages (from python-arango~=8.1->nx-arangodb) (1.0.0)\n",
      "Requirement already satisfied: PyJWT in ./.venv/lib/python3.10/site-packages (from python-arango~=8.1->nx-arangodb) (2.10.1)\n",
      "Requirement already satisfied: importlib_metadata>=4.7.1 in ./.venv/lib/python3.10/site-packages (from python-arango~=8.1->nx-arangodb) (8.6.1)\n",
      "Requirement already satisfied: packaging>=23.1 in ./.venv/lib/python3.10/site-packages (from python-arango~=8.1->nx-arangodb) (24.2)\n",
      "Requirement already satisfied: zipp>=3.20 in ./.venv/lib/python3.10/site-packages (from importlib_metadata>=4.7.1->python-arango~=8.1->nx-arangodb) (3.21.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests>=2.27.1->adbnx-adapter~=5.0.5->nx-arangodb) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests>=2.27.1->adbnx-adapter~=5.0.5->nx-arangodb) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests>=2.27.1->adbnx-adapter~=5.0.5->nx-arangodb) (2025.1.31)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.10/site-packages (from rich>=12.5.1->adbnx-adapter~=5.0.5->nx-arangodb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.10/site-packages (from rich>=12.5.1->adbnx-adapter~=5.0.5->nx-arangodb) (2.19.1)\n",
      "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in ./.venv/lib/python3.10/site-packages (from rich>=12.5.1->adbnx-adapter~=5.0.5->nx-arangodb) (4.12.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=12.5.1->adbnx-adapter~=5.0.5->nx-arangodb) (0.1.2)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.nvidia.com\n",
      "Collecting nx-cugraph-cu12\n",
      "  Downloading https://pypi.nvidia.com/nx-cugraph-cu12/nx_cugraph_cu12-25.2.0-py3-none-any.whl (160 kB)\n",
      "INFO: pip is looking at multiple versions of nx-cugraph-cu12 to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading https://pypi.nvidia.com/nx-cugraph-cu12/nx_cugraph_cu12-24.12.0-py3-none-any.whl (152 kB)\n",
      "  Downloading https://pypi.nvidia.com/nx-cugraph-cu12/nx_cugraph_cu12-24.10.0-py3-none-any.whl (149 kB)\n",
      "  Downloading https://pypi.nvidia.com/nx-cugraph-cu12/nx_cugraph_cu12-24.8.0-py3-none-any.whl (140 kB)\n",
      "  Downloading https://pypi.nvidia.com/nx-cugraph-cu12/nx_cugraph_cu12-24.6.1-py3-none-any.whl (129 kB)\n",
      "  Downloading https://pypi.nvidia.com/nx-cugraph-cu12/nx_cugraph_cu12-24.6.0-py3-none-any.whl (129 kB)\n",
      "  Downloading https://pypi.nvidia.com/nx-cugraph-cu12/nx_cugraph_cu12-24.4.0-py3-none-any.whl (125 kB)\n",
      "  Downloading https://pypi.nvidia.com/nx-cugraph-cu12/nx_cugraph_cu12-24.2.0-py3-none-any.whl (117 kB)\n",
      "INFO: pip is still looking at multiple versions of nx-cugraph-cu12 to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading https://pypi.nvidia.com/nx-cugraph-cu12/nx_cugraph_cu12-23.12.0-py3-none-any.whl (87 kB)\n",
      "  Downloading https://pypi.nvidia.com/nx-cugraph-cu12/nx_cugraph_cu12-23.10.0-py3-none-any.whl (39 kB)\n",
      "\u001b[31mERROR: Cannot install nx-cugraph-cu12==23.10.0, nx-cugraph-cu12==23.12.0, nx-cugraph-cu12==24.10.0, nx-cugraph-cu12==24.12.0, nx-cugraph-cu12==24.2.0, nx-cugraph-cu12==24.4.0, nx-cugraph-cu12==24.6.0, nx-cugraph-cu12==24.6.1, nx-cugraph-cu12==24.8.0 and nx-cugraph-cu12==25.2.0 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "The conflict is caused by:\n",
      "    nx-cugraph-cu12 25.2.0 depends on cupy-cuda12x>=12.0.0\n",
      "    nx-cugraph-cu12 24.12.0 depends on cupy-cuda12x>=12.0.0\n",
      "    nx-cugraph-cu12 24.10.0 depends on cupy-cuda12x>=12.0.0\n",
      "    nx-cugraph-cu12 24.8.0 depends on cupy-cuda12x>=12.0.0\n",
      "    nx-cugraph-cu12 24.6.1 depends on cupy-cuda12x>=12.0.0\n",
      "    nx-cugraph-cu12 24.6.0 depends on cupy-cuda12x>=12.0.0\n",
      "    nx-cugraph-cu12 24.4.0 depends on cupy-cuda12x>=12.0.0\n",
      "    nx-cugraph-cu12 24.2.0 depends on cupy-cuda12x>=12.0.0\n",
      "    nx-cugraph-cu12 23.12.0 depends on cupy-cuda12x>=12.0.0\n",
      "    nx-cugraph-cu12 23.10.0 depends on cupy-cuda12x>=12.0.0\n",
      "\n",
      "To fix this you could try to:\n",
      "1. loosen the range of package versions you've specified\n",
      "2. remove package versions to allow pip to attempt to solve the dependency conflict\n",
      "\n",
      "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
      "\u001b[0mRequirement already satisfied: langchain in ./.venv/lib/python3.10/site-packages (0.3.20)\n",
      "Requirement already satisfied: langchain-community in ./.venv/lib/python3.10/site-packages (0.3.19)\n",
      "Requirement already satisfied: langchain-openai in ./.venv/lib/python3.10/site-packages (0.3.8)\n",
      "Requirement already satisfied: langgraph in ./.venv/lib/python3.10/site-packages (0.3.5)\n",
      "Requirement already satisfied: langchain_mistralai in ./.venv/lib/python3.10/site-packages (0.2.7)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.41 in ./.venv/lib/python3.10/site-packages (from langchain) (0.3.43)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in ./.venv/lib/python3.10/site-packages (from langchain) (0.3.6)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in ./.venv/lib/python3.10/site-packages (from langchain) (0.3.11)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in ./.venv/lib/python3.10/site-packages (from langchain) (2.10.6)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./.venv/lib/python3.10/site-packages (from langchain) (2.0.38)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.venv/lib/python3.10/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.venv/lib/python3.10/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in ./.venv/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./.venv/lib/python3.10/site-packages (from langchain-community) (3.11.13)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in ./.venv/lib/python3.10/site-packages (from langchain-community) (9.0.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in ./.venv/lib/python3.10/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in ./.venv/lib/python3.10/site-packages (from langchain-community) (2.8.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in ./.venv/lib/python3.10/site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in ./.venv/lib/python3.10/site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.58.1 in ./.venv/lib/python3.10/site-packages (from langchain-openai) (1.65.1)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in ./.venv/lib/python3.10/site-packages (from langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.10 in ./.venv/lib/python3.10/site-packages (from langgraph) (2.0.16)\n",
      "Requirement already satisfied: langgraph-prebuilt<0.2,>=0.1.1 in ./.venv/lib/python3.10/site-packages (from langgraph) (0.1.1)\n",
      "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.42 in ./.venv/lib/python3.10/site-packages (from langgraph) (0.1.53)\n",
      "Requirement already satisfied: tokenizers<1,>=0.15.1 in ./.venv/lib/python3.10/site-packages (from langchain_mistralai) (0.21.0)\n",
      "Requirement already satisfied: httpx<1,>=0.25.2 in ./.venv/lib/python3.10/site-packages (from langchain_mistralai) (0.25.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.venv/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./.venv/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.10/site-packages (from httpx<1,>=0.25.2->langchain_mistralai) (4.8.0)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.10/site-packages (from httpx<1,>=0.25.2->langchain_mistralai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.10/site-packages (from httpx<1,>=0.25.2->langchain_mistralai) (1.0.7)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.10/site-packages (from httpx<1,>=0.25.2->langchain_mistralai) (3.10)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.10/site-packages (from httpx<1,>=0.25.2->langchain_mistralai) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.25.2->langchain_mistralai) (0.14.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.venv/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./.venv/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./.venv/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (4.12.2)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.1.0 in ./.venv/lib/python3.10/site-packages (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph) (1.1.0)\n",
      "Requirement already satisfied: orjson>=3.10.1 in ./.venv/lib/python3.10/site-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./.venv/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in ./.venv/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.10/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./.venv/lib/python3.10/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (0.8.2)\n",
      "Requirement already satisfied: tqdm>4 in ./.venv/lib/python3.10/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./.venv/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in ./.venv/lib/python3.10/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./.venv/lib/python3.10/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in ./.venv/lib/python3.10/site-packages (from tokenizers<1,>=0.15.1->langchain_mistralai) (0.29.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./.venv/lib/python3.10/site-packages (from anyio->httpx<1,>=0.25.2->langchain_mistralai) (1.2.2)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain_mistralai) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain_mistralai) (2025.2.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.41->langchain) (3.0.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.venv/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: networkx==3.4 in ./.venv/lib/python3.10/site-packages (3.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install nx-arangodb\n",
    "!pip install nx-cugraph-cu12 --extra-index-url https://pypi.nvidia.com # Requires CUDA-capable GPU\n",
    "!pip install --upgrade langchain langchain-community langchain-openai langgraph langchain_mistralai\n",
    "!pip install networkx==3.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import ast\n",
    "from typing import Dict, Set, List, Tuple, Optional,Any\n",
    "import json\n",
    "from arango import ArangoClient\n",
    "import nx_arangodb as nxadb\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.graphs import ArangoGraph\n",
    "from langchain_community.chains.graph_qa.arangodb import ArangoGraphQAChain\n",
    "from langchain_core.tools import tool\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the graph - \n",
    "- This code takes the root directory of the codebase as the input. \n",
    "- It then builds the graph based on the mentioned features and storing it in appropriate nodes and edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global data structures\n",
    "def initialize_data_structures():\n",
    "    data = {\n",
    "        'root_dir': '',\n",
    "        'graph': nx.DiGraph(),\n",
    "        'file_contents': {},  # file -> content\n",
    "        'import_relations': {},  # file -> [(module, line_no)]\n",
    "        'module_symbols': {},  # file -> {symbol -> {type, line_no, context}}\n",
    "        'symbol_references': {},  # symbol -> [(file, line_no, context)]\n",
    "        'file_index': {},  # Maps files to indices\n",
    "        'current_index': 0,\n",
    "        'directories': set(),\n",
    "        'symbol_index': {},  # symbol -> [{file, type, line_no, context}]\n",
    "        'supported_languages': [\"python\", \"cpp\", \"java\", \"go\"],\n",
    "        'language_extensions': {\n",
    "            \"python\": [\".py\"],\n",
    "            \"cpp\": [\".c\", \".cpp\", \".h\", \".hpp\", \".cc\", \".cxx\", \".hxx\"],\n",
    "            \"java\": [\".java\"],\n",
    "            \"go\": [\".go\"]\n",
    "        }\n",
    "    }\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Core utility functions for code processing\n",
    "- Functions for indexing, chunking, context extraction, and language detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_index(data):\n",
    "    \"\"\"Get next available index for file indexing.\"\"\"\n",
    "    data['current_index'] += 1\n",
    "    return data['current_index']\n",
    "\n",
    "def chunk_code(code, lines_per_chunk=20):\n",
    "    \"\"\"\n",
    "    Chunk the given code into snippets.\n",
    "    Returns a list of dictionaries with 'code_snippet', 'start_line', and 'end_line'.\n",
    "    \"\"\"\n",
    "    lines = code.splitlines()\n",
    "    chunks = []\n",
    "    for i in range(0, len(lines), lines_per_chunk):\n",
    "        chunk_lines = lines[i:i + lines_per_chunk]\n",
    "        chunk = {\n",
    "            'code_snippet': '\\n'.join(chunk_lines),\n",
    "            'start_line': i + 1,\n",
    "            'end_line': i + len(chunk_lines)\n",
    "        }\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "def get_context_around_line(data, file_path, line_no, context_lines=3):\n",
    "    \"\"\"Extract context around a specific line in a file.\"\"\"\n",
    "    if file_path not in data['file_contents']:\n",
    "        return \"\"\n",
    "    \n",
    "    lines = data['file_contents'][file_path].splitlines()\n",
    "    start = max(0, line_no - context_lines - 1)\n",
    "    end = min(len(lines), line_no + context_lines)\n",
    "    \n",
    "    context = \"\\n\".join(lines[start:end])\n",
    "    return context\n",
    "\n",
    "def detect_language(data, file_path):\n",
    "    \"\"\"Detect the programming language of a file based on its extension.\"\"\"\n",
    "    _, ext = os.path.splitext(file_path)\n",
    "    ext = ext.lower()\n",
    "    \n",
    "    for language, extensions in data['language_extensions'].items():\n",
    "        if ext in extensions:\n",
    "            return language\n",
    "            \n",
    "    return \"unknown\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python AST analysis functions\n",
    "- Extract source code from Python AST nodes\n",
    "- Analyze Python files for imports and symbol definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_python_node_source(source, node):\n",
    "    \"\"\"Extract the source code for a Python AST node.\"\"\"\n",
    "    try:\n",
    "        lines = source.splitlines()\n",
    "        if hasattr(node, 'lineno') and hasattr(node, 'end_lineno'):\n",
    "            start = node.lineno - 1\n",
    "            end = getattr(node, 'end_lineno', start + 1)\n",
    "            return '\\n'.join(lines[start:end])\n",
    "        return \"\"\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def analyze_python_file(data, file_path, content):\n",
    "    \"\"\"Analyze a Python file for imports and symbols.\"\"\"\n",
    "    try:\n",
    "        tree = ast.parse(content)\n",
    "        imports = []\n",
    "        symbols = {}\n",
    "\n",
    "        for node in ast.walk(tree):\n",
    "            # Track imports\n",
    "            if isinstance(node, (ast.Import, ast.ImportFrom)):\n",
    "                if isinstance(node, ast.Import):\n",
    "                    for name in node.names:\n",
    "                        imports.append((name.name, node.lineno))\n",
    "                else:  # ImportFrom\n",
    "                    module = node.module if node.module else ''\n",
    "                    for name in node.names:\n",
    "                        imports.append((f\"{module}.{name.name}\" if module else name.name, node.lineno))\n",
    "\n",
    "            # Track defined symbols with line numbers and context\n",
    "            elif isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.Assign)):\n",
    "                if isinstance(node, (ast.FunctionDef, ast.ClassDef)):\n",
    "                    symbol_name = node.name\n",
    "                    symbol_type = 'class' if isinstance(node, ast.ClassDef) else 'function'\n",
    "                    line_no = node.lineno\n",
    "                    context = extract_python_node_source(content, node)\n",
    "                    \n",
    "                    symbols[symbol_name] = {\n",
    "                        'type': symbol_type,\n",
    "                        'line_no': line_no,\n",
    "                        'context': context,\n",
    "                        'docstring': ast.get_docstring(node)\n",
    "                    }\n",
    "                elif isinstance(node, ast.Assign):\n",
    "                    # Handle variable assignments\n",
    "                    for target in node.targets:\n",
    "                        if isinstance(target, ast.Name):\n",
    "                            symbol_name = target.id\n",
    "                            line_no = node.lineno\n",
    "                            context = extract_python_node_source(content, node)\n",
    "                            \n",
    "                            symbols[symbol_name] = {\n",
    "                                'type': 'variable',\n",
    "                                'line_no': line_no,\n",
    "                                'context': context\n",
    "                            }\n",
    "\n",
    "        data['import_relations'][file_path] = imports\n",
    "        data['module_symbols'][file_path] = symbols\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing Python file {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Language-specific code analyzers\n",
    "- Extracts imports, symbols, and structure from C++, Java, and Go files\n",
    "- Uses regex patterns to identify language constructs like classes, functions, and namespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_cpp_file(data, file_path, content):\n",
    "    \"\"\"Analyze a C/C++ file for includes and symbols.\"\"\"\n",
    "    imports = []\n",
    "    symbols = {}\n",
    "    \n",
    "    # Process content line by line\n",
    "    lines = content.splitlines()\n",
    "    \n",
    "    # Regular expressions for C/C++ code analysis\n",
    "    include_pattern = re.compile(r'#include\\s+[<\"]([^>\"]+)[>\"]')\n",
    "    class_pattern = re.compile(r'(?:class|struct)\\s+(\\w+)')\n",
    "    function_pattern = re.compile(r'(\\w+)\\s*\\([^)]*\\)\\s*(?:const|override|final|noexcept)?\\s*(?:{|;)')\n",
    "    namespace_pattern = re.compile(r'namespace\\s+(\\w+)')\n",
    "    \n",
    "    for line_no, line in enumerate(lines, 1):\n",
    "        # Find include statements\n",
    "        include_match = include_pattern.search(line)\n",
    "        if include_match:\n",
    "            imports.append((include_match.group(1), line_no))\n",
    "        \n",
    "        # Find class/struct definitions\n",
    "        class_match = class_pattern.search(line)\n",
    "        if class_match:\n",
    "            class_name = class_match.group(1)\n",
    "            context = get_context_around_line(data, file_path, line_no)\n",
    "            symbols[class_name] = {\n",
    "                'type': 'class',\n",
    "                'line_no': line_no,\n",
    "                'context': context\n",
    "            }\n",
    "        \n",
    "        # Find function definitions (simplified)\n",
    "        function_match = function_pattern.search(line)\n",
    "        if function_match and not line.strip().startswith('#') and not line.strip().startswith('//'):\n",
    "            function_name = function_match.group(1)\n",
    "            # Skip some common keywords that might be mistaken for functions\n",
    "            if function_name not in ['if', 'while', 'for', 'switch', 'return']:\n",
    "                context = get_context_around_line(data, file_path, line_no)\n",
    "                symbols[function_name] = {\n",
    "                    'type': 'function',\n",
    "                    'line_no': line_no,\n",
    "                    'context': context\n",
    "                }\n",
    "        \n",
    "        # Find namespace definitions\n",
    "        namespace_match = namespace_pattern.search(line)\n",
    "        if namespace_match:\n",
    "            namespace_name = namespace_match.group(1)\n",
    "            context = get_context_around_line(data, file_path, line_no)\n",
    "            symbols[namespace_name] = {\n",
    "                'type': 'namespace',\n",
    "                'line_no': line_no,\n",
    "                'context': context\n",
    "            }\n",
    "    \n",
    "    data['import_relations'][file_path] = imports\n",
    "    data['module_symbols'][file_path] = symbols\n",
    "\n",
    "def analyze_java_file(data, file_path, content):\n",
    "    \"\"\"Analyze a Java file for imports and symbols.\"\"\"\n",
    "    imports = []\n",
    "    symbols = {}\n",
    "    \n",
    "    # Process content line by line\n",
    "    lines = content.splitlines()\n",
    "    \n",
    "    # Regular expressions for Java code analysis\n",
    "    package_pattern = re.compile(r'package\\s+([\\w.]+)')\n",
    "    import_pattern = re.compile(r'import\\s+([\\w.]+(?:\\.\\*)?)')\n",
    "    class_pattern = re.compile(r'(?:public|private|protected)?\\s*(?:abstract|final)?\\s*class\\s+(\\w+)')\n",
    "    interface_pattern = re.compile(r'(?:public|private|protected)?\\s*interface\\s+(\\w+)')\n",
    "    method_pattern = re.compile(r'(?:public|private|protected)?\\s*(?:static|final|abstract)?\\s*(?:[\\w<>[\\],\\s]+)\\s+(\\w+)\\s*\\([^)]*\\)')\n",
    "    \n",
    "    for line_no, line in enumerate(lines, 1):\n",
    "        # Find package declaration\n",
    "        package_match = package_pattern.search(line)\n",
    "        if package_match:\n",
    "            package_name = package_match.group(1)\n",
    "            imports.append((package_name, line_no))\n",
    "        \n",
    "        # Find import statements\n",
    "        import_match = import_pattern.search(line)\n",
    "        if import_match:\n",
    "            import_name = import_match.group(1)\n",
    "            imports.append((import_name, line_no))\n",
    "        \n",
    "        # Find class definitions\n",
    "        class_match = class_pattern.search(line)\n",
    "        if class_match:\n",
    "            class_name = class_match.group(1)\n",
    "            context = get_context_around_line(data, file_path, line_no)\n",
    "            symbols[class_name] = {\n",
    "                'type': 'class',\n",
    "                'line_no': line_no,\n",
    "                'context': context\n",
    "            }\n",
    "        \n",
    "        # Find interface definitions\n",
    "        interface_match = interface_pattern.search(line)\n",
    "        if interface_match:\n",
    "            interface_name = interface_match.group(1)\n",
    "            context = get_context_around_line(data, file_path, line_no)\n",
    "            symbols[interface_name] = {\n",
    "                'type': 'interface',\n",
    "                'line_no': line_no,\n",
    "                'context': context\n",
    "            }\n",
    "        \n",
    "        # Find method definitions\n",
    "        method_match = method_pattern.search(line)\n",
    "        if method_match:\n",
    "            method_name = method_match.group(1)\n",
    "            # Skip some common keywords that might be mistaken for methods\n",
    "            if method_name not in ['if', 'while', 'for', 'switch', 'return']:\n",
    "                context = get_context_around_line(data, file_path, line_no)\n",
    "                symbols[method_name] = {\n",
    "                    'type': 'method',\n",
    "                    'line_no': line_no,\n",
    "                    'context': context\n",
    "                }\n",
    "    \n",
    "    data['import_relations'][file_path] = imports\n",
    "    data['module_symbols'][file_path] = symbols\n",
    "\n",
    "def analyze_go_file(data, file_path, content):\n",
    "    \"\"\"Analyze a Go file for imports and symbols.\"\"\"\n",
    "    imports = []\n",
    "    symbols = {}\n",
    "    \n",
    "    # Process content line by line\n",
    "    lines = content.splitlines()\n",
    "    \n",
    "    # Regular expressions for Go code analysis\n",
    "    package_pattern = re.compile(r'package\\s+(\\w+)')\n",
    "    import_single_pattern = re.compile(r'import\\s+\"([^\"]+)\"')\n",
    "    import_multi_start_pattern = re.compile(r'import\\s+\\(')\n",
    "    import_multi_line_pattern = re.compile(r'\\s*\"([^\"]+)\"')\n",
    "    func_pattern = re.compile(r'func\\s+(?:\\([^)]+\\)\\s+)?(\\w+)')\n",
    "    struct_pattern = re.compile(r'type\\s+(\\w+)\\s+struct')\n",
    "    interface_pattern = re.compile(r'type\\s+(\\w+)\\s+interface')\n",
    "    \n",
    "    in_import_block = False\n",
    "    \n",
    "    for line_no, line in enumerate(lines, 1):\n",
    "        # Find package declaration\n",
    "        package_match = package_pattern.search(line)\n",
    "        if package_match:\n",
    "            package_name = package_match.group(1)\n",
    "            imports.append((f\"package {package_name}\", line_no))\n",
    "        \n",
    "        # Handle single-line imports\n",
    "        import_match = import_single_pattern.search(line)\n",
    "        if import_match:\n",
    "            import_name = import_match.group(1)\n",
    "            imports.append((import_name, line_no))\n",
    "        \n",
    "        # Handle multi-line imports\n",
    "        if import_multi_start_pattern.search(line):\n",
    "            in_import_block = True\n",
    "            continue\n",
    "        \n",
    "        if in_import_block:\n",
    "            if line.strip() == ')':\n",
    "                in_import_block = False\n",
    "                continue\n",
    "                \n",
    "            import_line_match = import_multi_line_pattern.search(line)\n",
    "            if import_line_match:\n",
    "                import_name = import_line_match.group(1)\n",
    "                imports.append((import_name, line_no))\n",
    "        \n",
    "        # Find function definitions\n",
    "        func_match = func_pattern.search(line)\n",
    "        if func_match:\n",
    "            func_name = func_match.group(1)\n",
    "            context = get_context_around_line(data, file_path, line_no)\n",
    "            symbols[func_name] = {\n",
    "                'type': 'function',\n",
    "                'line_no': line_no,\n",
    "                'context': context\n",
    "            }\n",
    "        \n",
    "        # Find struct definitions\n",
    "        struct_match = struct_pattern.search(line)\n",
    "        if struct_match:\n",
    "            struct_name = struct_match.group(1)\n",
    "            context = get_context_around_line(data, file_path, line_no)\n",
    "            symbols[struct_name] = {\n",
    "                'type': 'struct',\n",
    "                'line_no': line_no,\n",
    "                'context': context\n",
    "            }\n",
    "        \n",
    "        # Find interface definitions\n",
    "        interface_match = interface_pattern.search(line)\n",
    "        if interface_match:\n",
    "            interface_name = interface_match.group(1)\n",
    "            context = get_context_around_line(data, file_path, line_no)\n",
    "            symbols[interface_name] = {\n",
    "                'type': 'interface',\n",
    "                'line_no': line_no,\n",
    "                'context': context\n",
    "            }\n",
    "    \n",
    "    data['import_relations'][file_path] = imports\n",
    "    data['module_symbols'][file_path] = symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysize the file\n",
    "- Calls the respective language function based on the codebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_file(data, file_path, content, language):\n",
    "    \"\"\"Analyze a file for imports and symbols with line numbers and context.\"\"\"\n",
    "    if language == \"python\":\n",
    "        analyze_python_file(data, file_path, content)\n",
    "    elif language == \"cpp\":\n",
    "        analyze_cpp_file(data, file_path, content)\n",
    "    elif language == \"java\":\n",
    "        analyze_java_file(data, file_path, content)\n",
    "    elif language == \"go\":\n",
    "        analyze_go_file(data, file_path, content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Symbol reference analyzers\n",
    "- Identifies variable and function references across Python, Go, C++, and Java files\n",
    "- Uses AST parsing for Python and regex pattern matching for other languages\n",
    "- Tracks references with file location and surrounding context for better understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_references_in_python_file(data, file_path, content):\n",
    "    \"\"\"Find references to symbols in a Python file.\"\"\"\n",
    "    try:\n",
    "        tree = ast.parse(content)\n",
    "        \n",
    "        for node in ast.walk(tree):\n",
    "            # Find variable references\n",
    "            if isinstance(node, ast.Name) and isinstance(node.ctx, ast.Load):\n",
    "                symbol_name = node.id\n",
    "                line_no = node.lineno\n",
    "                \n",
    "                # Track reference with context\n",
    "                if symbol_name not in data['symbol_references']:\n",
    "                    data['symbol_references'][symbol_name] = []\n",
    "                \n",
    "                context = get_context_around_line(data, file_path, line_no)\n",
    "                data['symbol_references'][symbol_name].append((file_path, line_no, context))\n",
    "            \n",
    "            # Find attribute references (e.g., obj.method())\n",
    "            elif isinstance(node, ast.Attribute) and isinstance(node.ctx, ast.Load):\n",
    "                attr_name = node.attr\n",
    "                line_no = node.lineno\n",
    "                \n",
    "                if attr_name not in data['symbol_references']:\n",
    "                    data['symbol_references'][attr_name] = []\n",
    "                \n",
    "                context = get_context_around_line(data, file_path, line_no)\n",
    "                data['symbol_references'][attr_name].append((file_path, line_no, context))\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error finding references in Python file {file_path}: {e}\")\n",
    "\n",
    "def find_references_in_go_file(data, file_path, content):\n",
    "    \"\"\"Find references to symbols in a Go file with optimized performance.\"\"\"\n",
    "    # Get all symbol names from all files to check for references\n",
    "    all_symbols = set()\n",
    "    for symbols_dict in data['module_symbols'].values():\n",
    "        all_symbols.update(symbols_dict.keys())\n",
    "    \n",
    "    # Skip if no symbols to check or file is empty\n",
    "    if not all_symbols or not content:\n",
    "        return\n",
    "    \n",
    "    # Pre-compile all regex patterns\n",
    "    symbol_patterns = {}\n",
    "    for symbol_name in all_symbols:\n",
    "        # Only create patterns for symbols with reasonable length (avoid single-character symbols)\n",
    "        if len(symbol_name) > 2:\n",
    "            symbol_patterns[symbol_name] = re.compile(r'\\b' + re.escape(symbol_name) + r'\\b')\n",
    "    \n",
    "    # Process content line by line\n",
    "    lines = content.splitlines()\n",
    "    \n",
    "    # Skip definition lines for this file\n",
    "    definition_lines = {}\n",
    "    if file_path in data['module_symbols']:\n",
    "        for symbol, details in data['module_symbols'][file_path].items():\n",
    "            definition_lines[details['line_no']] = symbol\n",
    "    \n",
    "    for line_no, line in enumerate(lines, 1):\n",
    "        # Skip comment lines and import/package declarations\n",
    "        if (line.strip().startswith(\"//\") or \n",
    "            line.strip().startswith(\"/*\") or \n",
    "            line.strip().startswith(\"import \") or \n",
    "            line.strip().startswith(\"package \")):\n",
    "            continue\n",
    "        \n",
    "        # Skip if this line is a symbol definition\n",
    "        if line_no in definition_lines:\n",
    "            continue\n",
    "        \n",
    "        # Check for symbol references\n",
    "        for symbol_name, pattern in symbol_patterns.items():\n",
    "            if pattern.search(line):\n",
    "                # Skip if this is a definition line for this symbol\n",
    "                if (file_path in data['module_symbols'] and \n",
    "                    symbol_name in data['module_symbols'][file_path] and \n",
    "                    data['module_symbols'][file_path][symbol_name]['line_no'] == line_no):\n",
    "                    continue\n",
    "                \n",
    "                if symbol_name not in data['symbol_references']:\n",
    "                    data['symbol_references'][symbol_name] = []\n",
    "                \n",
    "                context = get_context_around_line(data, file_path, line_no)\n",
    "                data['symbol_references'][symbol_name].append((file_path, line_no, context))\n",
    "\n",
    "\n",
    "def find_references_in_cpp_file(data, file_path, content):\n",
    "    \"\"\"Find references to symbols in a C/C++ file with optimized performance.\"\"\"\n",
    "    # Get all symbol names from all files to check for references\n",
    "    all_symbols = set()\n",
    "    for symbols_dict in data['module_symbols'].values():\n",
    "        all_symbols.update(symbols_dict.keys())\n",
    "    \n",
    "    # Skip if no symbols to check or file is empty\n",
    "    if not all_symbols or not content:\n",
    "        return\n",
    "    \n",
    "    # Pre-compile all regex patterns for symbols with meaningful length\n",
    "    symbol_patterns = {}\n",
    "    for symbol_name in all_symbols:\n",
    "        # Skip very short symbols that would cause many false positives\n",
    "        if len(symbol_name) > 2:\n",
    "            symbol_patterns[symbol_name] = re.compile(r'\\b' + re.escape(symbol_name) + r'\\b')\n",
    "    \n",
    "    # Process content line by line\n",
    "    lines = content.splitlines()\n",
    "    \n",
    "    # Skip definition lines for this file\n",
    "    definition_lines = {}\n",
    "    if file_path in data['module_symbols']:\n",
    "        for symbol, details in data['module_symbols'][file_path].items():\n",
    "            definition_lines[details['line_no']] = symbol\n",
    "    \n",
    "    for line_no, line in enumerate(lines, 1):\n",
    "        # Skip comment lines and preprocessor directives\n",
    "        if (line.strip().startswith(\"//\") or \n",
    "            line.strip().startswith(\"/*\") or \n",
    "            line.strip().startswith(\"#\")):\n",
    "            continue\n",
    "        \n",
    "        # Skip if this line is a symbol definition\n",
    "        if line_no in definition_lines:\n",
    "            continue\n",
    "        \n",
    "        # Check for symbol references\n",
    "        for symbol_name, pattern in symbol_patterns.items():\n",
    "            if pattern.search(line):\n",
    "                # Skip if this is a definition line for this symbol\n",
    "                if (file_path in data['module_symbols'] and \n",
    "                    symbol_name in data['module_symbols'][file_path] and \n",
    "                    data['module_symbols'][file_path][symbol_name]['line_no'] == line_no):\n",
    "                    continue\n",
    "                \n",
    "                if symbol_name not in data['symbol_references']:\n",
    "                    data['symbol_references'][symbol_name] = []\n",
    "                \n",
    "                context = get_context_around_line(data, file_path, line_no)\n",
    "                data['symbol_references'][symbol_name].append((file_path, line_no, context))\n",
    "\n",
    "\n",
    "def find_references_in_java_file(data, file_path, content):\n",
    "    \"\"\"Find references to symbols in a Java file with optimized performance.\"\"\"\n",
    "    # Get all symbol names from all files to check for references\n",
    "    all_symbols = set()\n",
    "    for symbols_dict in data['module_symbols'].values():\n",
    "        all_symbols.update(symbols_dict.keys())\n",
    "    \n",
    "    # Skip if no symbols to check or file is empty\n",
    "    if not all_symbols or not content:\n",
    "        return\n",
    "    \n",
    "    # Pre-compile all regex patterns for symbols with meaningful length\n",
    "    symbol_patterns = {}\n",
    "    for symbol_name in all_symbols:\n",
    "        # Skip very short symbols that would cause many false positives\n",
    "        if len(symbol_name) > 2:\n",
    "            symbol_patterns[symbol_name] = re.compile(r'\\b' + re.escape(symbol_name) + r'\\b')\n",
    "    \n",
    "    # Process content line by line\n",
    "    lines = content.splitlines()\n",
    "    \n",
    "    # Skip definition lines for this file\n",
    "    definition_lines = {}\n",
    "    if file_path in data['module_symbols']:\n",
    "        for symbol, details in data['module_symbols'][file_path].items():\n",
    "            definition_lines[details['line_no']] = symbol\n",
    "    \n",
    "    for line_no, line in enumerate(lines, 1):\n",
    "        # Skip comment lines, imports, and package declarations\n",
    "        if (line.strip().startswith(\"//\") or \n",
    "            line.strip().startswith(\"/*\") or \n",
    "            line.strip().startswith(\"import \") or \n",
    "            line.strip().startswith(\"package \")):\n",
    "            continue\n",
    "        \n",
    "        # Skip if this line is a symbol definition\n",
    "        if line_no in definition_lines:\n",
    "            continue\n",
    "        \n",
    "        # Check for symbol references\n",
    "        for symbol_name, pattern in symbol_patterns.items():\n",
    "            if pattern.search(line):\n",
    "                # Skip if this is a definition line for this symbol\n",
    "                if (file_path in data['module_symbols'] and \n",
    "                    symbol_name in data['module_symbols'][file_path] and \n",
    "                    data['module_symbols'][file_path][symbol_name]['line_no'] == line_no):\n",
    "                    continue\n",
    "                \n",
    "                if symbol_name not in data['symbol_references']:\n",
    "                    data['symbol_references'][symbol_name] = []\n",
    "                \n",
    "                context = get_context_around_line(data, file_path, line_no)\n",
    "                data['symbol_references'][symbol_name].append((file_path, line_no, context))\n",
    "\n",
    "def find_references_in_file(data, file_path, content, language):\n",
    "    \"\"\"Find references to symbols in a file based on its language.\"\"\"\n",
    "    if language == \"python\":\n",
    "        find_references_in_python_file(data, file_path, content)\n",
    "    elif language == \"cpp\":\n",
    "        find_references_in_cpp_file(data, file_path, content)\n",
    "    elif language == \"java\":\n",
    "        find_references_in_java_file(data, file_path, content)\n",
    "    elif language == \"go\":\n",
    "        find_references_in_go_file(data, file_path, content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph building and indexing\n",
    "- Constructs a comprehensive symbol index of all definitions and references\n",
    "- Parses all files in the codebase to build directory and file relationships\n",
    "- Creates a graph structure with nodes for files/directories and edges for relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_symbol_index(data):\n",
    "    \"\"\"Build a comprehensive index of all symbols and where they're defined/used.\"\"\"\n",
    "    # Initialize the symbol index\n",
    "    data['symbol_index'] = {}\n",
    "    \n",
    "    # First, add all symbol definitions\n",
    "    for file_path, symbols in data['module_symbols'].items():\n",
    "        for symbol_name, details in symbols.items():\n",
    "            if symbol_name not in data['symbol_index']:\n",
    "                data['symbol_index'][symbol_name] = []\n",
    "            \n",
    "            data['symbol_index'][symbol_name].append({\n",
    "                'file': file_path,\n",
    "                'type': 'definition',\n",
    "                'symbol_type': details['type'],\n",
    "                'line_no': details['line_no'],\n",
    "                'context': details.get('context', ''),\n",
    "                'docstring': details.get('docstring', '')\n",
    "            })\n",
    "    \n",
    "    # Then, add all references\n",
    "    for symbol_name, references in data['symbol_references'].items():\n",
    "        if symbol_name not in data['symbol_index']:\n",
    "            data['symbol_index'][symbol_name] = []\n",
    "        \n",
    "        for file_path, line_no, context in references:\n",
    "            # Avoid duplicating references if they're already in definitions\n",
    "            if not any(ref['file'] == file_path and ref['line_no'] == line_no and ref['type'] == 'definition' \n",
    "                      for ref in data['symbol_index'].get(symbol_name, [])):\n",
    "                data['symbol_index'][symbol_name].append({\n",
    "                    'file': file_path,\n",
    "                    'type': 'reference',\n",
    "                    'line_no': line_no,\n",
    "                    'context': context\n",
    "                })\n",
    "\n",
    "def parse_files(data):\n",
    "    \"\"\"Parse all files in the directory and build relationships.\"\"\"\n",
    "    # First pass: Index all files and create directory nodes\n",
    "    for root, dirs, files in os.walk(data['root_dir']):\n",
    "        # Add directory node\n",
    "        rel_dir = os.path.relpath(root, data['root_dir'])\n",
    "        if rel_dir != '.':\n",
    "            data['directories'].add(rel_dir)\n",
    "            data['graph'].add_node(rel_dir, type='directory')\n",
    "            \n",
    "            # Add edge from parent directory to this directory (if not root)\n",
    "            parent_dir = os.path.dirname(rel_dir)\n",
    "            if parent_dir and parent_dir != '.':\n",
    "                data['graph'].add_edge(parent_dir, rel_dir, edge_type='contains_directory')\n",
    "\n",
    "        # Index files of supported languages\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            rel_path = os.path.relpath(file_path, data['root_dir'])\n",
    "            file_language = detect_language(data, file_path)\n",
    "            \n",
    "            if file_language in data['supported_languages']:\n",
    "                data['file_index'][rel_path] = get_next_index(data)\n",
    "                \n",
    "                # Add node for this file\n",
    "                data['graph'].add_node(rel_path, type='file', file_index=data['file_index'][rel_path], language=file_language)\n",
    "                \n",
    "                # Connect file to its directory\n",
    "                if rel_dir != '.':\n",
    "                    data['graph'].add_edge(rel_dir, rel_path, edge_type='contains_file')\n",
    "                \n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                        content = f.read()\n",
    "                        data['file_contents'][rel_path] = content\n",
    "                        analyze_file(data, rel_path, content, file_language)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error parsing {file_path}: {e}\")\n",
    "    \n",
    "    # Second pass: Find symbol references across files\n",
    "    for file_path, content in data['file_contents'].items():\n",
    "        file_language = detect_language(data, file_path)\n",
    "        find_references_in_file(data, file_path, content, file_language)\n",
    "    \n",
    "    # Build the symbol index after all analyses\n",
    "    build_symbol_index(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph construction\n",
    "- Builds NetworkX graph with enhanced node and edge information\n",
    "- Creates nodes for directories, files, code snippets, and symbols\n",
    "- Establishes relationships between files (imports, references) with line numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(data):\n",
    "    \"\"\"Build the NetworkX graph with enhanced node and edge information.\"\"\"\n",
    "    # We've already added basic file and directory nodes during parsing\n",
    "    # Now add more detailed connections and data\n",
    "    \n",
    "    # Add nodes for all directories (if not already added)\n",
    "    for directory in data['directories']:\n",
    "        if not data['graph'].has_node(directory):\n",
    "            data['graph'].add_node(directory, type='directory')\n",
    "        \n",
    "        # Ensure parent directories exist and are connected\n",
    "        parts = directory.split(os.sep)\n",
    "        for i in range(1, len(parts)):\n",
    "            parent_path = os.sep.join(parts[:i])\n",
    "            if parent_path and not data['graph'].has_node(parent_path):\n",
    "                data['graph'].add_node(parent_path, type='directory')\n",
    "                data['directories'].add(parent_path)\n",
    "            \n",
    "            # Connect parent to child directory\n",
    "            if parent_path:\n",
    "                child_path = os.sep.join(parts[:i+1])\n",
    "                data['graph'].add_edge(parent_path, child_path, edge_type='contains_directory')\n",
    "    \n",
    "    # Add nodes for all files with indices and code snippet nodes\n",
    "    for file_path, file_idx in data['file_index'].items():\n",
    "        language = detect_language(data, file_path)\n",
    "        \n",
    "        # Update file node if it exists, create it otherwise\n",
    "        if data['graph'].has_node(file_path):\n",
    "            data['graph'].nodes[file_path].update({\n",
    "                'file_index': file_idx,\n",
    "                'directory': os.path.dirname(file_path),\n",
    "                'language': language\n",
    "            })\n",
    "        else:\n",
    "            data['graph'].add_node(file_path, \n",
    "                               type='file',\n",
    "                               file_index=file_idx,\n",
    "                               directory=os.path.dirname(file_path),\n",
    "                               language=language)\n",
    "        \n",
    "        # Connect file to its directory\n",
    "        directory = os.path.dirname(file_path)\n",
    "        if directory:\n",
    "            # Make sure the directory node exists\n",
    "            if not data['graph'].has_node(directory):\n",
    "                data['graph'].add_node(directory, type='directory')\n",
    "                data['directories'].add(directory)\n",
    "            \n",
    "            # Add edge from directory to file if it doesn't exist\n",
    "            if not data['graph'].has_edge(directory, file_path):\n",
    "                data['graph'].add_edge(directory, file_path, edge_type='contains_file')\n",
    "        \n",
    "        # Create snippet nodes for the entire file\n",
    "        if file_path in data['file_contents']:\n",
    "            chunks = chunk_code(data['file_contents'][file_path])\n",
    "            for idx, chunk_info in enumerate(chunks):\n",
    "                snippet_node = f\"{file_path}::snippet::{idx}\"\n",
    "                data['graph'].add_node(snippet_node,\n",
    "                                   type='snippet',\n",
    "                                   code_snippet=chunk_info['code_snippet'],\n",
    "                                   start_line=chunk_info['start_line'],\n",
    "                                   end_line=chunk_info['end_line'],\n",
    "                                   language=language)\n",
    "                # Connect file node to snippet node\n",
    "                data['graph'].add_edge(file_path, snippet_node, \n",
    "                                   edge_type='contains_snippet',\n",
    "                                   start_line=chunk_info['start_line'],\n",
    "                                   end_line=chunk_info['end_line'])\n",
    "\n",
    "        # Add nodes for symbols in this file\n",
    "        for symbol, details in data['module_symbols'].get(file_path, {}).items():\n",
    "            symbol_node = f\"{file_path}::{symbol}\"\n",
    "            data['graph'].add_node(symbol_node, \n",
    "                               type='symbol',\n",
    "                               symbol_type=details['type'],\n",
    "                               line_number=details['line_no'],\n",
    "                               context=details.get('context', ''),\n",
    "                               docstring=details.get('docstring', ''))\n",
    "            data['graph'].add_edge(file_path, symbol_node, \n",
    "                               edge_type='defines',\n",
    "                               line_number=details['line_no'])\n",
    "\n",
    "    # Add edges for imports with line numbers\n",
    "    for file_path, imports in data['import_relations'].items():\n",
    "        for imp, line_no in imports:\n",
    "            # Look for matching files or symbols\n",
    "            for target_file, symbols in data['module_symbols'].items():\n",
    "                if imp in symbols:\n",
    "                    data['graph'].add_edge(file_path, \n",
    "                                       f\"{target_file}::{imp}\",\n",
    "                                       edge_type='import',\n",
    "                                       line_number=line_no)\n",
    "                # For Python, handle module imports\n",
    "                elif detect_language(data, file_path) == \"python\" and target_file.replace('.py', '').endswith(imp):\n",
    "                    data['graph'].add_edge(file_path, \n",
    "                                       target_file,\n",
    "                                       edge_type='import',\n",
    "                                       line_number=line_no)\n",
    "                # For Java, handle package imports\n",
    "                elif detect_language(data, file_path) == \"java\" and imp.startswith(os.path.splitext(os.path.basename(target_file))[0]):\n",
    "                    data['graph'].add_edge(file_path, \n",
    "                                       target_file,\n",
    "                                       edge_type='import',\n",
    "                                       line_number=line_no)\n",
    "    \n",
    "    # Add edges for symbol references\n",
    "    for symbol, references in data['symbol_references'].items():\n",
    "        for file_path, line_no, context in references:\n",
    "            # Find symbol nodes that match this reference\n",
    "            for target_file, symbols in data['module_symbols'].items():\n",
    "                if symbol in symbols:\n",
    "                    # Create reference edge\n",
    "                    data['graph'].add_edge(file_path, \n",
    "                                       f\"{target_file}::{symbol}\",\n",
    "                                       edge_type='references',\n",
    "                                       line_number=line_no,\n",
    "                                       context=context)\n",
    "    \n",
    "    return data['graph']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph validation and statistics\n",
    "- Validates graph for consistency between data structures and graph nodes/edges\n",
    "- Generates comprehensive statistics on files, directories, symbols, and connections\n",
    "- Identifies and reports issues like missing nodes or incomplete relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_graph_and_data(data, G):\n",
    "    \"\"\"Validate the graph and data structures for consistency and coverage.\"\"\"\n",
    "    report = {\n",
    "        'stats': {\n",
    "            'files': len(data['file_index']),\n",
    "            'directories': len(data['directories']),\n",
    "            'symbols': len(data['symbol_index']),\n",
    "            'nodes': len(G.nodes()),\n",
    "            'edges': len(G.edges())\n",
    "        },\n",
    "        'issues': []\n",
    "    }\n",
    "    \n",
    "    # Check that all files in file_index have corresponding nodes\n",
    "    for file_path in data['file_index']:\n",
    "        if not G.has_node(file_path):\n",
    "            report['issues'].append(f\"File {file_path} in index but missing from graph\")\n",
    "    \n",
    "    # Check that all directories have nodes\n",
    "    for directory in data['directories']:\n",
    "        if not G.has_node(directory):\n",
    "            report['issues'].append(f\"Directory {directory} in data but missing from graph\")\n",
    "    \n",
    "    # Check symbol connections\n",
    "    for symbol, entries in data['symbol_index'].items():\n",
    "        definition_files = [entry['file'] for entry in entries if entry['type'] == 'definition']\n",
    "        for def_file in definition_files:\n",
    "            symbol_node = f\"{def_file}::{symbol}\"\n",
    "            if not G.has_node(symbol_node):\n",
    "                report['issues'].append(f\"Symbol {symbol} defined in {def_file} but node missing from graph\")\n",
    "    \n",
    "    # Count symbols by type\n",
    "    symbol_types = {}\n",
    "    for entries in data['symbol_index'].values():\n",
    "        for entry in entries:\n",
    "            if entry['type'] == 'definition' and 'symbol_type' in entry:\n",
    "                symbol_type = entry['symbol_type']\n",
    "                if symbol_type not in symbol_types:\n",
    "                    symbol_types[symbol_type] = 0\n",
    "                symbol_types[symbol_type] += 1\n",
    "    \n",
    "    report['stats']['symbol_types'] = symbol_types\n",
    "    \n",
    "    # Count edge types\n",
    "    edge_types = {}\n",
    "    for _, _, attrs in G.edges(data=True):\n",
    "        edge_type = attrs.get('edge_type', 'unknown')\n",
    "        if edge_type not in edge_types:\n",
    "            edge_types[edge_type] = 0\n",
    "        edge_types[edge_type] += 1\n",
    "    \n",
    "    report['stats']['edge_types'] = edge_types\n",
    "    \n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main function and program execution\n",
    "- Initializes data structures and processes codebase starting from root directory\n",
    "- Parses files, builds graph representation, and validates resulting structures\n",
    "- Outputs statistics about parsed files, graph nodes/edges, and validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_graph_builder(root_directory):\n",
    "    # Initialize all data structures\n",
    "    data = initialize_data_structures()\n",
    "    data['root_dir'] = root_directory\n",
    "    \n",
    "    print(f\"Starting to parse files in {root_directory}...\")\n",
    "    \n",
    "    # Parse all the files in the directory\n",
    "    parse_files(data)\n",
    "    print(f\"Parsed {len(data['file_index'])} files\")\n",
    "    \n",
    "    # Build the graph representation\n",
    "    G = build_graph(data)\n",
    "    print(f\"Graph has {len(G.nodes())} nodes and {len(G.edges())} edges\")\n",
    "    \n",
    "    # Validate the graph and data\n",
    "    report = validate_graph_and_data(data, G)\n",
    "    print(json.dumps(report, indent=2))\n",
    "    \n",
    "    return data, G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"flask\"\n",
    "\n",
    "data, G = main_graph_builder(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Database\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "- Rename the .env.temp with your creditials to write into the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "ARANGO_HOST = os.getenv(\"ARANGO_HOST\")\n",
    "ARANGO_USERNAME = os.getenv(\"ARANGO_USERNAME\")\n",
    "ARANGO_PASSWORD = os.getenv(\"ARANGO_PASSWORD\")\n",
    "ARANGO_VERIFY = os.getenv(\"ARANGO_VERIFY\") == \"True\"\n",
    "GRAPH_NAME = os.getenv(\"GRAPH_NAME\")\n",
    "WRITE_BATCH_SIZE = int(os.getenv(\"WRITE_BATCH_SIZE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to ArangoDB using the credentials from the environment variables\n",
    "client = ArangoClient(hosts=ARANGO_HOST)\n",
    "db = client.db(username=ARANGO_USERNAME, password=ARANGO_PASSWORD, verify=ARANGO_VERIFY)\n",
    "print(\"Database connection:\", db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the nxadb Graph object with initial graph data\n",
    "G_adb = nxadb.Graph(\n",
    "    name=GRAPH_NAME,\n",
    "    db=db,\n",
    "    incoming_graph_data=G,\n",
    "    write_batch_size=WRITE_BATCH_SIZE,\n",
    "    overwrite_graph=True\n",
    ")\n",
    "\n",
    "print(\"Graph object with data:\", G_adb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run from loaded database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database connection: <StandardDatabase _system>\n"
     ]
    }
   ],
   "source": [
    "# Connect to ArangoDB using the credentials from the environment variables\n",
    "client = ArangoClient(hosts=ARANGO_HOST)\n",
    "db = client.db(username=ARANGO_USERNAME, password=ARANGO_PASSWORD, verify=ARANGO_VERIFY)\n",
    "print(\"Database connection:\", db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10:46:50 +0530] [INFO]: Graph 'FlaskRepv1' exists.\n",
      "[10:46:51 +0530] [INFO]: Default node type set to 'FlaskRepv1_node'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph object type: <class 'nx_arangodb.classes.graph.Graph'>\n"
     ]
    }
   ],
   "source": [
    "G_adb_loaded = nxadb.Graph(\n",
    "    name=GRAPH_NAME,\n",
    "    db=db,\n",
    ")\n",
    "print(\"Graph object type:\", type(G_adb_loaded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM chatbot for querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import traceback\n",
    "from typing import Dict, Optional, List, Any\n",
    "from arango import ArangoClient\n",
    "from mistralai.client import MistralClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global variables and database connections\n",
    "- Stores references to database connections, clients, and models\n",
    "- Maintains collections for nodes, edges, and various indexing structures\n",
    "- Includes mappings between files, snippets, symbols, and their relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = None\n",
    "client = None\n",
    "mistral_client = None\n",
    "model = None\n",
    "graph_name = None\n",
    "node_collection = None\n",
    "edge_collection = None\n",
    "files = {}\n",
    "snippets = {}\n",
    "symbols = {}\n",
    "db_schema = {}\n",
    "node_types = {}\n",
    "symbol_name_index = {}\n",
    "file_to_snippets = {}\n",
    "file_to_symbols = {}\n",
    "snippet_to_symbols = {}\n",
    "conversation_history = []\n",
    "type_field = None\n",
    "path_field = None\n",
    "edge_type_field = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph structure discovery\n",
    "- Dynamically identifies ArangoDB graph structure and collections\n",
    "- Retrieves edge definitions and node collections from the database\n",
    "- Falls back to default naming patterns if explicit structure not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discover_graph_structure():\n",
    "    \"\"\"Dynamically discover the graph structure in ArangoDB with improved directory detection\"\"\"\n",
    "    global node_collection, edge_collection\n",
    "    \n",
    "    try:\n",
    "        # Get graph object\n",
    "        graph = db.graph(graph_name)\n",
    "        graph_info = graph.properties()\n",
    "        \n",
    "        # Get the edge collection name from graph properties\n",
    "        edge_definitions = graph_info.get('edgeDefinitions', [])\n",
    "        \n",
    "        # If no edge definitions exist, set defaults and retry\n",
    "        if not edge_definitions:\n",
    "            print(f\"No edge definitions found, using default naming pattern\")\n",
    "            node_collection = f\"{graph_name}_node\"\n",
    "            edge_collection = f\"{graph_name}_node_to_{graph_name}_node\"\n",
    "            print(f\"Using default collections: Nodes={node_collection}, Edges={edge_collection}\")\n",
    "            # Validate the schema to understand the field names\n",
    "            validate_schema()\n",
    "            return\n",
    "        \n",
    "        # Get the edge collection\n",
    "        edge_def = edge_definitions[0]\n",
    "        edge_collection = edge_def.get('collection')\n",
    "        \n",
    "        # Get node collection\n",
    "        from_collections = edge_def.get('from', [])\n",
    "        if not from_collections:\n",
    "            # No 'from' collections, use defaults\n",
    "            node_collection = f\"{graph_name}_nodes\"\n",
    "            print(f\"No 'from' collections found, using default node collection: {node_collection}\")\n",
    "        else:\n",
    "            node_collection = from_collections[0]\n",
    "        \n",
    "        print(f\"Using collections: Nodes={node_collection}, Edges={edge_collection}\")\n",
    "        \n",
    "        # Validate the schema to understand the field names\n",
    "        validate_schema()\n",
    "    except Exception as e:\n",
    "        print(f\"Error discovering graph structure: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Schema Validation Function\n",
    "- Identifies key schema fields (type, path, edge_type) by sampling nodes and edges\n",
    "- Searches through candidate field names to find the actual fields used in the database\n",
    "- Handles errors and provides validation feedback through console output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_schema():\n",
    "    \"\"\"Validate the schema and identify the key field names used in this database\"\"\"\n",
    "    global type_field, path_field, edge_type_field\n",
    "    \n",
    "    try:\n",
    "        # Sample nodes to understand the schema\n",
    "        aql = f\"\"\"\n",
    "        FOR v IN {node_collection}\n",
    "        LIMIT 10\n",
    "        RETURN v\n",
    "        \"\"\"\n",
    "        cursor = db.aql.execute(aql)\n",
    "        sample_nodes = [doc for doc in cursor]\n",
    "        \n",
    "        if not sample_nodes:\n",
    "            raise ValueError(f\"No nodes found in collection {node_collection}\")\n",
    "        \n",
    "        # Identify the type field\n",
    "        type_field_candidates = ['type', 'ast_type', 'node_type']\n",
    "        type_field = None\n",
    "        \n",
    "        for field in type_field_candidates:\n",
    "            for node in sample_nodes:\n",
    "                if field in node:\n",
    "                    type_field = field\n",
    "                    print(f\"Found type field: {field}\")\n",
    "                    break\n",
    "            if type_field:\n",
    "                break\n",
    "                \n",
    "        if not type_field:\n",
    "            print(\"Warning: Could not identify a type field in nodes\")\n",
    "        \n",
    "        # Identify path field\n",
    "        path_field_candidates = ['path', 'file_path', 'rel_path']\n",
    "        path_field = None\n",
    "        \n",
    "        for field in path_field_candidates:\n",
    "            for node in sample_nodes:\n",
    "                if field in node:\n",
    "                    path_field = field\n",
    "                    print(f\"Found path field: {field}\")\n",
    "                    break\n",
    "            if path_field:\n",
    "                break\n",
    "        \n",
    "        # Sample edges to understand relationship types\n",
    "        aql = f\"\"\"\n",
    "        FOR e IN {edge_collection}\n",
    "        LIMIT 10\n",
    "        RETURN e\n",
    "        \"\"\"\n",
    "        cursor = db.aql.execute(aql)\n",
    "        sample_edges = [doc for doc in cursor]\n",
    "        \n",
    "        # Identify edge type field\n",
    "        edge_type_field_candidates = ['edge_type', 'relation', 'relationship', 'type']\n",
    "        edge_type_field = None\n",
    "        \n",
    "        for field in edge_type_field_candidates:\n",
    "            for edge in sample_edges:\n",
    "                if field in edge:\n",
    "                    edge_type_field = field\n",
    "                    print(f\"Found edge type field: {field}\")\n",
    "                    break\n",
    "            if edge_type_field:\n",
    "                break\n",
    "        \n",
    "        print(f\"Schema validation complete: type_field={type_field}, path_field={path_field}, edge_type_field={edge_type_field}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error validating schema: {str(e)}\")\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Node Type Validation Function\n",
    "- Verifies directory nodes exist in the graph by checking for 'directory' type nodes\n",
    "- Fallbacks to alternative type field names if standard fields aren't found\n",
    "- Validates edge relationships between directories, checking for 'contains_directory' edges\n",
    "- Includes graceful error handling to allow process to continue even if validation issues occur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_node_types():\n",
    "    \"\"\"Validate that all necessary node types are accessible in the graph\"\"\"\n",
    "    try:\n",
    "        # Check for directory nodes specifically\n",
    "        aql = f\"\"\"\n",
    "        FOR v IN {node_collection}\n",
    "            FILTER v.type == 'directory'\n",
    "            LIMIT 1\n",
    "            RETURN v\n",
    "        \"\"\"\n",
    "        cursor = db.aql.execute(aql)\n",
    "        directories = [doc for doc in cursor]\n",
    "        \n",
    "        if not directories:\n",
    "            print(\"Warning: No directory nodes found in the collection.\")\n",
    "            # Try alternative fields\n",
    "            alternative_fields = ['ast_type', 'node_type']\n",
    "            for field in alternative_fields:\n",
    "                aql = f\"\"\"\n",
    "                FOR v IN {node_collection}\n",
    "                    FILTER v.{field} == 'directory' OR v.{field} == 'Directory'\n",
    "                    LIMIT 1\n",
    "                    RETURN v\n",
    "                \"\"\"\n",
    "                cursor = db.aql.execute(aql)\n",
    "                alternative_dirs = [doc for doc in cursor]\n",
    "                if alternative_dirs:\n",
    "                    print(f\"Found directory nodes using alternate field: {field}\")\n",
    "                    break\n",
    "        else:\n",
    "            print(f\"Found directory nodes successfully\")\n",
    "            \n",
    "        # Also check for edges that connect directories\n",
    "        aql = f\"\"\"\n",
    "        FOR e IN {edge_collection}\n",
    "            FILTER e.edge_type == 'contains_directory'\n",
    "            LIMIT 1\n",
    "            RETURN e\n",
    "        \"\"\"\n",
    "        cursor = db.aql.execute(aql)\n",
    "        dir_edges = [doc for doc in cursor]\n",
    "        \n",
    "        if not dir_edges:\n",
    "            print(\"Warning: No 'contains_directory' edges found in the edge collection.\")\n",
    "            # Try alternative edge types\n",
    "            alt_edge_types = ['contains', 'has_directory', 'parent']\n",
    "            for edge_type in alt_edge_types:\n",
    "                aql = f\"\"\"\n",
    "                FOR e IN {edge_collection}\n",
    "                    FILTER e.edge_type == '{edge_type}' OR e.relation == '{edge_type}' OR e.relationship == '{edge_type}'\n",
    "                    FOR v1 IN {node_collection}\n",
    "                        FILTER v1._id == e._from\n",
    "                        FOR v2 IN {node_collection}\n",
    "                            FILTER v2._id == e._to\n",
    "                            FILTER (v1.type == 'directory' OR v2.type == 'directory')\n",
    "                            LIMIT 1\n",
    "                            RETURN e\n",
    "                \"\"\"\n",
    "                cursor = db.aql.execute(aql)\n",
    "                alt_dir_edges = [doc for doc in cursor]\n",
    "                if alt_dir_edges:\n",
    "                    print(f\"Found directory edges using alternate edge type: {edge_type}\")\n",
    "                    break\n",
    "        else:\n",
    "            print(f\"Found directory edge relationships successfully\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error validating node types: {str(e)}\")\n",
    "        # Not raising the exception here to allow the process to continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Database Schema Retrieval Function\n",
    "- Collects comprehensive schema information including collections, graphs, and edge definitions\n",
    "- Samples edges to identify relationship types and structures within the graph\n",
    "- Organizes schema data hierarchically with graphs, collections, edge definitions, and types\n",
    "- Returns structured dictionary with placeholders for node types and relationships to be filled by other functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_db_schema() -> Dict:\n",
    "    \"\"\"Get detailed schema information with better type understanding\"\"\"\n",
    "    try:\n",
    "        # Basic schema information\n",
    "        collections = db.collections()\n",
    "        collection_names = [c['name'] for c in collections if not c['name'].startswith('_')]\n",
    "        \n",
    "        # Get graphs\n",
    "        graphs = db.graphs()\n",
    "        graph_names = [g['name'] for g in graphs]\n",
    "        graph_details = []\n",
    "        \n",
    "        for graph_name in graph_names:\n",
    "            graph = db.graph(graph_name)\n",
    "            graph_info = graph.properties()\n",
    "            \n",
    "            # Get edge definitions for better understanding of relationships\n",
    "            edge_definitions = graph_info.get('edgeDefinitions', [])\n",
    "            enhanced_edge_defs = []\n",
    "            \n",
    "            for edge_def in edge_definitions:\n",
    "                collection = edge_def.get('collection', '')\n",
    "                from_collections = edge_def.get('from', [])\n",
    "                to_collections = edge_def.get('to', [])\n",
    "                \n",
    "                # Sample some edges to understand relationship types\n",
    "                edge_samples = []\n",
    "                if collection:\n",
    "                    try:\n",
    "                        cursor = db.aql.execute(\n",
    "                            f\"FOR e IN {collection} LIMIT 5 RETURN e\"\n",
    "                        )\n",
    "                        edge_samples = [edge for edge in cursor]\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error sampling edges from {collection}: {str(e)}\")\n",
    "                \n",
    "                # Extract edge types if they exist\n",
    "                edge_types = set()\n",
    "                for edge in edge_samples:\n",
    "                    if 'edge_type' in edge:\n",
    "                        edge_types.add(edge['edge_type'])\n",
    "                \n",
    "                enhanced_edge_defs.append({\n",
    "                    'collection': collection,\n",
    "                    'from_collections': from_collections,\n",
    "                    'to_collections': to_collections,\n",
    "                    'edge_types': list(edge_types),\n",
    "                    'sample_count': len(edge_samples),\n",
    "                })\n",
    "            \n",
    "            graph_details.append({\n",
    "                'name': graph_info.get('name'),\n",
    "                'edge_definitions': enhanced_edge_defs,\n",
    "                'orphan_collections': graph_info.get('orphanCollections', [])\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            \"Graph Schema\": graph_details,\n",
    "            \"Collection Schema\": [c for c in collection_names],\n",
    "            \"Node Types\": {},  # Will be filled by analyze_node_types\n",
    "            \"Type Relationships\": []  # Will be filled later\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting enhanced schema: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Node Type Analysis Function\n",
    "- Analyzes node collection to identify distinct node types and their distribution\n",
    "- Uses detected schema type field or falls back to inferring types from properties\n",
    "- Samples each node type to understand structure and stores in the global schema dictionary\n",
    "- Includes special handling for important node types like 'directory' and 'file' if not found directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_node_types():\n",
    "    \"\"\"Analyze and cache the node types in the database using the detected schema fields\"\"\"\n",
    "    node_types_dict = {}\n",
    "    try:\n",
    "        # Use the detected type field\n",
    "        if not type_field:\n",
    "            print(\"No type field detected, trying to infer node types from other properties\")\n",
    "            # Fallback logic to infer types\n",
    "            return infer_node_types()\n",
    "        \n",
    "        # Query distinct node types\n",
    "        aql = f\"\"\"\n",
    "        FOR v IN {node_collection}\n",
    "            FILTER HAS(v, \"{type_field}\")\n",
    "            COLLECT type = v.{type_field} WITH COUNT INTO count\n",
    "            RETURN {{\n",
    "                \"type\": type,\n",
    "                \"count\": count\n",
    "            }}\n",
    "        \"\"\"\n",
    "        cursor = db.aql.execute(aql)\n",
    "        type_counts = [doc for doc in cursor]\n",
    "        \n",
    "        # For each node type, get a sample and analyze structure\n",
    "        for type_info in type_counts:\n",
    "            node_type = type_info.get('type')\n",
    "            count = type_info.get('count', 0)\n",
    "            \n",
    "            if not node_type:\n",
    "                continue\n",
    "            \n",
    "            # Get a sample for this node type\n",
    "            aql = f\"\"\"\n",
    "            FOR v IN {node_collection}\n",
    "                FILTER v.{type_field} == '{node_type}'\n",
    "                LIMIT 1\n",
    "                RETURN v\n",
    "            \"\"\"\n",
    "            cursor = db.aql.execute(aql)\n",
    "            samples = [doc for doc in cursor]\n",
    "            \n",
    "            if not samples:\n",
    "                continue\n",
    "            \n",
    "            sample = samples[0]\n",
    "            \n",
    "            # Normalize the node type name\n",
    "            normalized_type = node_type\n",
    "            \n",
    "            # Add to node types dictionary\n",
    "            node_types_dict[normalized_type] = {\n",
    "                'count': count,\n",
    "                'field': type_field,\n",
    "                'sample_structure': list(sample.keys()),\n",
    "                'sample': sample\n",
    "            }\n",
    "            \n",
    "            print(f\"Type: {node_type}, Count: {count}\")\n",
    "        \n",
    "        # Update the db_schema with node types\n",
    "        global db_schema\n",
    "        db_schema[\"Node Types\"] = node_types_dict\n",
    "        \n",
    "        # Special handling for directories and files if not found\n",
    "        for important_type in ['directory', 'file']:\n",
    "            if important_type not in node_types_dict:\n",
    "                detect_special_type(important_type, node_types_dict)\n",
    "                \n",
    "        return node_types_dict\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing node types: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Node Type Inference Function\n",
    "- Provides a fallback approach when explicit type fields aren't found in the database\n",
    "- Infers node types by searching for characteristic properties (filename, children, function_name, etc.)\n",
    "- Implements property-based clustering as a second fallback to group similar nodes\n",
    "- Adds inferred type information to the global schema with confidence indicators and inference basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_node_types():\n",
    "    \"\"\"Fallback method to infer node types when type field is not detected\"\"\"\n",
    "    node_types_dict = {}\n",
    "    try:\n",
    "        # Try to infer types based on common patterns in node properties\n",
    "        print(\"Attempting to infer node types from property patterns...\")\n",
    "        \n",
    "        # Check for nodes with filename property (likely files)\n",
    "        aql = f\"\"\"\n",
    "        FOR v IN {node_collection}\n",
    "            FILTER HAS(v, \"filename\") OR HAS(v, \"file_name\")\n",
    "            LIMIT 100\n",
    "            RETURN v\n",
    "        \"\"\"\n",
    "        cursor = db.aql.execute(aql)\n",
    "        file_nodes = [doc for doc in cursor]\n",
    "        \n",
    "        if file_nodes:\n",
    "            sample = file_nodes[0]\n",
    "            node_types_dict[\"file\"] = {\n",
    "                'count': len(file_nodes),\n",
    "                'inferred': True,\n",
    "                'inference_basis': 'Has filename property',\n",
    "                'sample_structure': list(sample.keys()),\n",
    "                'sample': sample\n",
    "            }\n",
    "            print(f\"Inferred type: file, Count: {len(file_nodes)}\")\n",
    "        \n",
    "        # Check for nodes with children or is_directory properties (likely directories)\n",
    "        aql = f\"\"\"\n",
    "        FOR v IN {node_collection}\n",
    "            FILTER HAS(v, \"children\") OR HAS(v, \"is_directory\") OR HAS(v, \"dir_name\")\n",
    "            LIMIT 100\n",
    "            RETURN v\n",
    "        \"\"\"\n",
    "        cursor = db.aql.execute(aql)\n",
    "        dir_nodes = [doc for doc in cursor]\n",
    "        \n",
    "        if dir_nodes:\n",
    "            sample = dir_nodes[0]\n",
    "            node_types_dict[\"directory\"] = {\n",
    "                'count': len(dir_nodes),\n",
    "                'inferred': True,\n",
    "                'inference_basis': 'Has directory-related properties',\n",
    "                'sample_structure': list(sample.keys()),\n",
    "                'sample': sample\n",
    "            }\n",
    "            print(f\"Inferred type: directory, Count: {len(dir_nodes)}\")\n",
    "        \n",
    "        # Check for nodes with function-related properties\n",
    "        aql = f\"\"\"\n",
    "        FOR v IN {node_collection}\n",
    "            FILTER HAS(v, \"function_name\") OR HAS(v, \"params\") OR HAS(v, \"return_type\")\n",
    "            LIMIT 100\n",
    "            RETURN v\n",
    "        \"\"\"\n",
    "        cursor = db.aql.execute(aql)\n",
    "        function_nodes = [doc for doc in cursor]\n",
    "        \n",
    "        if function_nodes:\n",
    "            sample = function_nodes[0]\n",
    "            node_types_dict[\"function\"] = {\n",
    "                'count': len(function_nodes),\n",
    "                'inferred': True,\n",
    "                'inference_basis': 'Has function-related properties',\n",
    "                'sample_structure': list(sample.keys()),\n",
    "                'sample': sample\n",
    "            }\n",
    "            print(f\"Inferred type: function, Count: {len(function_nodes)}\")\n",
    "        \n",
    "        # Check for nodes with class-related properties\n",
    "        aql = f\"\"\"\n",
    "        FOR v IN {node_collection}\n",
    "            FILTER HAS(v, \"class_name\") OR HAS(v, \"methods\") OR HAS(v, \"extends\")\n",
    "            LIMIT 100\n",
    "            RETURN v\n",
    "        \"\"\"\n",
    "        cursor = db.aql.execute(aql)\n",
    "        class_nodes = [doc for doc in cursor]\n",
    "        \n",
    "        if class_nodes:\n",
    "            sample = class_nodes[0]\n",
    "            node_types_dict[\"class\"] = {\n",
    "                'count': len(class_nodes),\n",
    "                'inferred': True,\n",
    "                'inference_basis': 'Has class-related properties',\n",
    "                'sample_structure': list(sample.keys()),\n",
    "                'sample': sample\n",
    "            }\n",
    "            print(f\"Inferred type: class, Count: {len(class_nodes)}\")\n",
    "        \n",
    "        # Check for variable declarations\n",
    "        aql = f\"\"\"\n",
    "        FOR v IN {node_collection}\n",
    "            FILTER HAS(v, \"var_name\") OR HAS(v, \"variable_name\") OR HAS(v, \"variable_type\")\n",
    "            LIMIT 100\n",
    "            RETURN v\n",
    "        \"\"\"\n",
    "        cursor = db.aql.execute(aql)\n",
    "        var_nodes = [doc for doc in cursor]\n",
    "        \n",
    "        if var_nodes:\n",
    "            sample = var_nodes[0]\n",
    "            node_types_dict[\"variable\"] = {\n",
    "                'count': len(var_nodes),\n",
    "                'inferred': True,\n",
    "                'inference_basis': 'Has variable-related properties',\n",
    "                'sample_structure': list(sample.keys()),\n",
    "                'sample': sample\n",
    "            }\n",
    "            print(f\"Inferred type: variable, Count: {len(var_nodes)}\")\n",
    "        \n",
    "        # Fall back to clustering by property sets if no types inferred\n",
    "        if not node_types_dict:\n",
    "            print(\"No clear types inferred, attempting property-based clustering...\")\n",
    "            return cluster_by_properties()\n",
    "        \n",
    "        # Update the global schema with inferred node types\n",
    "        global db_schema\n",
    "        db_schema[\"Node Types\"] = node_types_dict\n",
    "        \n",
    "        return node_types_dict\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error inferring node types: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return {}\n",
    "\n",
    "def cluster_by_properties():\n",
    "    \"\"\"Group nodes by similar property sets to infer types\"\"\"\n",
    "    property_clusters = {}\n",
    "    try:\n",
    "        # Sample nodes to analyze property patterns\n",
    "        aql = f\"\"\"\n",
    "        FOR v IN {node_collection}\n",
    "        LIMIT 200\n",
    "        RETURN v\n",
    "        \"\"\"\n",
    "        cursor = db.aql.execute(aql)\n",
    "        nodes = [doc for doc in cursor]\n",
    "        \n",
    "        # Create property signatures for each node\n",
    "        for i, node in enumerate(nodes):\n",
    "            # Sort keys to create consistent signatures\n",
    "            prop_keys = sorted(list(node.keys()))\n",
    "            # Filter out system properties\n",
    "            prop_keys = [k for k in prop_keys if not k.startswith('_')]\n",
    "            \n",
    "            # Create a signature string\n",
    "            signature = \",\".join(prop_keys)\n",
    "            \n",
    "            if signature not in property_clusters:\n",
    "                property_clusters[signature] = {\n",
    "                    'nodes': [],\n",
    "                    'properties': prop_keys\n",
    "                }\n",
    "            \n",
    "            property_clusters[signature]['nodes'].append(node)\n",
    "        \n",
    "        # Convert clusters to inferred types\n",
    "        inferred_types = {}\n",
    "        for i, (signature, cluster) in enumerate(property_clusters.items()):\n",
    "            if len(cluster['nodes']) < 5:  # Skip tiny clusters\n",
    "                continue\n",
    "                \n",
    "            # Try to find a meaningful name based on properties\n",
    "            type_name = \"unknown_type_\" + str(i)\n",
    "            for name_prop in ['name', 'type', 'kind', 'node_kind']:\n",
    "                if name_prop in cluster['properties']:\n",
    "                    most_common = {}\n",
    "                    for node in cluster['nodes']:\n",
    "                        val = str(node.get(name_prop, ''))\n",
    "                        if val:\n",
    "                            most_common[val] = most_common.get(val, 0) + 1\n",
    "                    \n",
    "                    if most_common:\n",
    "                        # Get most common value\n",
    "                        type_name = max(most_common.items(), key=lambda x: x[1])[0]\n",
    "                        break\n",
    "            \n",
    "            inferred_types[type_name] = {\n",
    "                'count': len(cluster['nodes']),\n",
    "                'inferred': True,\n",
    "                'inference_basis': 'Property signature clustering',\n",
    "                'property_signature': signature,\n",
    "                'sample_structure': cluster['properties'],\n",
    "                'sample': cluster['nodes'][0] if cluster['nodes'] else {}\n",
    "            }\n",
    "            \n",
    "            print(f\"Inferred type via clustering: {type_name}, Count: {len(cluster['nodes'])}\")\n",
    "        \n",
    "        # Update the db_schema\n",
    "        global db_schema\n",
    "        db_schema[\"Node Types\"] = inferred_types\n",
    "        \n",
    "        return inferred_types\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error clustering by properties: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Special Type Detection Function\n",
    "- Implements specialized detection strategies for important node types (directories, files)\n",
    "- Uses heuristic indicators like path patterns and property names to identify unlabeled nodes\n",
    "- Leverages regex patterns on path fields to distinguish files (with extensions) from directories\n",
    "- Updates the node types dictionary with inferred types when successful detection occurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_special_type(type_name, node_types_dict):\n",
    "    \"\"\"Try to detect special types like directories and files if they weren't found by regular means\"\"\"\n",
    "    try:\n",
    "        # Different detection strategies based on type\n",
    "        if type_name == 'directory':\n",
    "            # Look for nodes with directory-like properties\n",
    "            indicators = ['path', 'directory', 'dir_name', 'folder']\n",
    "            filter_conditions = []\n",
    "            \n",
    "            for indicator in indicators:\n",
    "                filter_conditions.append(f'HAS(v, \"{indicator}\")')\n",
    "                \n",
    "            if path_field:\n",
    "                # Add condition that path doesn't end with file extension\n",
    "                filter_conditions.append(f'NOT REGEX_TEST(v.{path_field}, \"\\\\.[a-zA-Z0-9]+$\")')\n",
    "            \n",
    "            filter_str = \" OR \".join(filter_conditions)\n",
    "            \n",
    "            aql = f\"\"\"\n",
    "            FOR v IN {node_collection}\n",
    "                FILTER {filter_str}\n",
    "                LIMIT 100\n",
    "                RETURN v\n",
    "            \"\"\"\n",
    "            \n",
    "        elif type_name == 'file':\n",
    "            # Look for nodes with file-like properties\n",
    "            indicators = ['file', 'file_name', 'filename']\n",
    "            filter_conditions = []\n",
    "            \n",
    "            for indicator in indicators:\n",
    "                filter_conditions.append(f'HAS(v, \"{indicator}\")')\n",
    "                \n",
    "            if path_field:\n",
    "                # Add condition that path ends with file extension\n",
    "                filter_conditions.append(f'REGEX_TEST(v.{path_field}, \"\\\\.[a-zA-Z0-9]+$\")')\n",
    "            \n",
    "            filter_str = \" OR \".join(filter_conditions)\n",
    "            \n",
    "            aql = f\"\"\"\n",
    "            FOR v IN {node_collection}\n",
    "                FILTER {filter_str}\n",
    "                LIMIT 100\n",
    "                RETURN v\n",
    "            \"\"\"\n",
    "        \n",
    "        cursor = db.aql.execute(aql)\n",
    "        detected_nodes = [doc for doc in cursor]\n",
    "        \n",
    "        if detected_nodes:\n",
    "            print(f\"Detected {len(detected_nodes)} potential {type_name} nodes\")\n",
    "            \n",
    "            # Use the first node as a sample\n",
    "            sample = detected_nodes[0]\n",
    "            \n",
    "            node_types_dict[type_name] = {\n",
    "                'count': len(detected_nodes),\n",
    "                'field': 'inferred',\n",
    "                'sample_structure': list(sample.keys()),\n",
    "                'sample': sample\n",
    "            }\n",
    "            \n",
    "            print(f\"Added inferred {type_name} type to node types\")\n",
    "        else:\n",
    "            print(f\"Could not detect any {type_name} nodes\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error detecting {type_name} nodes: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Type Relationship Analysis Function\n",
    "- Examines connections between different node types in the graph database\n",
    "- Searches for edges between each pair of node types to build a relationship map\n",
    "- Identifies specific edge types that connect different node categories\n",
    "- Updates the global schema dictionary with discovered type relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_type_relationships(node_types_dict):\n",
    "    \"\"\"Analyze relationships between different node types\"\"\"\n",
    "    type_relationships = []\n",
    "    try:\n",
    "        node_type_keys = list(node_types_dict.keys())\n",
    "        \n",
    "        # For each node type pair, check if there are edges between them\n",
    "        for from_type in node_type_keys:\n",
    "            for to_type in node_type_keys:\n",
    "                aql = f\"\"\"\n",
    "                FOR v1 IN {node_collection}\n",
    "                    FILTER v1.type == '{from_type}'\n",
    "                    LIMIT 1\n",
    "                    FOR v2 IN {node_collection}\n",
    "                        FILTER v2.type == '{to_type}'\n",
    "                        LIMIT 1\n",
    "                        FOR e IN {edge_collection}\n",
    "                            FILTER e._from == v1._id AND e._to == v2._id\n",
    "                            RETURN DISTINCT {{\n",
    "                                \"from_type\": '{from_type}',\n",
    "                                \"to_type\": '{to_type}',\n",
    "                                \"edge_type\": e.edge_type\n",
    "                            }}\n",
    "                \"\"\"\n",
    "                cursor = db.aql.execute(aql)\n",
    "                relationships = [doc for doc in cursor]\n",
    "                \n",
    "                for rel in relationships:\n",
    "                    type_relationships.append(rel)\n",
    "        \n",
    "        # Update db_schema with type relationships\n",
    "        global db_schema\n",
    "        db_schema[\"Type Relationships\"] = type_relationships\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing type relationships: {str(e)}\")\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directory Structure Builder Function\n",
    "- Constructs a hierarchical representation of the project's directory structure\n",
    "- First attempts to identify directory nodes from the graph database\n",
    "- Falls back to extracting directory paths from file nodes if needed\n",
    "- Builds a nested tree structure with directories containing subdirectories and files\n",
    "- Handles edge cases like missing directories by creating synthetic paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_directory_structure() -> Dict:\n",
    "    \"\"\"\n",
    "    Build a hierarchical representation of the directory structure\n",
    "    Returns:\n",
    "        Dictionary representing the directory tree\n",
    "    \"\"\"\n",
    "    directory_tree = {}\n",
    "    \n",
    "    try:\n",
    "        # First, identify all directory nodes\n",
    "        directory_field = 'type'\n",
    "        if 'directory' in node_types:\n",
    "            directory_field = node_types['directory'].get('field', 'type')\n",
    "            \n",
    "        # Get all directory nodes\n",
    "        aql = f\"\"\"\n",
    "        FOR v IN {node_collection}\n",
    "            FILTER v.{directory_field} == 'directory'\n",
    "            RETURN {{\n",
    "                \"key\": v._key,\n",
    "                \"path\": v.path,\n",
    "                \"name\": v.name\n",
    "            }}\n",
    "        \"\"\"\n",
    "        cursor = db.aql.execute(aql)\n",
    "        directories = [doc for doc in cursor]\n",
    "        \n",
    "        # If no explicit directory nodes found, try to extract directories from file paths\n",
    "        if not directories:\n",
    "            # Extract directories from file paths\n",
    "            all_directories = set()\n",
    "            for file_info in files.values():\n",
    "                file_path = file_info.get(\"file_path\", \"\")\n",
    "                if file_path:\n",
    "                    # Extract all parent directories\n",
    "                    parts = file_path.split('/')\n",
    "                    for i in range(1, len(parts)):\n",
    "                        dir_path = '/'.join(parts[:i])\n",
    "                        if dir_path:\n",
    "                            all_directories.add(dir_path)\n",
    "            \n",
    "            # Create synthetic directory nodes\n",
    "            directories = [{\"path\": dir_path, \"name\": dir_path.split('/')[-1]} for dir_path in all_directories]\n",
    "            \n",
    "        # Build directory tree\n",
    "        for directory in directories:\n",
    "            path = directory.get(\"path\", \"\")\n",
    "            if not path:\n",
    "                continue\n",
    "                \n",
    "            # Add to tree\n",
    "            current = directory_tree\n",
    "            parts = path.split('/')\n",
    "            for i, part in enumerate(parts):\n",
    "                if not part:\n",
    "                    continue\n",
    "                    \n",
    "                if part not in current:\n",
    "                    current[part] = {\"files\": [], \"dirs\": {}}\n",
    "                    \n",
    "                if i == len(parts) - 1:\n",
    "                    # This is the target directory, add its key\n",
    "                    current[part][\"key\"] = directory.get(\"key\")\n",
    "                else:\n",
    "                    current = current[part][\"dirs\"]\n",
    "                    \n",
    "        # Add files to their respective directories\n",
    "        for file_key, file_info in files.items():\n",
    "            file_path = file_info.get(\"file_path\", \"\")\n",
    "            if not file_path:\n",
    "                continue\n",
    "                \n",
    "            # Determine directory path and file name\n",
    "            parts = file_path.split('/')\n",
    "            file_name = parts[-1]\n",
    "            dir_path = '/'.join(parts[:-1])\n",
    "            \n",
    "            # Find directory in tree\n",
    "            current = directory_tree\n",
    "            if dir_path:\n",
    "                found = True\n",
    "                for part in dir_path.split('/'):\n",
    "                    if not part:\n",
    "                        continue\n",
    "                    if part in current:\n",
    "                        current = current[part][\"dirs\"]\n",
    "                    else:\n",
    "                        # Directory not found in tree, create it\n",
    "                        found = False\n",
    "                        break\n",
    "                        \n",
    "                if not found:\n",
    "                    # Create missing directory path\n",
    "                    current = directory_tree\n",
    "                    for part in dir_path.split('/'):\n",
    "                        if not part:\n",
    "                            continue\n",
    "                        if part not in current:\n",
    "                            current[part] = {\"files\": [], \"dirs\": {}}\n",
    "                        current = current[part][\"dirs\"]\n",
    "            \n",
    "            # Find parent directory and add file\n",
    "            parent = directory_tree\n",
    "            for part in parts[:-1]:\n",
    "                if not part:\n",
    "                    continue\n",
    "                if part not in parent:\n",
    "                    parent[part] = {\"files\": [], \"dirs\": {}}\n",
    "                parent = parent[part][\"dirs\"]\n",
    "            \n",
    "            # Add file to parent directory if parent exists and is a valid dictionary\n",
    "            if parts[-2] in parent and isinstance(parent[parts[-2]], dict):\n",
    "                parent[parts[-2]][\"files\"].append({\n",
    "                    \"key\": file_key,\n",
    "                    \"name\": file_name,\n",
    "                    \"path\": file_path,\n",
    "                    \"language\": file_info.get(\"language\", \"\")\n",
    "                })\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error building directory structure: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        \n",
    "    return directory_tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File and Code Knowledge Base Initialization\n",
    "- Caches files, code snippets, and symbols from database with automatic field detection\n",
    "- Establishes relationships between files, snippets, and symbols for fast traversal\n",
    "- Extracts key metadata (file paths, languages, symbol definitions, documentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_cache():\n",
    "    \"\"\"Initialize cache of files, code snippets, and symbols using detected schema fields\"\"\"\n",
    "    global files, snippets, symbols, symbol_name_index\n",
    "    \n",
    "    try:\n",
    "        # Initialize file cache\n",
    "        if 'file' in node_types:\n",
    "            # Determine best field for file info\n",
    "            field_info = node_types['file']\n",
    "            \n",
    "            path_field = None\n",
    "            name_field = None\n",
    "            \n",
    "            # Try to find the best fields for path and name\n",
    "            sample = field_info.get('sample', {})\n",
    "            for field in sample:\n",
    "                lower_field = field.lower()\n",
    "                if 'path' in lower_field and not path_field:\n",
    "                    path_field = field\n",
    "                elif ('name' in lower_field or 'file' in lower_field) and 'path' not in lower_field and not name_field:\n",
    "                    name_field = field\n",
    "            \n",
    "            # Use detected fields or defaults\n",
    "            path_field = path_field or 'path'\n",
    "            name_field = name_field or 'file_name'\n",
    "            type_field = field_info.get('field') or 'type'\n",
    "            \n",
    "            aql = f\"\"\"\n",
    "            FOR v IN {node_collection}\n",
    "                FILTER v.{type_field} == 'file'\n",
    "                RETURN v\n",
    "            \"\"\"\n",
    "            cursor = db.aql.execute(aql)\n",
    "            \n",
    "            # Process each file\n",
    "            for doc in cursor:\n",
    "                file_key = doc.get('_key')\n",
    "                file_path = doc.get(path_field, \"\")\n",
    "                file_name = doc.get(name_field, \"\")\n",
    "                \n",
    "                if not file_path and not file_name:\n",
    "                    continue\n",
    "                \n",
    "                if not file_path and file_name:\n",
    "                    # Try to construct a path\n",
    "                    for key in doc:\n",
    "                        if 'dir' in key.lower() or 'folder' in key.lower():\n",
    "                            directory = doc.get(key, \"\")\n",
    "                            file_path = f\"{directory}/{file_name}\" if directory else file_name\n",
    "                            break\n",
    "                \n",
    "                language = \"\"\n",
    "                # Try to detect language from extension\n",
    "                if file_path:\n",
    "                    ext = file_path.split('.')[-1].lower() if '.' in file_path else \"\"\n",
    "                    if ext == 'py':\n",
    "                        language = 'python'\n",
    "                    elif ext in ['js', 'ts']:\n",
    "                        language = 'javascript'\n",
    "                    elif ext in ['java']:\n",
    "                        language = 'java'\n",
    "                    elif ext in ['c', 'cpp', 'h', 'hpp']:\n",
    "                        language = 'c/c++'\n",
    "                \n",
    "                files[file_key] = {\n",
    "                    \"key\": file_key,\n",
    "                    \"file_name\": file_name,\n",
    "                    \"file_path\": file_path,\n",
    "                    \"language\": language\n",
    "                }\n",
    "            \n",
    "            print(f\"Cached {len(files)} files\")\n",
    "            \n",
    "        # Initialize snippet cache\n",
    "        if 'snippet' in node_types:\n",
    "            # Determine best fields for snippet info\n",
    "            field_info = node_types['snippet']\n",
    "            \n",
    "            content_field = None\n",
    "            name_field = None\n",
    "            \n",
    "            # Try to find the best fields for content and name\n",
    "            sample = field_info.get('sample', {})\n",
    "            for field in sample:\n",
    "                lower_field = field.lower()\n",
    "                if ('content' in lower_field or 'code' in lower_field) and not content_field:\n",
    "                    content_field = field\n",
    "                elif ('name' in lower_field or 'title' in lower_field) and not name_field:\n",
    "                    name_field = field\n",
    "            \n",
    "            # Use detected fields or defaults\n",
    "            content_field = content_field or 'content'\n",
    "            name_field = name_field or 'snippet_name'\n",
    "            type_field = field_info.get('field') or 'type'\n",
    "            \n",
    "            aql = f\"\"\"\n",
    "            FOR v IN {node_collection}\n",
    "                FILTER v.{type_field} == 'snippet'\n",
    "                RETURN v\n",
    "            \"\"\"\n",
    "            cursor = db.aql.execute(aql)\n",
    "            \n",
    "            # Process each snippet\n",
    "            for doc in cursor:\n",
    "                snippet_key = doc.get('_key')\n",
    "                content = doc.get(content_field, \"\")\n",
    "                snippet_name = doc.get(name_field, \"\")\n",
    "                \n",
    "                if not content:\n",
    "                    continue\n",
    "                \n",
    "                # Try to determine file relationship\n",
    "                file_key = None\n",
    "                for key in doc:\n",
    "                    if 'file' in key.lower() and key != name_field:\n",
    "                        file_key = doc.get(key)\n",
    "                        break\n",
    "                \n",
    "                # Try to determine language\n",
    "                language = \"\"\n",
    "                for key in doc:\n",
    "                    if 'lang' in key.lower():\n",
    "                        language = doc.get(key, \"\")\n",
    "                        break\n",
    "                \n",
    "                if not language and file_key in files:\n",
    "                    language = files[file_key].get('language', \"\")\n",
    "                \n",
    "                snippets[snippet_key] = {\n",
    "                    \"key\": snippet_key,\n",
    "                    \"snippet_name\": snippet_name,\n",
    "                    \"content\": content,\n",
    "                    \"file_key\": file_key,\n",
    "                    \"language\": language\n",
    "                }\n",
    "            \n",
    "            print(f\"Cached {len(snippets)} code snippets\")\n",
    "        \n",
    "            # Initialize symbol cache\n",
    "            if 'symbol' in node_types:  # This is checking for an exact match with 'symbol'\n",
    "                # Determine best fields for symbol info\n",
    "                field_info = node_types['symbol']\n",
    "                \n",
    "                name_field = None\n",
    "                type_name_field = None\n",
    "                \n",
    "                # Try to find the best fields for symbol name and symbol type\n",
    "                sample = field_info.get('sample', {})\n",
    "                for field in sample:\n",
    "                    lower_field = field.lower()\n",
    "                    if 'name' in lower_field and not name_field:\n",
    "                        name_field = field\n",
    "                    elif ('type' in lower_field and 'name' in lower_field) and not type_name_field:\n",
    "                        type_name_field = field\n",
    "                \n",
    "                # Add fallback detection for symbol name field\n",
    "                if not name_field and 'context' in sample:\n",
    "                    name_field = 'context'\n",
    "                    print(f\"Using 'context' as fallback for symbol name field\")\n",
    "                \n",
    "                # Use detected fields or defaults\n",
    "                name_field = name_field or 'symbol_name'\n",
    "                type_name_field = type_name_field or 'symbol_type'\n",
    "                type_field = field_info.get('field') or 'type'\n",
    "                \n",
    "                print(f\"Using name_field: {name_field}, type_field: {type_field}\")\n",
    "                \n",
    "                aql = f\"\"\"\n",
    "                FOR v IN {node_collection}\n",
    "                    FILTER v.{type_field} == 'symbol'\n",
    "                    RETURN v\n",
    "                \"\"\"\n",
    "                print(f\"Symbol query: {aql}\")\n",
    "                cursor = db.aql.execute(aql)\n",
    "                sample_symbols = [doc for doc in cursor]\n",
    "                print(f\"Sample symbol count: {len(sample_symbols)}\")\n",
    "                \n",
    "                if sample_symbols:\n",
    "                    print(f\"Sample symbol fields: {list(sample_symbols[0].keys())}\")\n",
    "                    print(f\"Sample symbol name value: {sample_symbols[0].get(name_field, 'NOT FOUND')}\")\n",
    "                    print(f\"Sample symbol type value: {sample_symbols[0].get(type_name_field, 'NOT FOUND')}\")\n",
    "\n",
    "                # Re-execute the query\n",
    "                cursor = db.aql.execute(aql)\n",
    "                \n",
    "                # Process counter\n",
    "                processed_count = 0\n",
    "                \n",
    "                # Process each symbol\n",
    "                for doc in cursor:\n",
    "                    symbol_key = doc.get('_key')\n",
    "                    symbol_name = doc.get(name_field, \"\")\n",
    "                    symbol_type = doc.get(type_name_field, \"\")\n",
    "                    \n",
    "                    if not symbol_name:\n",
    "                        # Try context as a fallback\n",
    "                        symbol_name = doc.get('context', \"\")\n",
    "                        if not symbol_name:\n",
    "                            continue\n",
    "                    \n",
    "                    # Try to determine file relationship\n",
    "                    file_key = None\n",
    "                    for key in doc:\n",
    "                        if 'file' in key.lower() and key != name_field:\n",
    "                            file_key = doc.get(key)\n",
    "                            break\n",
    "                    \n",
    "                    # Try to determine snippet relationship\n",
    "                    snippet_key = None\n",
    "                    for key in doc:\n",
    "                        if 'snippet' in key.lower():\n",
    "                            snippet_key = doc.get(key)\n",
    "                            break\n",
    "                    \n",
    "                    # Try to get definition and documentation\n",
    "                    definition = \"\"\n",
    "                    documentation = doc.get('docstring', \"\")  # Try the known docstring field first\n",
    "                    \n",
    "                    for key in doc:\n",
    "                        lower_key = key.lower()\n",
    "                        if 'def' in lower_key or 'decl' in lower_key:\n",
    "                            definition = doc.get(key, \"\")\n",
    "                        elif ('doc' in lower_key or 'comment' in lower_key) and not documentation:\n",
    "                            documentation = doc.get(key, \"\")\n",
    "                    \n",
    "                    symbols[symbol_key] = {\n",
    "                        \"key\": symbol_key,\n",
    "                        \"symbol_name\": symbol_name,\n",
    "                        \"symbol_type\": symbol_type,\n",
    "                        \"file_key\": file_key,\n",
    "                        \"snippet_key\": snippet_key,\n",
    "                        \"definition\": definition,\n",
    "                        \"documentation\": documentation\n",
    "                    }\n",
    "                    \n",
    "                    # Index by name for quick lookups\n",
    "                    if symbol_name:\n",
    "                        if symbol_name not in symbol_name_index:\n",
    "                            symbol_name_index[symbol_name] = []\n",
    "                        symbol_name_index[symbol_name].append(symbol_key)\n",
    "                    \n",
    "                    processed_count += 1\n",
    "                    if processed_count % 200 == 0:\n",
    "                        print(f\"Processed {processed_count} symbols so far\")\n",
    "                \n",
    "                print(f\"Cached {len(symbols)} symbols\")\n",
    "            \n",
    "        # Build relationship indexes for faster traversal\n",
    "        build_relationship_indexes()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing cache: {str(e)}\")\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relationship Index Building for Knowledge Base\n",
    "- Creates mapping indexes between files, snippets, and symbols for efficient lookup\n",
    "- Builds file-to-snippets, file-to-symbols, and snippet-to-symbols relationships\n",
    "- Enables quick traversal and relationship querying across codebase elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_relationship_indexes():\n",
    "    \"\"\"Build indexes for quick relationship lookup between files, snippets and symbols\"\"\"\n",
    "    global file_to_snippets, file_to_symbols, snippet_to_symbols\n",
    "    \n",
    "    try:\n",
    "        # Build file -> snippets index\n",
    "        for snippet_key, snippet in snippets.items():\n",
    "            file_key = snippet.get('file_key')\n",
    "            if file_key:\n",
    "                if file_key not in file_to_snippets:\n",
    "                    file_to_snippets[file_key] = []\n",
    "                file_to_snippets[file_key].append(snippet_key)\n",
    "        \n",
    "        # Build file -> symbols index\n",
    "        for symbol_key, symbol in symbols.items():\n",
    "            file_key = symbol.get('file_key')\n",
    "            if file_key:\n",
    "                if file_key not in file_to_symbols:\n",
    "                    file_to_symbols[file_key] = []\n",
    "                file_to_symbols[file_key].append(symbol_key)\n",
    "        \n",
    "        # Build snippet -> symbols index\n",
    "        for symbol_key, symbol in symbols.items():\n",
    "            snippet_key = symbol.get('snippet_key')\n",
    "            if snippet_key:\n",
    "                if snippet_key not in snippet_to_symbols:\n",
    "                    snippet_to_symbols[snippet_key] = []\n",
    "                snippet_to_symbols[snippet_key].append(symbol_key)\n",
    "        \n",
    "        print(\"Built relationship indexes for files, snippets, and symbols\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error building relationship indexes: {str(e)}\")\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File Retrieval by Key Helper Function\n",
    "- Fetches file information by key from cache or database if not already loaded\n",
    "- Returns comprehensive file metadata including path, name, directory, and language\n",
    "- Handles error cases gracefully with detailed logging for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_by_key(file_key: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Helper method to retrieve file node by key\n",
    "    \n",
    "    Args:\n",
    "        file_key: The key of the file node\n",
    "        \n",
    "    Returns:\n",
    "        Dict containing file information\n",
    "    \"\"\"\n",
    "    if file_key in files:\n",
    "        return files[file_key]\n",
    "    \n",
    "    try:\n",
    "        aql = f\"\"\"\n",
    "        FOR file IN {node_collection}\n",
    "            FILTER file._key == '{file_key}' AND file.type == 'file'\n",
    "            RETURN {{\n",
    "                \"key\": file._key,\n",
    "                \"directory\": file.directory,\n",
    "                \"file_name\": file.file_name,\n",
    "                \"file_path\": file.path || (file.directory + '/' + file.file_name),\n",
    "                \"language\": file.language\n",
    "            }}\n",
    "        \"\"\"\n",
    "        cursor = db.aql.execute(aql)\n",
    "        found_files = [doc for doc in cursor]\n",
    "        \n",
    "        if found_files:\n",
    "            files[file_key] = found_files[0]\n",
    "            return found_files[0]\n",
    "        \n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving file by key: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Symbol Occurrence Finder Across Codebase\n",
    "- Searches for symbol occurrences in both symbol nodes and code snippets\n",
    "- Links found symbols with their file contexts including path and language information\n",
    "- Handles dynamic field detection for different database schemas and snippet formats\n",
    "- Provides comprehensive results with docstrings, line numbers and contextual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_symbol_occurrences(symbol_name: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Find all occurrences of a symbol using both the symbol nodes and code snippets\n",
    "    \n",
    "    Args:\n",
    "        symbol_name: The name of the symbol to find\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries containing symbol occurrences\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    try:\n",
    "        # Look for symbol nodes\n",
    "        if 'symbol' in node_types:\n",
    "            aql = f\"\"\"\n",
    "            FOR symbol IN {node_collection}\n",
    "                FILTER symbol.type == 'symbol' AND symbol.name == '{symbol_name}'\n",
    "                LET file = (\n",
    "                    FOR edge IN {edge_collection}\n",
    "                        FILTER edge._to == symbol._id\n",
    "                        FOR file IN {node_collection}\n",
    "                            FILTER file._id == edge._from AND file.type == 'file'\n",
    "                            RETURN {{\n",
    "                                \"key\": file._key,\n",
    "                                \"directory\": file.directory,\n",
    "                                \"file_name\": file.file_name,\n",
    "                                \"file_path\": file.path || (file.directory + '/' + file.file_name),\n",
    "                                \"language\": file.language\n",
    "                            }}\n",
    "                )\n",
    "                RETURN {{\n",
    "                    \"type\": \"symbol\",\n",
    "                    \"name\": symbol.name,\n",
    "                    \"symbol_type\": symbol.symbol_type,\n",
    "                    \"line_number\": symbol.line_number,\n",
    "                    \"context\": symbol.context,\n",
    "                    \"docstring\": symbol.docstring,\n",
    "                    \"file\": LENGTH(file) > 0 ? file[0] : null\n",
    "                }}\n",
    "            \"\"\"\n",
    "            cursor = db.aql.execute(aql)\n",
    "            symbol_results = [doc for doc in cursor]\n",
    "            results.extend(symbol_results)\n",
    "        \n",
    "        # Look for symbol occurrences in code snippets\n",
    "        if 'snippet' in node_types:\n",
    "            # Determine the best attribute for code based on the sample\n",
    "            code_field = 'code_snippet'\n",
    "            snippet_sample = node_types.get('snippet', {}).get('sample', {})\n",
    "            \n",
    "            if 'code_snippet' in snippet_sample:\n",
    "                code_field = 'code_snippet'\n",
    "            elif 'code' in snippet_sample:\n",
    "                code_field = 'code'\n",
    "            elif 'snippet' in snippet_sample:\n",
    "                code_field = 'snippet'\n",
    "            \n",
    "            aql = f\"\"\"\n",
    "            FOR snippet IN {node_collection}\n",
    "                FILTER snippet.type == 'snippet' AND snippet.{code_field} LIKE '%{symbol_name}%'\n",
    "                LET file = (\n",
    "                    FOR edge IN {edge_collection}\n",
    "                        FILTER edge._to == snippet._id\n",
    "                        FOR file IN {node_collection}\n",
    "                            FILTER file._id == edge._from AND file.type == 'file'\n",
    "                            RETURN {{\n",
    "                                \"key\": file._key,\n",
    "                                \"directory\": file.directory,\n",
    "                                \"file_name\": file.file_name,\n",
    "                                \"file_path\": file.path || (file.directory + '/' + file.file_name),\n",
    "                                \"language\": file.language\n",
    "                            }}\n",
    "                )\n",
    "                RETURN {{\n",
    "                    \"type\": \"snippet\",\n",
    "                    \"code\": snippet.{code_field},\n",
    "                    \"start_line\": snippet.start_line,\n",
    "                    \"end_line\": snippet.end_line,\n",
    "                    \"file\": LENGTH(file) > 0 ? file[0] : null\n",
    "                }}\n",
    "            \"\"\"\n",
    "            cursor = db.aql.execute(aql)\n",
    "            snippet_results = [doc for doc in cursor]\n",
    "            results.extend(snippet_results)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error finding symbol occurrences: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intelligent Symbol Name Search with Type Filtering\n",
    "- Searches for functions, classes, or other symbols by name with optional type filtering\n",
    "- Performs multi-strategy search using both symbol nodes and pattern matching in code snippets\n",
    "- Handles diverse language patterns (Python, JavaScript, Java, C/C++, Go) for accurate detection\n",
    "- Returns comprehensive results with file context, docstrings, and associated code snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_by_name(name: str, symbol_type: Optional[str] = None) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Find function/class snippets by name with improved matching across all files\n",
    "    \n",
    "    Args:\n",
    "        name: The name of the function/class to find\n",
    "        symbol_type: Optional filter for symbol type (e.g., 'function', 'class')\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries containing matching symbols and snippets\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    try:\n",
    "        # Look for symbol nodes first\n",
    "        if 'symbol' in node_types:\n",
    "            type_filter = f\" AND symbol.symbol_type == '{symbol_type}'\" if symbol_type else \"\"\n",
    "            \n",
    "            aql = f\"\"\"\n",
    "            FOR symbol IN {node_collection}\n",
    "                FILTER symbol.type == 'symbol' AND symbol.name == '{name}'{type_filter}\n",
    "                LET file = (\n",
    "                    FOR edge IN {edge_collection}\n",
    "                        FILTER edge._to == symbol._id\n",
    "                        FOR file IN {node_collection}\n",
    "                            FILTER file._id == edge._from AND file.type == 'file'\n",
    "                            RETURN {{\n",
    "                                \"key\": file._key,\n",
    "                                \"directory\": file.directory,\n",
    "                                \"file_name\": file.file_name,\n",
    "                                \"file_path\": file.path || (file.directory + '/' + file.file_name),\n",
    "                                \"language\": file.language\n",
    "                            }}\n",
    "                )\n",
    "                LET snippet = (\n",
    "                    FOR edge IN {edge_collection}\n",
    "                        FILTER edge._from == symbol._id\n",
    "                        FOR snippet IN {node_collection}\n",
    "                            FILTER snippet._id == edge._to AND snippet.type == 'snippet'\n",
    "                            RETURN snippet\n",
    "                )\n",
    "                RETURN {{\n",
    "                    \"type\": \"symbol\",\n",
    "                    \"name\": symbol.name,\n",
    "                    \"symbol_type\": symbol.symbol_type,\n",
    "                    \"line_number\": symbol.line_number,\n",
    "                    \"context\": symbol.context,\n",
    "                    \"docstring\": symbol.docstring,\n",
    "                    \"file\": LENGTH(file) > 0 ? file[0] : null,\n",
    "                    \"snippet\": LENGTH(snippet) > 0 ? snippet[0] : null\n",
    "                }}\n",
    "            \"\"\"\n",
    "            cursor = db.aql.execute(aql)\n",
    "            symbol_results = [doc for doc in cursor]\n",
    "            results.extend(symbol_results)\n",
    "        \n",
    "        # If no symbols found or symbol cache is empty, try fuzzy matching in snippets\n",
    "        if not results and 'snippet' in node_types:\n",
    "            # Determine the best attribute for code based on the sample\n",
    "            code_field = 'code_snippet'\n",
    "            snippet_sample = node_types.get('snippet', {}).get('sample', {})\n",
    "            \n",
    "            if 'code_snippet' in snippet_sample:\n",
    "                code_field = 'code_snippet'\n",
    "            elif 'code' in snippet_sample:\n",
    "                code_field = 'code'\n",
    "            elif 'snippet' in snippet_sample:\n",
    "                code_field = 'snippet'\n",
    "            \n",
    "            # Common patterns for function/class definitions in different languages\n",
    "            patterns = []\n",
    "            \n",
    "            if not symbol_type or symbol_type == 'function':\n",
    "                patterns.extend([\n",
    "                    f\"function {name}\",  # JavaScript\n",
    "                    f\"def {name}\",       # Python\n",
    "                    f\"{name} = function\", # JavaScript\n",
    "                    f\"const {name} = \", # JavaScript arrow function\n",
    "                    f\"let {name} = \",   # JavaScript arrow function\n",
    "                    f\"var {name} = \",   # JavaScript arrow function\n",
    "                    f\"{name}\\\\(\",       # C/C++/Java method\n",
    "                    f\"func {name}\",     # Go\n",
    "                ])\n",
    "            \n",
    "            if not symbol_type or symbol_type == 'class':\n",
    "                patterns.extend([\n",
    "                    f\"class {name}\",     # Python/JavaScript/Java\n",
    "                    f\"interface {name}\", # TypeScript/Java\n",
    "                    f\"struct {name}\",    # C/C++/Go\n",
    "                    f\"type {name} struct\", # Go\n",
    "                ])\n",
    "            \n",
    "            # Create LIKE conditions for each pattern\n",
    "            like_conditions = [f\"snippet.{code_field} LIKE '%{pattern}%'\" for pattern in patterns]\n",
    "            like_filter = \" OR \".join(like_conditions)\n",
    "            \n",
    "            aql = f\"\"\"\n",
    "            FOR snippet IN {node_collection}\n",
    "                FILTER snippet.type == 'snippet' AND ({like_filter})\n",
    "                LET file = (\n",
    "                    FOR edge IN {edge_collection}\n",
    "                        FILTER edge._to == snippet._id\n",
    "                        FOR file IN {node_collection}\n",
    "                            FILTER file._id == edge._from AND file.type == 'file'\n",
    "                            RETURN {{\n",
    "                                \"key\": file._key,\n",
    "                                \"directory\": file.directory,\n",
    "                                \"file_name\": file.file_name,\n",
    "                                \"file_path\": file.path || (file.directory + '/' + file.file_name),\n",
    "                                \"language\": file.language\n",
    "                            }}\n",
    "                )\n",
    "                RETURN {{\n",
    "                    \"type\": \"snippet\",\n",
    "                    \"code\": snippet.{code_field},\n",
    "                    \"start_line\": snippet.start_line,\n",
    "                    \"end_line\": snippet.end_line,\n",
    "                    \"file\": LENGTH(file) > 0 ? file[0] : null\n",
    "                }}\n",
    "            \"\"\"\n",
    "            cursor = db.aql.execute(aql)\n",
    "            snippet_results = [doc for doc in cursor]\n",
    "            results.extend(snippet_results)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error finding by name: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intelligent Symbol Analysis and Discovery\n",
    "- Queries and analyzes implementations of specific functions/classes across a codebase\n",
    "- Searches by name with optional type filtering, collecting all occurrences\n",
    "- Organizes results by file with implementation details (docstrings, line numbers, code snippets)\n",
    "- Returns structured JSON with comprehensive analysis including LLM-enhanced insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_symbol(name: str, symbol_type: Optional[str] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Query about a specific function/class and get an analysis in JSON format.\n",
    "    Will return all implementations across different files.\n",
    "    \n",
    "    Args:\n",
    "        name: The name of the function/class to analyze\n",
    "        symbol_type: Optional filter for symbol type (e.g., 'function', 'class')\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing analysis of the symbol\n",
    "    \"\"\"\n",
    "    # First, find all occurrences\n",
    "    occurrences = find_by_name(name, symbol_type)\n",
    "    \n",
    "    if not occurrences:\n",
    "        return {\"error\": f\"No {symbol_type or 'symbol'} named '{name}' found in the codebase\"}\n",
    "    \n",
    "    # Extract code snippets and organize by file\n",
    "    implementations_by_file = {}\n",
    "    for occurrence in occurrences:\n",
    "        file_info = occurrence.get(\"file\", {})\n",
    "        file_path = file_info.get(\"file_path\", \"unknown_path\")\n",
    "        \n",
    "        if file_path not in implementations_by_file:\n",
    "            implementations_by_file[file_path] = {\n",
    "                \"file_info\": file_info,\n",
    "                \"implementations\": []\n",
    "            }\n",
    "        \n",
    "        if occurrence.get(\"type\") == \"symbol\":\n",
    "            # For symbol occurrence, get its snippet\n",
    "            snippet = occurrence.get(\"snippet\", {})\n",
    "            implementations_by_file[file_path][\"implementations\"].append({\n",
    "                \"type\": occurrence.get(\"symbol_type\", \"unknown\"),\n",
    "                \"name\": occurrence.get(\"name\", name),\n",
    "                \"line_number\": occurrence.get(\"line_number\"),\n",
    "                \"docstring\": occurrence.get(\"docstring\", \"\"),\n",
    "                \"context\": occurrence.get(\"context\", \"\"),\n",
    "                \"code\": snippet.get(\"code_snippet\", snippet.get(\"code\", snippet.get(\"snippet\", \"\")))\n",
    "            })\n",
    "        elif occurrence.get(\"type\") == \"snippet\":\n",
    "            # For snippet occurrence\n",
    "            implementations_by_file[file_path][\"implementations\"].append({\n",
    "                \"type\": symbol_type or \"unknown\",\n",
    "                \"name\": name,\n",
    "                \"line_number\": occurrence.get(\"start_line\"),\n",
    "                \"code\": occurrence.get(\"code\", \"\")\n",
    "            })\n",
    "    \n",
    "    # Use Mistral LLM to analyze the symbol\n",
    "    symbol_analysis = analyze_with_llm(name, symbol_type, implementations_by_file)\n",
    "    \n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"type\": symbol_type or \"unknown\",\n",
    "        \"implementations_count\": len(occurrences),\n",
    "        \"files_count\": len(implementations_by_file),\n",
    "        \"implementations_by_file\": implementations_by_file,\n",
    "        \"analysis\": symbol_analysis\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AI-Powered Code Symbol Analysis\n",
    "- Leverages Mistral LLM to provide intelligent analysis of code symbols (functions, classes)\n",
    "- Extracts and processes code snippets and docstrings from multiple implementations\n",
    "- Generates structured insights including purpose, parameters, dependencies, and complexity\n",
    "- Returns comprehensive JSON analysis with usage patterns and improvement suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_with_llm(name: str, symbol_type: Optional[str], implementations: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Use Mistral API to analyze a symbol based on its implementations\n",
    "    \n",
    "    Args:\n",
    "        name: The name of the symbol to analyze\n",
    "        symbol_type: The type of the symbol (function, class, etc.)\n",
    "        implementations: Dictionary with implementations by file\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with LLM analysis\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract all code snippets from implementations\n",
    "        all_code = []\n",
    "        for file_path, file_data in implementations.items():\n",
    "            for implementation in file_data[\"implementations\"]:\n",
    "                code = implementation.get(\"code\", \"\")\n",
    "                docstring = implementation.get(\"docstring\", \"\")\n",
    "                if code:\n",
    "                    all_code.append(f\"File: {file_path}\\n{code}\")\n",
    "                if docstring:\n",
    "                    all_code.append(f\"Docstring: {docstring}\")\n",
    "        \n",
    "        # Join all code with separators\n",
    "        code_text = \"\\n\\n\" + \"-\" * 40 + \"\\n\\n\".join(all_code)\n",
    "        \n",
    "        # Create a prompt for the LLM\n",
    "        prompt = f\"\"\"\n",
    "        Please analyze this {symbol_type or 'symbol'} named '{name}' from a codebase:\n",
    "        \n",
    "        {code_text}\n",
    "        \n",
    "        Provide a JSON response with the following fields:\n",
    "        1. purpose: A clear description of what this {symbol_type or 'symbol'} does\n",
    "        2. parameters: List of parameters with their types and purpose (if applicable)\n",
    "        3. return_value: What this {symbol_type or 'symbol'} returns (if applicable)\n",
    "        4. dependencies: Other functions/classes/modules it depends on\n",
    "        5. usage_pattern: How this {symbol_type or 'symbol'} is typically used\n",
    "        6. edge_cases: Potential edge cases or error handling\n",
    "        7. complexity: Analysis of time/space complexity (if applicable)\n",
    "        8. suggestions: Any improvements or best practices that could be applied\n",
    "        \n",
    "        Format your response as a valid JSON object without any extra text or markdown.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create message for the LLM\n",
    "        messages = [\n",
    "            ChatMessage(role=\"user\", content=prompt)\n",
    "        ]\n",
    "        \n",
    "        # Get completion from Mistral\n",
    "        chat_response = mistral_client.chat(\n",
    "            model=model,\n",
    "            messages=messages\n",
    "        )\n",
    "        \n",
    "        # Extract the content from the response\n",
    "        content = chat_response.choices[0].message.content\n",
    "        \n",
    "        # Try to parse the response as JSON\n",
    "        try:\n",
    "            analysis = json.loads(content)\n",
    "            return analysis\n",
    "        except json.JSONDecodeError:\n",
    "            # If JSON parsing fails, return the raw text\n",
    "            return {\"raw_analysis\": content}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing with LLM: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return {\"error\": str(e)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intelligent Error Analysis System\n",
    "- Analyzes codebase error messages by extracting keywords and finding related code snippets\n",
    "- Searches for similar error patterns and error handling approaches across the codebase\n",
    "- Leverages Mistral LLM to provide detailed analysis with causes and suggested solutions\n",
    "- Returns structured JSON with error classification, affected components, and preventive measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_error(error_message: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze a specific error message in the codebase and suggest solutions\n",
    "    \n",
    "    Args:\n",
    "        error_message: The error message to analyze\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing error analysis and potential solutions\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # First, search for similar error patterns in the code\n",
    "        # Split error message into keywords\n",
    "        keywords = error_message.lower().split()\n",
    "        keywords = [kw for kw in keywords if len(kw) > 3]  # Filter out short words\n",
    "        \n",
    "        # Create LIKE conditions for each keyword\n",
    "        code_field = 'code_snippet'\n",
    "        snippet_sample = node_types.get('snippet', {}).get('sample', {})\n",
    "        \n",
    "        if 'code_snippet' in snippet_sample:\n",
    "            code_field = 'code_snippet'\n",
    "        elif 'code' in snippet_sample:\n",
    "            code_field = 'code'\n",
    "        elif 'snippet' in snippet_sample:\n",
    "            code_field = 'snippet'\n",
    "            \n",
    "        # Find code snippets that might contain error handling for similar errors\n",
    "        related_snippets = []\n",
    "        \n",
    "        for keyword in keywords:\n",
    "            aql = f\"\"\"\n",
    "            FOR snippet IN {node_collection}\n",
    "                FILTER snippet.type == 'snippet' \n",
    "                AND (\n",
    "                    snippet.{code_field} LIKE '%error%' \n",
    "                    AND snippet.{code_field} LIKE '%{keyword}%'\n",
    "                )\n",
    "                LET file = (\n",
    "                    FOR edge IN {edge_collection}\n",
    "                        FILTER edge._to == snippet._id\n",
    "                        FOR file IN {node_collection}\n",
    "                            FILTER file._id == edge._from AND file.type == 'file'\n",
    "                            RETURN {{\n",
    "                                \"key\": file._key,\n",
    "                                \"file_path\": file.path || (file.directory + '/' + file.file_name)\n",
    "                            }}\n",
    "                )\n",
    "                RETURN {{\n",
    "                    \"code\": snippet.{code_field},\n",
    "                    \"start_line\": snippet.start_line,\n",
    "                    \"end_line\": snippet.end_line,\n",
    "                    \"file\": LENGTH(file) > 0 ? file[0] : null\n",
    "                }}\n",
    "            \"\"\"\n",
    "            cursor = db.aql.execute(aql)\n",
    "            for doc in cursor:\n",
    "                if doc not in related_snippets:\n",
    "                    related_snippets.append(doc)\n",
    "        \n",
    "        # Format snippets for LLM\n",
    "        snippets_text = \"\"\n",
    "        for i, snippet in enumerate(related_snippets):\n",
    "            file_info = snippet.get(\"file\", {})\n",
    "            file_path = file_info.get(\"file_path\", \"unknown\")\n",
    "            code = snippet.get(\"code\", \"\")\n",
    "            \n",
    "            snippets_text += f\"\\nSnippet {i+1} from {file_path}:\\n{code}\\n\"\n",
    "        \n",
    "        # Create a prompt for the LLM\n",
    "        prompt = f\"\"\"\n",
    "        Please analyze this error message from a codebase:\n",
    "        \n",
    "        ```\n",
    "        {error_message}\n",
    "        ```\n",
    "        \n",
    "        I found these potentially related code snippets from the codebase:\n",
    "        {snippets_text if snippets_text else \"No directly related snippets found.\"}\n",
    "        \n",
    "        Provide a JSON response with the following fields:\n",
    "        1. error_type: Classification of this error\n",
    "        2. likely_causes: List of potential causes for this error\n",
    "        3. affected_components: Which parts of the code might be affected\n",
    "        4. solution_suggestions: Specific recommendations to fix this error\n",
    "        5. preventive_measures: How to prevent this type of error in the future\n",
    "        \n",
    "        Format your response as a valid JSON object without any extra text or markdown.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create message for the LLM\n",
    "        messages = [\n",
    "            ChatMessage(role=\"user\", content=prompt)\n",
    "        ]\n",
    "        \n",
    "        # Get completion from Mistral\n",
    "        chat_response = mistral_client.chat(\n",
    "            model=model,\n",
    "            messages=messages\n",
    "        )\n",
    "        \n",
    "        # Extract the content from the response\n",
    "        content = chat_response.choices[0].message.content\n",
    "        \n",
    "        # Try to parse the response as JSON\n",
    "        try:\n",
    "            analysis = json.loads(content)\n",
    "            return {\n",
    "                \"error_message\": error_message,\n",
    "                \"related_snippets_count\": len(related_snippets),\n",
    "                \"analysis\": analysis\n",
    "            }\n",
    "        except json.JSONDecodeError:\n",
    "            # If JSON parsing fails, return the raw text\n",
    "            return {\n",
    "                \"error_message\": error_message,\n",
    "                \"related_snippets_count\": len(related_snippets),\n",
    "                \"raw_analysis\": content\n",
    "            }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing error: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Repository Structure Analyzer\n",
    "- Extracts comprehensive database structure information from code repositories\n",
    "- Maps relationships between node types, files, snippets, and symbols across the codebase\n",
    "- Organizes code elements by language, directory structure, and relationship types\n",
    "- Returns detailed JSON representation of repository architecture for improved navigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_database_structure() -> Dict:\n",
    "    \"\"\"\n",
    "    Answer questions about the database structure\n",
    "    Returns:\n",
    "        Dictionary containing information about the database structure\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Most of this information was already gathered during initialization\n",
    "        # Just format it in a more user-friendly way\n",
    "        # Extract node types with counts\n",
    "        node_types_info = {}\n",
    "        for node_type, info in node_types.items():\n",
    "            node_types_info[node_type] = {\n",
    "                \"count\": info.get(\"count\", 0),\n",
    "                \"properties\": info.get(\"sample_structure\", [])\n",
    "            }\n",
    "            \n",
    "        # Extract relationship types\n",
    "        relationship_types = {}\n",
    "        for rel in db_schema.get(\"Type Relationships\", []):\n",
    "            from_type = rel.get(\"from_type\", \"\")\n",
    "            to_type = rel.get(\"to_type\", \"\")\n",
    "            edge_type = rel.get(\"edge_type\", \"\")\n",
    "            key = f\"{from_type}_to_{to_type}\"\n",
    "            if key not in relationship_types:\n",
    "                relationship_types[key] = {\n",
    "                    \"from_type\": from_type,\n",
    "                    \"to_type\": to_type,\n",
    "                    \"edge_types\": []\n",
    "                }\n",
    "            if edge_type and edge_type not in relationship_types[key][\"edge_types\"]:\n",
    "                relationship_types[key][\"edge_types\"].append(edge_type)\n",
    "                \n",
    "        # Count files by language\n",
    "        languages = {}\n",
    "        for file_info in files.values():\n",
    "            language = file_info.get(\"language\", \"unknown\")\n",
    "            if language not in languages:\n",
    "                languages[language] = 0\n",
    "            languages[language] += 1\n",
    "            \n",
    "        # Build directory structure map for improved path navigation\n",
    "        directory_structure = build_directory_structure()\n",
    "            \n",
    "        return {\n",
    "            \"graph_name\": graph_name,\n",
    "            \"node_collection\": node_collection,\n",
    "            \"edge_collection\": edge_collection,\n",
    "            \"node_types\": node_types_info,\n",
    "            \"relationship_types\": list(relationship_types.values()),\n",
    "            \"file_count\": len(files),\n",
    "            \"snippet_count\": len(snippets),\n",
    "            \"symbol_count\": len(symbols),\n",
    "            \"languages\": languages,\n",
    "            \"directory_structure\": directory_structure\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting database structure: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Codebase Directory Analysis Tool\n",
    "- Analyzes specific directory structures with flexible path matching capabilities\n",
    "- Maps files, code snippets, and symbols contained within target directories\n",
    "- Provides hierarchical directory content visualization for improved navigation\n",
    "- Returns comprehensive JSON with file counts, symbols, and nested directory structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_directory(path: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze a specific directory in the codebase\n",
    "    \n",
    "    Args:\n",
    "        path: Path to directory to analyze\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with directory analysis results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Analyzing code structure at path: {path}\")\n",
    "        \n",
    "        # Normalize path for consistent matching\n",
    "        normalized_path = path.rstrip('/')\n",
    "        \n",
    "        # First try direct path matching for directory nodes\n",
    "        print(f\"Looking for files with path pattern: {normalized_path}\")\n",
    "        \n",
    "        # Query files with matching path prefix\n",
    "        matching_files = []\n",
    "        for file_key, file_info in files.items():\n",
    "            file_path = file_info.get(\"file_path\", \"\")\n",
    "            if file_path and (file_path.startswith(f\"{normalized_path}/\") or file_path == normalized_path):\n",
    "                matching_files.append(file_info)\n",
    "        \n",
    "        # Sort files for consistent output\n",
    "        matching_files.sort(key=lambda x: x.get(\"file_path\", \"\"))\n",
    "        \n",
    "        # Print sample paths for debugging\n",
    "        print(\"Sample file paths in database:\")\n",
    "        for i, file_info in enumerate(list(files.values())[:6]):\n",
    "            print(f\"File {i+1}: {file_info.get('file_path', '')}\")\n",
    "        \n",
    "        # If no files found with direct path matching, try more flexible matching\n",
    "        if not matching_files:\n",
    "            # Try to find files that might contain the path (handle relative paths)\n",
    "            for file_key, file_info in files.items():\n",
    "                file_path = file_info.get(\"file_path\", \"\")\n",
    "                path_parts = normalized_path.split('/')\n",
    "                \n",
    "                # Check if all path parts appear in order in the file path\n",
    "                if file_path:\n",
    "                    file_parts = file_path.split('/')\n",
    "                    for i in range(len(file_parts) - len(path_parts) + 1):\n",
    "                        if file_parts[i:i+len(path_parts)] == path_parts:\n",
    "                            matching_files.append(file_info)\n",
    "                            break\n",
    "            \n",
    "            # Sort again after flexible matching\n",
    "            matching_files.sort(key=lambda x: x.get(\"file_path\", \"\"))\n",
    "        \n",
    "        # Get directory structure\n",
    "        directory_structure = get_directory_contents(normalized_path)\n",
    "        \n",
    "        # Get snippets for matching files\n",
    "        file_keys = [file_info.get(\"key\") for file_info in matching_files]\n",
    "        matching_snippets = []\n",
    "        for snippet_key, snippet_info in snippets.items():\n",
    "            if snippet_info.get(\"file_key\") in file_keys:\n",
    "                matching_snippets.append(snippet_info)\n",
    "        \n",
    "        # Get symbols for matching files\n",
    "        matching_symbols = []\n",
    "        for symbol_key, symbol_info in symbols.items():\n",
    "            if symbol_info.get(\"file_key\") in file_keys:\n",
    "                matching_symbols.append(symbol_info)\n",
    "        \n",
    "        return {\n",
    "            \"path\": normalized_path,\n",
    "            \"files\": matching_files,\n",
    "            \"file_count\": len(matching_files),\n",
    "            \"directory_structure\": directory_structure,\n",
    "            \"snippets_count\": len(matching_snippets),\n",
    "            \"symbols_count\": len(matching_symbols)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing directory: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directory Contents Explorer\n",
    "- Retrieves and organizes file and subdirectory information from specified directory paths\n",
    "- Maps file metadata including language, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directory_contents(path: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Get contents of a specific directory\n",
    "    \n",
    "    Args:\n",
    "        path: Path to directory\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with directory contents\n",
    "    \"\"\"\n",
    "    contents = {\"files\": [], \"subdirectories\": []}\n",
    "    \n",
    "    # Normalize path\n",
    "    normalized_path = path.rstrip('/')\n",
    "    \n",
    "    # Get files directly in this directory\n",
    "    for file_key, file_info in files.items():\n",
    "        file_path = file_info.get(\"file_path\", \"\")\n",
    "        if not file_path:\n",
    "            continue\n",
    "            \n",
    "        file_dir = '/'.join(file_path.split('/')[:-1])\n",
    "        \n",
    "        if file_dir == normalized_path:\n",
    "            contents[\"files\"].append({\n",
    "                \"key\": file_key,\n",
    "                \"name\": file_info.get(\"file_name\", \"\"),\n",
    "                \"path\": file_path,\n",
    "                \"language\": file_info.get(\"language\", \"\")\n",
    "            })\n",
    "    \n",
    "    # Get subdirectories\n",
    "    seen_subdirs = set()\n",
    "    for file_key, file_info in files.items():\n",
    "        file_path = file_info.get(\"file_path\", \"\")\n",
    "        if not file_path or not file_path.startswith(f\"{normalized_path}/\"):\n",
    "            continue\n",
    "            \n",
    "        # Get next directory level\n",
    "        remaining_path = file_path[len(normalized_path)+1:]\n",
    "        if '/' in remaining_path:\n",
    "            subdir = remaining_path.split('/')[0]\n",
    "            subdir_path = f\"{normalized_path}/{subdir}\"\n",
    "            \n",
    "            if subdir_path not in seen_subdirs:\n",
    "                seen_subdirs.add(subdir_path)\n",
    "                contents[\"subdirectories\"].append({\n",
    "                    \"name\": subdir,\n",
    "                    \"path\": subdir_path\n",
    "                })\n",
    "    \n",
    "    return contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Codebase Search Engine\n",
    "- Finds code snippets containing specific search terms across the entire codebase\n",
    "- Dynamically adapts to different database schemas by detecting appropriate code field names\n",
    "- Retrieves complete context including file paths, line numbers, and language information\n",
    "- Returns comprehensive results with full code snippets and their associated file metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_code(term: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Search for code containing a specific term\n",
    "    \n",
    "    Args:\n",
    "        term: The term to search for\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries containing matching code snippets\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    try:\n",
    "        # Determine the best attribute for code based on the sample\n",
    "        code_field = 'code_snippet'\n",
    "        snippet_sample = node_types.get('snippet', {}).get('sample', {})\n",
    "        \n",
    "        if 'code_snippet' in snippet_sample:\n",
    "            code_field = 'code_snippet'\n",
    "        elif 'code' in snippet_sample:\n",
    "            code_field = 'code'\n",
    "        elif 'snippet' in snippet_sample:\n",
    "            code_field = 'snippet'\n",
    "        \n",
    "        aql = f\"\"\"\n",
    "        FOR snippet IN {node_collection}\n",
    "            FILTER snippet.type == 'snippet' AND snippet.{code_field} LIKE '%{term}%'\n",
    "            LET file = (\n",
    "                FOR edge IN {edge_collection}\n",
    "                    FILTER edge._to == snippet._id\n",
    "                    FOR file IN {node_collection}\n",
    "                        FILTER file._id == edge._from AND file.type == 'file'\n",
    "                        RETURN {{\n",
    "                            \"key\": file._key,\n",
    "                            \"directory\": file.directory,\n",
    "                            \"file_name\": file.file_name,\n",
    "                            \"file_path\": file.path || (file.directory + '/' + file.file_name),\n",
    "                            \"language\": file.language\n",
    "                        }}\n",
    "            )\n",
    "            RETURN {{\n",
    "                \"key\": snippet._key,\n",
    "                \"code\": snippet.{code_field},\n",
    "                \"start_line\": snippet.start_line,\n",
    "                \"end_line\": snippet.end_line,\n",
    "                \"file\": LENGTH(file) > 0 ? file[0] : null\n",
    "            }}\n",
    "        \"\"\"\n",
    "        cursor = db.aql.execute(aql)\n",
    "        for doc in cursor:\n",
    "            results.append(doc)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error searching code: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Structure Analysis Tool\n",
    "- Analyzes and visualizes the structure of a codebase for a specific file or directory\n",
    "- Groups files by directory, counts symbols by type, and analyzes language distribution\n",
    "- Leverages Mistral LLM to provide high-level insights about architecture patterns and key components\n",
    "- Returns comprehensive analysis including file counts, directory structure, and improvement recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_code_structure(path: Optional[str] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze and visualize the structure of the code, either for a specific file or directory\n",
    "    \n",
    "    Args:\n",
    "        path: Optional path to focus the analysis on\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing code structure analysis\n",
    "    \"\"\"\n",
    "    print(f\"Analyzing code structure at path: {path}\")\n",
    "    # Print the query you're using to find files\n",
    "    print(f\"Looking for files with path pattern: {path}\")\n",
    "    # Print a few sample files from your cache for comparison\n",
    "    print(\"Sample file paths in database:\")\n",
    "    for i, (key, file_info) in enumerate(files.items()):\n",
    "        print(f\"File {i+1}: {file_info.get('file_path', 'unknown')}\")\n",
    "        if i >= 5:\n",
    "            break\n",
    "            \n",
    "    try:\n",
    "        # If path is provided, filter by that path\n",
    "        path_filter = \"\"\n",
    "        if path:\n",
    "            path_filter = f\" AND (file.path LIKE '{path}/%' OR file.path == '{path}')\"\n",
    "        \n",
    "        # First, gather file structure\n",
    "        aql = f\"\"\"\n",
    "        FOR file IN {node_collection}\n",
    "            FILTER file.type == 'file'{path_filter}\n",
    "            RETURN {{\n",
    "                \"key\": file._key,\n",
    "                \"file_path\": file.path || (file.directory + '/' + file.file_name),\n",
    "                \"language\": file.language\n",
    "            }}\n",
    "        \"\"\"\n",
    "        cursor = db.aql.execute(aql)\n",
    "        files_list = [doc for doc in cursor]\n",
    "        \n",
    "        # Group files by directory\n",
    "        directory_structure = {}\n",
    "        for file in files_list:\n",
    "            file_path = file.get(\"file_path\", \"\")\n",
    "            if not file_path:\n",
    "                continue\n",
    "            \n",
    "            # Split path and use all but the last part as directory\n",
    "            path_parts = file_path.split('/')\n",
    "            if len(path_parts) > 1:\n",
    "                directory = '/'.join(path_parts[:-1])\n",
    "                filename = path_parts[-1]\n",
    "            else:\n",
    "                directory = \".\"\n",
    "                filename = file_path\n",
    "            \n",
    "            if directory not in directory_structure:\n",
    "                directory_structure[directory] = []\n",
    "            \n",
    "            directory_structure[directory].append({\n",
    "                \"file_name\": filename,\n",
    "                \"file_path\": file_path,\n",
    "                \"key\": file.get(\"key\"),\n",
    "                \"language\": file.get(\"language\", \"unknown\")\n",
    "            })\n",
    "        \n",
    "        # Count symbols by type and file\n",
    "        symbol_counts = {}\n",
    "        if 'symbol' in node_types:\n",
    "            path_join = \"\"\n",
    "            if path:\n",
    "                path_join = f\" AND (file.path LIKE '{path}/%' OR file.path == '{path}')\"\n",
    "            \n",
    "            aql = f\"\"\"\n",
    "            FOR symbol IN {node_collection}\n",
    "                FILTER symbol.type == 'symbol'\n",
    "                LET file = (\n",
    "                    FOR edge IN {edge_collection}\n",
    "                        FILTER edge._to == symbol._id\n",
    "                        FOR file IN {node_collection}\n",
    "                            FILTER file._id == edge._from AND file.type == 'file'{path_join}\n",
    "                            RETURN file\n",
    "                )\n",
    "                FILTER LENGTH(file) > 0\n",
    "                COLLECT file_path = file[0].path || (file[0].directory + '/' + file[0].file_name),\n",
    "                        symbol_type = symbol.symbol_type WITH COUNT INTO count\n",
    "                RETURN {{\n",
    "                    \"file_path\": file_path,\n",
    "                    \"symbol_type\": symbol_type,\n",
    "                    \"count\": count\n",
    "                }}\n",
    "            \"\"\"\n",
    "            cursor = db.aql.execute(aql)\n",
    "            for doc in cursor:\n",
    "                file_path = doc.get(\"file_path\", \"\")\n",
    "                symbol_type = doc.get(\"symbol_type\", \"unknown\")\n",
    "                count = doc.get(\"count\", 0)\n",
    "                \n",
    "                if file_path not in symbol_counts:\n",
    "                    symbol_counts[file_path] = {}\n",
    "                \n",
    "                symbol_counts[file_path][symbol_type] = count\n",
    "        \n",
    "        # Prepare analysis data for LLM\n",
    "        file_count = len(files_list)\n",
    "        directory_count = len(directory_structure)\n",
    "        \n",
    "        # Prepare information for visualization\n",
    "        directory_tree = []\n",
    "        for directory, file_list in directory_structure.items():\n",
    "            directory_tree.append({\n",
    "                \"directory\": directory,\n",
    "                \"files\": file_list,\n",
    "                \"file_count\": len(file_list)\n",
    "            })\n",
    "        \n",
    "        # Sort directories by file count (descending)\n",
    "        directory_tree.sort(key=lambda x: x[\"file_count\"], reverse=True)\n",
    "        \n",
    "        # Analyze distribution of languages\n",
    "        language_counts = {}\n",
    "        for file in files_list:\n",
    "            language = file.get(\"language\", \"unknown\")\n",
    "            if language not in language_counts:\n",
    "                language_counts[language] = 0\n",
    "            language_counts[language] += 1\n",
    "        \n",
    "        # Create an analysis with Mistral\n",
    "        if files_list:\n",
    "            structure_info = {\n",
    "                \"file_count\": file_count,\n",
    "                \"directory_count\": directory_count,\n",
    "                \"top_directories\": [d[\"directory\"] for d in directory_tree[:5]],\n",
    "                \"language_distribution\": language_counts,\n",
    "                \"symbol_type_distribution\": symbol_counts\n",
    "            }\n",
    "            \n",
    "            # Create a prompt for the LLM to analyze the structure\n",
    "            prompt = f\"\"\"\n",
    "            Please analyze this codebase structure:\n",
    "            \n",
    "            {json.dumps(structure_info, indent=2)}\n",
    "            \n",
    "            Provide a JSON response with the following fields:\n",
    "            1. overview: High-level description of the codebase structure\n",
    "            2. architecture_patterns: Any architectural patterns you can identify\n",
    "            3. key_components: The most important directories/modules\n",
    "            4. language_insights: Analysis of the programming language usage\n",
    "            5. recommendations: Suggestions for organization or structure improvements\n",
    "            \n",
    "            Format your response as a valid JSON object without any extra text or markdown.\n",
    "            \"\"\"\n",
    "            \n",
    "            # Create message for the LLM\n",
    "            messages = [\n",
    "                ChatMessage(role=\"user\", content=prompt)\n",
    "            ]\n",
    "            \n",
    "            # Get completion from Mistral\n",
    "            chat_response = mistral_client.chat(\n",
    "                model=model,\n",
    "                messages=messages\n",
    "            )\n",
    "            \n",
    "            # Extract the content from the response\n",
    "            content = chat_response.choices[0].message.content\n",
    "            \n",
    "            # Try to parse the response as JSON\n",
    "            try:\n",
    "                analysis = json.loads(content)\n",
    "            except json.JSONDecodeError:\n",
    "                # If JSON parsing fails, return the raw text\n",
    "                analysis = {\"raw_analysis\": content}\n",
    "        else:\n",
    "            analysis = {\"message\": \"No files found matching the specified path\"}\n",
    "        \n",
    "        return {\n",
    "            \"path\": path or \"entire codebase\",\n",
    "            \"file_count\": file_count,\n",
    "            \"directory_count\": directory_count,\n",
    "            \"directory_structure\": directory_tree,\n",
    "            \"language_distribution\": language_counts,\n",
    "            \"symbol_distribution\": symbol_counts,\n",
    "            \"analysis\": analysis\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing code structure: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Natural Language Query Processor for Codebase\n",
    "- Processes natural language queries about the codebase using LLM-powered understanding\n",
    "- Routes queries to appropriate specialized functions (symbol search, error analysis, code structure)\n",
    "- Provides fallback handling when direct matches aren't found in the codebase\n",
    "- Returns comprehensive results with conversational explanations of technical findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(query: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Process natural language queries about the codebase\n",
    "    \n",
    "    Args:\n",
    "        query: Natural language query about the codebase\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing the response to the query\n",
    "    \"\"\"\n",
    "    global conversation_history, mistral_client, model\n",
    "    \n",
    "    try:\n",
    "        # Save the query to conversation history\n",
    "        conversation_history.append({\"role\": \"user\", \"content\": query})\n",
    "        \n",
    "        # Get database structure for context\n",
    "        db_structure = get_database_structure()\n",
    "        \n",
    "        # Create context for the LLM\n",
    "        context = {\n",
    "            \"db_structure\": db_structure,\n",
    "            \"conversation_history\": conversation_history[-5:] if len(conversation_history) > 1 else []\n",
    "        }\n",
    "        \n",
    "        # Create a prompt for the LLM to analyze the query and decide what action to take\n",
    "        prompt = f\"\"\"\n",
    "        You are a codebase assistant that helps users find information in their codebase.\n",
    "        \n",
    "        Database Structure:\n",
    "        {json.dumps(db_structure, indent=2)}\n",
    "        \n",
    "        Available functions:\n",
    "        1. find_symbol_occurrences(symbol_name): Find all occurrences of a symbol\n",
    "        2. find_by_name(name, symbol_type): Find function/class snippets by name\n",
    "        3. analyze_symbol(name, symbol_type): Get detailed analysis of a function/class\n",
    "        4. analyze_error(error_message): Analyze an error message and suggest solutions\n",
    "        5. search_code(term): Search for code containing specific terms\n",
    "        6. analyze_code_structure(path): Analyze the structure of the code\n",
    "        7. analyze_directory(path): Analyze a specific directory in the codebase\n",
    "        \n",
    "        Conversation History:\n",
    "        {json.dumps(context[\"conversation_history\"], indent=2)}\n",
    "        \n",
    "        User Query: {query}\n",
    "        \n",
    "        First, determine what the user is asking and which function would be most appropriate to answer their query.\n",
    "        \n",
    "        Return a JSON response with:\n",
    "        1. understanding: Brief explanation of what you think the user is asking\n",
    "        2. function_to_call: The most appropriate function to call based on the query\n",
    "        3. parameters: Parameters to pass to the function\n",
    "        \n",
    "        Format your response as a valid JSON object without any extra text or markdown.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create message for the LLM\n",
    "        messages = [\n",
    "            ChatMessage(role=\"user\", content=prompt)\n",
    "        ]\n",
    "        \n",
    "        # Get completion from Mistral\n",
    "        chat_response = mistral_client.chat(\n",
    "            model=model,\n",
    "            messages=messages\n",
    "        )\n",
    "        \n",
    "        # Extract the content from the response\n",
    "        content = chat_response.choices[0].message.content\n",
    "        \n",
    "        # Try to parse the response as JSON\n",
    "        try:\n",
    "            # Clean up the content to remove markdown code blocks if present\n",
    "            cleaned_content = content\n",
    "            if content.strip().startswith(\"```\") and content.strip().endswith(\"```\"):\n",
    "                # Extract the content between the backticks\n",
    "                cleaned_content = \"\\n\".join(content.strip().split(\"\\n\")[1:-1])\n",
    "            query_analysis = json.loads(cleaned_content)\n",
    "        except json.JSONDecodeError:\n",
    "            return {\"error\": \"Failed to parse LLM response as JSON\", \"raw_response\": content}\n",
    "        \n",
    "        # Get the function to call and parameters\n",
    "        function_name = query_analysis.get(\"function_to_call\", \"\")\n",
    "        parameters = query_analysis.get(\"parameters\", {})\n",
    "        \n",
    "        # Call the appropriate function based on the analysis\n",
    "        result = None\n",
    "        if function_name == \"find_symbol_occurrences\":\n",
    "            symbol_name = parameters.get(\"symbol_name\", \"\")\n",
    "            if symbol_name:\n",
    "                result = find_symbol_occurrences(symbol_name)\n",
    "        elif function_name == \"find_by_name\":\n",
    "            name = parameters.get(\"name\", \"\")\n",
    "            symbol_type = parameters.get(\"symbol_type\")\n",
    "            if name:\n",
    "                result = find_by_name(name, symbol_type)\n",
    "        elif function_name == \"analyze_symbol\":\n",
    "            name = parameters.get(\"name\", \"\")\n",
    "            symbol_type = parameters.get(\"symbol_type\")\n",
    "            if name:\n",
    "                result = analyze_symbol(name, symbol_type)\n",
    "        elif function_name == \"analyze_error\":\n",
    "            error_message = parameters.get(\"error_message\", \"\")\n",
    "            if error_message:\n",
    "                result = analyze_error(error_message)\n",
    "        elif function_name == \"search_code\":\n",
    "            term = parameters.get(\"term\", \"\")\n",
    "            if term:\n",
    "                result = search_code(term)\n",
    "        elif function_name == \"analyze_code_structure\":\n",
    "            path = parameters.get(\"path\")\n",
    "            result = analyze_code_structure(path)\n",
    "        elif function_name == \"analyze_directory\":\n",
    "            path = parameters.get(\"path\", \"\")\n",
    "            if path:\n",
    "                result = analyze_directory(path)\n",
    "        else:\n",
    "            result = {\"error\": f\"Unknown function: {function_name}\"}\n",
    "        \n",
    "        # If result is None or empty, try to handle the query directly\n",
    "        if result is None or (isinstance(result, list) and len(result) == 0):\n",
    "            # Create a fallback prompt for the LLM\n",
    "            fallback_prompt = f\"\"\"\n",
    "            You are a codebase assistant that helps users find information in their codebase.\n",
    "            \n",
    "            Database Structure:\n",
    "            {json.dumps(db_structure, indent=2)}\n",
    "            \n",
    "            Unfortunately, I couldn't find specific information to answer the user's query:\n",
    "            \n",
    "            User Query: {query}\n",
    "            \n",
    "            Please provide a helpful response based on the general codebase structure.\n",
    "            Your response should:\n",
    "            1. Acknowledge what information is missing\n",
    "            2. Suggest alternative approaches based on the available database structure\n",
    "            3. Ask for any clarification if needed\n",
    "            \n",
    "            Format your response as a conversation, not as JSON.\n",
    "            \"\"\"\n",
    "            \n",
    "            # Create message for the LLM\n",
    "            fallback_messages = [\n",
    "                ChatMessage(role=\"user\", content=fallback_prompt)\n",
    "            ]\n",
    "            \n",
    "            # Get completion from Mistral\n",
    "            fallback_response = mistral_client.chat(\n",
    "                model=model,\n",
    "                messages=fallback_messages\n",
    "            )\n",
    "            \n",
    "            # Extract the content from the response\n",
    "            fallback_content = fallback_response.choices[0].message.content\n",
    "            \n",
    "            # Add the fallback response to conversation history\n",
    "            conversation_history.append({\"role\": \"assistant\", \"content\": fallback_content})\n",
    "            \n",
    "            return {\n",
    "                \"query\": query,\n",
    "                \"understanding\": query_analysis.get(\"understanding\", \"\"),\n",
    "                \"response_type\": \"fallback\",\n",
    "                \"response\": fallback_content\n",
    "            }\n",
    "        \n",
    "        # Generate a user-friendly explanation of the result\n",
    "        explanation_prompt = f\"\"\"\n",
    "        You are a codebase assistant that helps users find information in their codebase.\n",
    "        \n",
    "        User Query: {query}\n",
    "        \n",
    "        Understanding: {query_analysis.get(\"understanding\", \"\")}\n",
    "        \n",
    "        Result: {json.dumps(result, indent=2)}\n",
    "        \n",
    "        Please explain these results to the user in a clear, conversational way.\n",
    "        If results include code snippets, explain what the code does.\n",
    "        If there are multiple results, summarize the key findings.\n",
    "        Include specific details from the results to make your explanation concrete.\n",
    "        \n",
    "        Format your response as a conversation, not as JSON.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create message for the LLM\n",
    "        explanation_messages = [\n",
    "            ChatMessage(role=\"user\", content=explanation_prompt)\n",
    "        ]\n",
    "        \n",
    "        # Get completion from Mistral\n",
    "        explanation_response = mistral_client.chat(\n",
    "            model=model,\n",
    "            messages=explanation_messages\n",
    "        )\n",
    "        \n",
    "        # Extract the content from the response\n",
    "        explanation = explanation_response.choices[0].message.content\n",
    "        \n",
    "        # Add the explanation to conversation history\n",
    "        conversation_history.append({\"role\": \"assistant\", \"content\": explanation})\n",
    "        \n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"understanding\": query_analysis.get(\"understanding\", \"\"),\n",
    "            \"function_called\": function_name,\n",
    "            \"parameters\": parameters,\n",
    "            \"raw_result\": result,\n",
    "            \"explanation\": explanation\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing query: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return {\"error\": str(e)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Codebase Conversational Interface\n",
    "- Serves as the main entry point for users to interact with the codebase through natural language\n",
    "- Processes queries through the query processor and handles different response types\n",
    "- Provides graceful error handling and user-friendly error messages\n",
    "- Returns conversational responses that explain technical codebase information in accessible language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_codebase(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Main conversational function that processes user queries about the codebase\n",
    "    \n",
    "    Args:\n",
    "        query: User's natural language query\n",
    "        \n",
    "    Returns:\n",
    "        String containing the response to the user\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Process the query\n",
    "        result = process_query(query)\n",
    "        \n",
    "        # If an error occurred, return an error message\n",
    "        if \"error\" in result:\n",
    "            error_message = result.get(\"error\", \"An unknown error occurred\")\n",
    "            if \"raw_response\" in result:\n",
    "                return f\"I encountered an error: {error_message}\\n\\nRaw response from LLM: {result['raw_response']}\"\n",
    "            return f\"I encountered an error: {error_message}\"\n",
    "        \n",
    "        # If the result contains an explanation, return it\n",
    "        if \"explanation\" in result:\n",
    "            return result[\"explanation\"]\n",
    "        \n",
    "        # If the result contains a response, return it\n",
    "        if \"response\" in result:\n",
    "            return result[\"response\"]\n",
    "        \n",
    "        # This is a fallback if neither explanation nor response are available\n",
    "        return \"I processed your query but couldn't generate a proper explanation. Please try rephrasing your question.\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in chat_with_codebase: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return f\"I'm sorry, I encountered an error while processing your query: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversation State Management\n",
    "- Resets the conversation history to provide a clean slate for new interactions\n",
    "- Clears accumulated context to prevent interactions from being influenced by previous queries\n",
    "- Helps manage memory usage by removing stored conversation data\n",
    "- Provides users with a way to start fresh conversations about different parts of the codebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_conversation():\n",
    "    \"\"\"Reset the conversation history\"\"\"\n",
    "    global conversation_history\n",
    "    conversation_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Codebase Query System Initializer\n",
    "- Sets up connections to ArangoDB and Mistral API for codebase analysis\n",
    "- Dynamically discovers graph structure and initializes data caches for efficient querying\n",
    "- Configures conversation state and relationship mappings between files, snippets, and symbols\n",
    "- Provides flexible configuration through environment variables and explicit parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_codebase_query(\n",
    "    db_name: str = \"_system\",\n",
    "    username: str = \"root\",\n",
    "    password: str = None,\n",
    "    host: str = None,\n",
    "    mistral_api_key: Optional[str] = None,\n",
    "    model_name: str = \"mistral-large-latest\",\n",
    "    graph: str = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Initialize the codebase query system that dynamically discovers the graph structure.\n",
    "    \n",
    "    Args:\n",
    "        db_name: ArangoDB database name\n",
    "        username: ArangoDB username\n",
    "        password: ArangoDB password\n",
    "        host: ArangoDB host URL\n",
    "        mistral_api_key: Mistral API key (if None, will try to get from environment)\n",
    "        model_name: Mistral model to use\n",
    "        graph: Graph name (if None, will try to discover the first available graph)\n",
    "    \"\"\"\n",
    "    global db, client, mistral_client, model, graph_name, files, snippets, symbols\n",
    "    global db_schema, node_types, symbol_name_index, file_to_snippets, file_to_symbols, snippet_to_symbols\n",
    "    global conversation_history, node_collection, edge_collection\n",
    "    \n",
    "    # Connect to ArangoDB\n",
    "    if not host:\n",
    "        host = os.environ.get(\"ARANGO_HOST\", \"http://localhost:8529\")\n",
    "    client = ArangoClient(hosts=host)\n",
    "    \n",
    "    if not password:\n",
    "        password = os.environ.get(\"ARANGO_PASSWORD\")\n",
    "        if not password:\n",
    "            raise ValueError(\"ArangoDB password not provided and not found in environment\")\n",
    "    \n",
    "    db = client.db(db_name, username=username, password=password)\n",
    "    \n",
    "    # Connect to Mistral API\n",
    "    if mistral_api_key is None:\n",
    "        mistral_api_key = os.environ.get(\"MISTRAL_API_KEY\")\n",
    "    if mistral_api_key is None:\n",
    "        raise ValueError(\"Mistral API key not provided and not found in environment\")\n",
    "    \n",
    "    # Initialize Mistral client\n",
    "    mistral_client = MistralClient(api_key=mistral_api_key)\n",
    "    model = model_name\n",
    "    \n",
    "    # Dynamically discover graph structure\n",
    "    graph_name = graph\n",
    "    \n",
    "    # Initialize data structures\n",
    "    files = {}\n",
    "    snippets = {}\n",
    "    symbols = {}\n",
    "    symbol_name_index = {}\n",
    "    file_to_snippets = {}\n",
    "    file_to_symbols = {}\n",
    "    snippet_to_symbols = {}\n",
    "    conversation_history = []\n",
    "    \n",
    "    # Discover graph structure\n",
    "    discover_graph_structure()\n",
    "    \n",
    "    # Get schema information\n",
    "    db_schema = get_db_schema()\n",
    "    \n",
    "    # Analyze node types\n",
    "    node_types = analyze_node_types()\n",
    "    \n",
    "    # Initialize cache\n",
    "    initialize_cache()\n",
    "    \n",
    "    print(\"Codebase query system initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Codebase Query System Usage Example\n",
    "- Demonstrates practical implementation in a Jupyter notebook with environment variable configuration\n",
    "- Shows complete workflow from initialization to querying and conversation management\n",
    "- Includes environment loading, system initialization with credentials, and graph specification\n",
    "- Provides a concrete example of querying for error handling functions and conversation reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No edge definitions found, using default naming pattern\n",
      "Using default collections: Nodes=FlaskRepv1_node, Edges=FlaskRepv1_node_to_FlaskRepv1_node\n",
      "Found type field: type\n",
      "Found edge type field: edge_type\n",
      "Schema validation complete: type_field=type, path_field=None, edge_type_field=edge_type\n",
      "Type: directory, Count: 69\n",
      "Type: file, Count: 83\n",
      "Type: snippet, Count: 930\n",
      "Type: symbol, Count: 1893\n",
      "Cached 83 files\n",
      "Cached 930 code snippets\n",
      "Using 'context' as fallback for symbol name field\n",
      "Using name_field: context, type_field: type\n",
      "Symbol query: \n",
      "                FOR v IN FlaskRepv1_node\n",
      "                    FILTER v.type == 'symbol'\n",
      "                    RETURN v\n",
      "                \n",
      "Sample symbol count: 1893\n",
      "Sample symbol fields: ['_key', '_id', '_rev', 'type', 'symbol_type', 'line_number', 'context', 'docstring']\n",
      "Sample symbol name value: def test_index(client, auth):\n",
      "    response = client.get(\"/\")\n",
      "    assert b\"Log In\" in response.data\n",
      "    assert b\"Register\" in response.data\n",
      "\n",
      "    auth.login()\n",
      "    response = client.get(\"/\")\n",
      "    assert b\"test title\" in response.data\n",
      "    assert b\"by test on 2018-01-01\" in response.data\n",
      "    assert b\"test\\nbody\" in response.data\n",
      "    assert b'href=\"/1/update\"' in response.data\n",
      "Sample symbol type value: function\n",
      "Processed 200 symbols so far\n",
      "Processed 400 symbols so far\n",
      "Processed 600 symbols so far\n",
      "Processed 800 symbols so far\n",
      "Processed 1000 symbols so far\n",
      "Processed 1200 symbols so far\n",
      "Processed 1400 symbols so far\n",
      "Processed 1600 symbols so far\n",
      "Processed 1800 symbols so far\n",
      "Cached 1893 symbols\n",
      "Built relationship indexes for files, snippets, and symbols\n",
      "Codebase query system initialized successfully\n",
      "Error processing query: 'MistralClient' object has no attribute '_default_model'\n",
      "I encountered an error: 'MistralClient' object has no attribute '_default_model'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/_2/xx5z8xdj6j98wh4vt2b59jz80000gp/T/ipykernel_49116/3271334447.py\", line 63, in process_query\n",
      "    chat_response = mistral_client.chat(\n",
      "  File \"/Users/Viku/GitHub/scopium/.venv/lib/python3.10/site-packages/mistralai/client.py\", line 202, in chat\n",
      "    request = self._make_chat_request(\n",
      "  File \"/Users/Viku/GitHub/scopium/.venv/lib/python3.10/site-packages/mistralai/client_base.py\", line 178, in _make_chat_request\n",
      "    request_data[\"model\"] = self._get_model(model)\n",
      "  File \"/Users/Viku/GitHub/scopium/.venv/lib/python3.10/site-packages/mistralai/client_base.py\", line 50, in _get_model\n",
      "    if self._default_model is None:\n",
      "AttributeError: 'MistralClient' object has no attribute '_default_model'\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "graph = \"FlaskRepv1\"\n",
    "\n",
    "# Initialize the system\n",
    "initialize_codebase_query(\n",
    "    db_name=os.getenv(\"ARANGO_DB_NAME\"),\n",
    "    username=os.getenv(\"ARANGO_USERNAME\"),\n",
    "    password=os.getenv(\"ARANGO_PASSWORD\"),\n",
    "    host=os.getenv(\"ARANGO_HOST\"),\n",
    "    mistral_api_key=os.getenv(\"MISTRAL_API_KEY\"),\n",
    "    model_name=os.getenv(\"MISTRAL_MODEL_NAME\"),\n",
    "    graph=graph\n",
    ")\n",
    "\n",
    "# Ask a question about the codebase\n",
    "query = \"Explain me the has_level_handler function in my codebase\"\n",
    "response = chat_with_codebase(query)\n",
    "print(response)\n",
    "\n",
    "# Reset the conversation if needed\n",
    "reset_conversation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix 1\n",
    "- Class for visualising the graph made using a .html file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import ast\n",
    "# import networkx as nx\n",
    "# from typing import Dict, Set, List, Tuple, Optional, Union\n",
    "# import json\n",
    "# from arango import ArangoClient\n",
    "# import re\n",
    "# import glob\n",
    "\n",
    "# class CodebaseVisualizer:\n",
    "#     def __init__(self, root_dir: str, supported_languages=None):\n",
    "#         self.root_dir = root_dir\n",
    "#         self.graph = nx.DiGraph()\n",
    "#         self.file_contents: Dict[str, str] = {}\n",
    "#         self.import_relations: Dict[str, List[Tuple[str, int]]] = {}  # file -> [(module, line_no)]\n",
    "#         self.module_symbols: Dict[str, Dict[str, Dict[str, any]]] = {}  # file -> {symbol -> {type, line_no, context}}\n",
    "#         self.symbol_references: Dict[str, List[Tuple[str, int, str]]] = {}  # symbol -> [(file, line_no, context)]\n",
    "#         self.file_index: Dict[str, int] = {}  # Maps files to indices\n",
    "#         self.current_index = 0\n",
    "#         self.directories: Set[str] = set()\n",
    "#         # Add a new index for all symbols to quickly locate them\n",
    "#         self.symbol_index: Dict[str, List[Dict]] = {}  # symbol -> [{file, type, line_no, context}]\n",
    "        \n",
    "#         # Define supported languages\n",
    "#         self.supported_languages = supported_languages or [\"python\", \"cpp\", \"java\", \"go\"]\n",
    "        \n",
    "#         # Language file extensions mapping\n",
    "#         self.language_extensions = {\n",
    "#             \"python\": [\".py\"],\n",
    "#             \"cpp\": [\".c\", \".cpp\", \".h\", \".hpp\", \".cc\", \".cxx\", \".hxx\"],\n",
    "#             \"java\": [\".java\"],\n",
    "#             \"go\": [\".go\"]\n",
    "#         }\n",
    "\n",
    "#     def _get_next_index(self) -> int:\n",
    "#         \"\"\"Get next available index for file indexing.\"\"\"\n",
    "#         self.current_index += 1\n",
    "#         return self.current_index\n",
    "\n",
    "#     def _chunk_code(self, code: str, lines_per_chunk: int = 20) -> List[Dict]:\n",
    "#         \"\"\"\n",
    "#         Chunk the given code into snippets.\n",
    "#         Returns a list of dictionaries with 'code_snippet', 'start_line', and 'end_line'.\n",
    "#         \"\"\"\n",
    "#         lines = code.splitlines()\n",
    "#         chunks = []\n",
    "#         for i in range(0, len(lines), lines_per_chunk):\n",
    "#             chunk_lines = lines[i:i + lines_per_chunk]\n",
    "#             chunk = {\n",
    "#                 'code_snippet': '\\n'.join(chunk_lines),\n",
    "#                 'start_line': i + 1,\n",
    "#                 'end_line': i + len(chunk_lines)\n",
    "#             }\n",
    "#             chunks.append(chunk)\n",
    "#         return chunks\n",
    "\n",
    "#     def _get_context_around_line(self, file_path: str, line_no: int, context_lines: int = 3) -> str:\n",
    "#         \"\"\"Extract context around a specific line in a file.\"\"\"\n",
    "#         if file_path not in self.file_contents:\n",
    "#             return \"\"\n",
    "        \n",
    "#         lines = self.file_contents[file_path].splitlines()\n",
    "#         start = max(0, line_no - context_lines - 1)\n",
    "#         end = min(len(lines), line_no + context_lines)\n",
    "        \n",
    "#         context = \"\\n\".join(lines[start:end])\n",
    "#         return context\n",
    "\n",
    "#     def _detect_language(self, file_path: str) -> str:\n",
    "#         \"\"\"Detect the programming language of a file based on its extension.\"\"\"\n",
    "#         _, ext = os.path.splitext(file_path)\n",
    "#         ext = ext.lower()\n",
    "        \n",
    "#         for language, extensions in self.language_extensions.items():\n",
    "#             if ext in extensions:\n",
    "#                 return language\n",
    "                \n",
    "#         return \"unknown\"\n",
    "\n",
    "#     def parse_files(self) -> None:\n",
    "#         \"\"\"Parse all files in the directory and build relationships.\"\"\"\n",
    "#         # First pass: Index all files and create directory nodes\n",
    "#         for root, dirs, files in os.walk(self.root_dir):\n",
    "#             # Add directory node\n",
    "#             rel_dir = os.path.relpath(root, self.root_dir)\n",
    "#             if rel_dir != '.':\n",
    "#                 self.directories.add(rel_dir)\n",
    "#                 self.graph.add_node(rel_dir, type='directory')\n",
    "                \n",
    "#                 # Add edge from parent directory to this directory (if not root)\n",
    "#                 parent_dir = os.path.dirname(rel_dir)\n",
    "#                 if parent_dir and parent_dir != '.':\n",
    "#                     self.graph.add_edge(parent_dir, rel_dir, edge_type='contains_directory')\n",
    "\n",
    "#             # Index files of supported languages\n",
    "#             for file in files:\n",
    "#                 file_path = os.path.join(root, file)\n",
    "#                 rel_path = os.path.relpath(file_path, self.root_dir)\n",
    "#                 file_language = self._detect_language(file_path)\n",
    "                \n",
    "#                 if file_language in self.supported_languages:\n",
    "#                     self.file_index[rel_path] = self._get_next_index()\n",
    "                    \n",
    "#                     # Add node for this file\n",
    "#                     self.graph.add_node(rel_path, type='file', file_index=self.file_index[rel_path], language=file_language)\n",
    "                    \n",
    "#                     # Connect file to its directory\n",
    "#                     if rel_dir != '.':\n",
    "#                         self.graph.add_edge(rel_dir, rel_path, edge_type='contains_file')\n",
    "                    \n",
    "#                     try:\n",
    "#                         with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "#                             content = f.read()\n",
    "#                             self.file_contents[rel_path] = content\n",
    "#                             self._analyze_file(rel_path, content, file_language)\n",
    "#                     except Exception as e:\n",
    "#                         print(f\"Error parsing {file_path}: {e}\")\n",
    "        \n",
    "#         # Second pass: Find symbol references across files\n",
    "#         for file_path, content in self.file_contents.items():\n",
    "#             file_language = self._detect_language(file_path)\n",
    "#             self._find_references_in_file(file_path, content, file_language)\n",
    "        \n",
    "#         # Build the symbol index after all analyses\n",
    "#         self._build_symbol_index()\n",
    "\n",
    "#     def _analyze_file(self, file_path: str, content: str, language: str) -> None:\n",
    "#         \"\"\"Analyze a file for imports and symbols with line numbers and context.\"\"\"\n",
    "#         if language == \"python\":\n",
    "#             self._analyze_python_file(file_path, content)\n",
    "#         elif language == \"cpp\":\n",
    "#             self._analyze_cpp_file(file_path, content)\n",
    "#         elif language == \"java\":\n",
    "#             self._analyze_java_file(file_path, content)\n",
    "#         elif language == \"go\":\n",
    "#             self._analyze_go_file(file_path, content)\n",
    "\n",
    "#     def _analyze_python_file(self, file_path: str, content: str) -> None:\n",
    "#         \"\"\"Analyze a Python file for imports and symbols.\"\"\"\n",
    "#         try:\n",
    "#             tree = ast.parse(content)\n",
    "#             imports = []\n",
    "#             symbols = {}\n",
    "\n",
    "#             for node in ast.walk(tree):\n",
    "#                 # Track imports\n",
    "#                 if isinstance(node, (ast.Import, ast.ImportFrom)):\n",
    "#                     if isinstance(node, ast.Import):\n",
    "#                         for name in node.names:\n",
    "#                             imports.append((name.name, node.lineno))\n",
    "#                     else:  # ImportFrom\n",
    "#                         module = node.module if node.module else ''\n",
    "#                         for name in node.names:\n",
    "#                             imports.append((f\"{module}.{name.name}\" if module else name.name, node.lineno))\n",
    "\n",
    "#                 # Track defined symbols with line numbers and context\n",
    "#                 elif isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.Assign)):\n",
    "#                     if isinstance(node, (ast.FunctionDef, ast.ClassDef)):\n",
    "#                         symbol_name = node.name\n",
    "#                         symbol_type = 'class' if isinstance(node, ast.ClassDef) else 'function'\n",
    "#                         line_no = node.lineno\n",
    "#                         context = self._extract_python_node_source(content, node)\n",
    "                        \n",
    "#                         symbols[symbol_name] = {\n",
    "#                             'type': symbol_type,\n",
    "#                             'line_no': line_no,\n",
    "#                             'context': context,\n",
    "#                             'docstring': ast.get_docstring(node)\n",
    "#                         }\n",
    "#                     elif isinstance(node, ast.Assign):\n",
    "#                         # Handle variable assignments\n",
    "#                         for target in node.targets:\n",
    "#                             if isinstance(target, ast.Name):\n",
    "#                                 symbol_name = target.id\n",
    "#                                 line_no = node.lineno\n",
    "#                                 context = self._extract_python_node_source(content, node)\n",
    "                                \n",
    "#                                 symbols[symbol_name] = {\n",
    "#                                     'type': 'variable',\n",
    "#                                     'line_no': line_no,\n",
    "#                                     'context': context\n",
    "#                                 }\n",
    "\n",
    "#             self.import_relations[file_path] = imports\n",
    "#             self.module_symbols[file_path] = symbols\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error analyzing Python file {file_path}: {e}\")\n",
    "\n",
    "#     def _extract_python_node_source(self, source: str, node) -> str:\n",
    "#         \"\"\"Extract the source code for a Python AST node.\"\"\"\n",
    "#         try:\n",
    "#             lines = source.splitlines()\n",
    "#             if hasattr(node, 'lineno') and hasattr(node, 'end_lineno'):\n",
    "#                 start = node.lineno - 1\n",
    "#                 end = getattr(node, 'end_lineno', start + 1)\n",
    "#                 return '\\n'.join(lines[start:end])\n",
    "#             return \"\"\n",
    "#         except Exception:\n",
    "#             return \"\"\n",
    "\n",
    "#     def _analyze_cpp_file(self, file_path: str, content: str) -> None:\n",
    "#         \"\"\"Analyze a C/C++ file for includes and symbols.\"\"\"\n",
    "#         imports = []\n",
    "#         symbols = {}\n",
    "        \n",
    "#         # Process content line by line\n",
    "#         lines = content.splitlines()\n",
    "        \n",
    "#         # Regular expressions for C/C++ code analysis\n",
    "#         include_pattern = re.compile(r'#include\\s+[<\"]([^>\"]+)[>\"]')\n",
    "#         class_pattern = re.compile(r'(?:class|struct)\\s+(\\w+)')\n",
    "#         function_pattern = re.compile(r'(\\w+)\\s*\\([^)]*\\)\\s*(?:const|override|final|noexcept)?\\s*(?:{|;)')\n",
    "#         namespace_pattern = re.compile(r'namespace\\s+(\\w+)')\n",
    "        \n",
    "#         for line_no, line in enumerate(lines, 1):\n",
    "#             # Find include statements\n",
    "#             include_match = include_pattern.search(line)\n",
    "#             if include_match:\n",
    "#                 imports.append((include_match.group(1), line_no))\n",
    "            \n",
    "#             # Find class/struct definitions\n",
    "#             class_match = class_pattern.search(line)\n",
    "#             if class_match:\n",
    "#                 class_name = class_match.group(1)\n",
    "#                 context = self._get_context_around_line(file_path, line_no)\n",
    "#                 symbols[class_name] = {\n",
    "#                     'type': 'class',\n",
    "#                     'line_no': line_no,\n",
    "#                     'context': context\n",
    "#                 }\n",
    "            \n",
    "#             # Find function definitions (simplified)\n",
    "#             function_match = function_pattern.search(line)\n",
    "#             if function_match and not line.strip().startswith('#') and not line.strip().startswith('//'):\n",
    "#                 function_name = function_match.group(1)\n",
    "#                 # Skip some common keywords that might be mistaken for functions\n",
    "#                 if function_name not in ['if', 'while', 'for', 'switch', 'return']:\n",
    "#                     context = self._get_context_around_line(file_path, line_no)\n",
    "#                     symbols[function_name] = {\n",
    "#                         'type': 'function',\n",
    "#                         'line_no': line_no,\n",
    "#                         'context': context\n",
    "#                     }\n",
    "            \n",
    "#             # Find namespace definitions\n",
    "#             namespace_match = namespace_pattern.search(line)\n",
    "#             if namespace_match:\n",
    "#                 namespace_name = namespace_match.group(1)\n",
    "#                 context = self._get_context_around_line(file_path, line_no)\n",
    "#                 symbols[namespace_name] = {\n",
    "#                     'type': 'namespace',\n",
    "#                     'line_no': line_no,\n",
    "#                     'context': context\n",
    "#                 }\n",
    "        \n",
    "#         self.import_relations[file_path] = imports\n",
    "#         self.module_symbols[file_path] = symbols\n",
    "\n",
    "#     def _analyze_java_file(self, file_path: str, content: str) -> None:\n",
    "#         \"\"\"Analyze a Java file for imports and symbols.\"\"\"\n",
    "#         imports = []\n",
    "#         symbols = {}\n",
    "        \n",
    "#         # Process content line by line\n",
    "#         lines = content.splitlines()\n",
    "        \n",
    "#         # Regular expressions for Java code analysis\n",
    "#         package_pattern = re.compile(r'package\\s+([\\w.]+)')\n",
    "#         import_pattern = re.compile(r'import\\s+([\\w.]+(?:\\.\\*)?)')\n",
    "#         class_pattern = re.compile(r'(?:public|private|protected)?\\s*(?:abstract|final)?\\s*class\\s+(\\w+)')\n",
    "#         interface_pattern = re.compile(r'(?:public|private|protected)?\\s*interface\\s+(\\w+)')\n",
    "#         method_pattern = re.compile(r'(?:public|private|protected)?\\s*(?:static|final|abstract)?\\s*(?:[\\w<>[\\],\\s]+)\\s+(\\w+)\\s*\\([^)]*\\)')\n",
    "        \n",
    "#         for line_no, line in enumerate(lines, 1):\n",
    "#             # Find package declaration\n",
    "#             package_match = package_pattern.search(line)\n",
    "#             if package_match:\n",
    "#                 package_name = package_match.group(1)\n",
    "#                 imports.append((package_name, line_no))\n",
    "            \n",
    "#             # Find import statements\n",
    "#             import_match = import_pattern.search(line)\n",
    "#             if import_match:\n",
    "#                 import_name = import_match.group(1)\n",
    "#                 imports.append((import_name, line_no))\n",
    "            \n",
    "#             # Find class definitions\n",
    "#             class_match = class_pattern.search(line)\n",
    "#             if class_match:\n",
    "#                 class_name = class_match.group(1)\n",
    "#                 context = self._get_context_around_line(file_path, line_no)\n",
    "#                 symbols[class_name] = {\n",
    "#                     'type': 'class',\n",
    "#                     'line_no': line_no,\n",
    "#                     'context': context\n",
    "#                 }\n",
    "            \n",
    "#             # Find interface definitions\n",
    "#             interface_match = interface_pattern.search(line)\n",
    "#             if interface_match:\n",
    "#                 interface_name = interface_match.group(1)\n",
    "#                 context = self._get_context_around_line(file_path, line_no)\n",
    "#                 symbols[interface_name] = {\n",
    "#                     'type': 'interface',\n",
    "#                     'line_no': line_no,\n",
    "#                     'context': context\n",
    "#                 }\n",
    "            \n",
    "#             # Find method definitions\n",
    "#             method_match = method_pattern.search(line)\n",
    "#             if method_match:\n",
    "#                 method_name = method_match.group(1)\n",
    "#                 # Skip some common keywords that might be mistaken for methods\n",
    "#                 if method_name not in ['if', 'while', 'for', 'switch', 'return']:\n",
    "#                     context = self._get_context_around_line(file_path, line_no)\n",
    "#                     symbols[method_name] = {\n",
    "#                         'type': 'method',\n",
    "#                         'line_no': line_no,\n",
    "#                         'context': context\n",
    "#                     }\n",
    "        \n",
    "#         self.import_relations[file_path] = imports\n",
    "#         self.module_symbols[file_path] = symbols\n",
    "\n",
    "#     def _analyze_go_file(self, file_path: str, content: str) -> None:\n",
    "#         \"\"\"Analyze a Go file for imports and symbols.\"\"\"\n",
    "#         imports = []\n",
    "#         symbols = {}\n",
    "        \n",
    "#         # Process content line by line\n",
    "#         lines = content.splitlines()\n",
    "        \n",
    "#         # Regular expressions for Go code analysis\n",
    "#         package_pattern = re.compile(r'package\\s+(\\w+)')\n",
    "#         import_single_pattern = re.compile(r'import\\s+\"([^\"]+)\"')\n",
    "#         import_multi_start_pattern = re.compile(r'import\\s+\\(')\n",
    "#         import_multi_line_pattern = re.compile(r'\\s*\"([^\"]+)\"')\n",
    "#         func_pattern = re.compile(r'func\\s+(?:\\([^)]+\\)\\s+)?(\\w+)')\n",
    "#         struct_pattern = re.compile(r'type\\s+(\\w+)\\s+struct')\n",
    "#         interface_pattern = re.compile(r'type\\s+(\\w+)\\s+interface')\n",
    "        \n",
    "#         in_import_block = False\n",
    "        \n",
    "#         for line_no, line in enumerate(lines, 1):\n",
    "#             # Find package declaration\n",
    "#             package_match = package_pattern.search(line)\n",
    "#             if package_match:\n",
    "#                 package_name = package_match.group(1)\n",
    "#                 imports.append((f\"package {package_name}\", line_no))\n",
    "            \n",
    "#             # Handle single-line imports\n",
    "#             import_match = import_single_pattern.search(line)\n",
    "#             if import_match:\n",
    "#                 import_name = import_match.group(1)\n",
    "#                 imports.append((import_name, line_no))\n",
    "            \n",
    "#             # Handle multi-line imports\n",
    "#             if import_multi_start_pattern.search(line):\n",
    "#                 in_import_block = True\n",
    "#                 continue\n",
    "            \n",
    "#             if in_import_block:\n",
    "#                 if line.strip() == ')':\n",
    "#                     in_import_block = False\n",
    "#                     continue\n",
    "                    \n",
    "#                 import_line_match = import_multi_line_pattern.search(line)\n",
    "#                 if import_line_match:\n",
    "#                     import_name = import_line_match.group(1)\n",
    "#                     imports.append((import_name, line_no))\n",
    "            \n",
    "#             # Find function definitions\n",
    "#             func_match = func_pattern.search(line)\n",
    "#             if func_match:\n",
    "#                 func_name = func_match.group(1)\n",
    "#                 context = self._get_context_around_line(file_path, line_no)\n",
    "#                 symbols[func_name] = {\n",
    "#                     'type': 'function',\n",
    "#                     'line_no': line_no,\n",
    "#                     'context': context\n",
    "#                 }\n",
    "            \n",
    "#             # Find struct definitions\n",
    "#             struct_match = struct_pattern.search(line)\n",
    "#             if struct_match:\n",
    "#                 struct_name = struct_match.group(1)\n",
    "#                 context = self._get_context_around_line(file_path, line_no)\n",
    "#                 symbols[struct_name] = {\n",
    "#                     'type': 'struct',\n",
    "#                     'line_no': line_no,\n",
    "#                     'context': context\n",
    "#                 }\n",
    "            \n",
    "#             # Find interface definitions\n",
    "#             interface_match = interface_pattern.search(line)\n",
    "#             if interface_match:\n",
    "#                 interface_name = interface_match.group(1)\n",
    "#                 context = self._get_context_around_line(file_path, line_no)\n",
    "#                 symbols[interface_name] = {\n",
    "#                     'type': 'interface',\n",
    "#                     'line_no': line_no,\n",
    "#                     'context': context\n",
    "#                 }\n",
    "        \n",
    "#         self.import_relations[file_path] = imports\n",
    "#         self.module_symbols[file_path] = symbols\n",
    "\n",
    "#     def _find_references_in_file(self, file_path: str, content: str, language: str) -> None:\n",
    "#         \"\"\"Find references to symbols in a file based on its language.\"\"\"\n",
    "#         if language == \"python\":\n",
    "#             self._find_references_in_python_file(file_path, content)\n",
    "#         elif language == \"cpp\":\n",
    "#             self._find_references_in_cpp_file(file_path, content)\n",
    "#         elif language == \"java\":\n",
    "#             self._find_references_in_java_file(file_path, content)\n",
    "#         elif language == \"go\":\n",
    "#             self._find_references_in_go_file(file_path, content)\n",
    "\n",
    "#     def _find_references_in_python_file(self, file_path: str, content: str) -> None:\n",
    "#         \"\"\"Find references to symbols in a Python file.\"\"\"\n",
    "#         try:\n",
    "#             tree = ast.parse(content)\n",
    "            \n",
    "#             for node in ast.walk(tree):\n",
    "#                 # Find variable references\n",
    "#                 if isinstance(node, ast.Name) and isinstance(node.ctx, ast.Load):\n",
    "#                     symbol_name = node.id\n",
    "#                     line_no = node.lineno\n",
    "                    \n",
    "#                     # Track reference with context\n",
    "#                     if symbol_name not in self.symbol_references:\n",
    "#                         self.symbol_references[symbol_name] = []\n",
    "                    \n",
    "#                     context = self._get_context_around_line(file_path, line_no)\n",
    "#                     self.symbol_references[symbol_name].append((file_path, line_no, context))\n",
    "                \n",
    "#                 # Find attribute references (e.g., obj.method())\n",
    "#                 elif isinstance(node, ast.Attribute) and isinstance(node.ctx, ast.Load):\n",
    "#                     attr_name = node.attr\n",
    "#                     line_no = node.lineno\n",
    "                    \n",
    "#                     if attr_name not in self.symbol_references:\n",
    "#                         self.symbol_references[attr_name] = []\n",
    "                    \n",
    "#                     context = self._get_context_around_line(file_path, line_no)\n",
    "#                     self.symbol_references[attr_name].append((file_path, line_no, context))\n",
    "        \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error finding references in Python file {file_path}: {e}\")\n",
    "\n",
    "#     def _find_references_in_cpp_file(self, file_path: str, content: str) -> None:\n",
    "#         \"\"\"Find references to symbols in a C/C++ file.\"\"\"\n",
    "#         # Get all symbol names from all files to check for references\n",
    "#         all_symbols = set()\n",
    "#         for symbols_dict in self.module_symbols.values():\n",
    "#             all_symbols.update(symbols_dict.keys())\n",
    "        \n",
    "#         # Process content line by line\n",
    "#         lines = content.splitlines()\n",
    "        \n",
    "#         for line_no, line in enumerate(lines, 1):\n",
    "#             # Look for references to any known symbol\n",
    "#             for symbol_name in all_symbols:\n",
    "#                 # Simple pattern matching (would be more robust with proper C++ parsing)\n",
    "#                 pattern = r'\\b' + re.escape(symbol_name) + r'\\b'\n",
    "#                 if re.search(pattern, line):\n",
    "#                     # Check if this is not the definition line\n",
    "#                     if (file_path in self.module_symbols and \n",
    "#                         symbol_name in self.module_symbols[file_path] and \n",
    "#                         self.module_symbols[file_path][symbol_name]['line_no'] == line_no):\n",
    "#                         continue\n",
    "                    \n",
    "#                     if symbol_name not in self.symbol_references:\n",
    "#                         self.symbol_references[symbol_name] = []\n",
    "                    \n",
    "#                     context = self._get_context_around_line(file_path, line_no)\n",
    "#                     self.symbol_references[symbol_name].append((file_path, line_no, context))\n",
    "\n",
    "#     def _find_references_in_java_file(self, file_path: str, content: str) -> None:\n",
    "#         \"\"\"Find references to symbols in a Java file.\"\"\"\n",
    "#         # Get all symbol names from all files to check for references\n",
    "#         all_symbols = set()\n",
    "#         for symbols_dict in self.module_symbols.values():\n",
    "#             all_symbols.update(symbols_dict.keys())\n",
    "        \n",
    "#         # Process content line by line\n",
    "#         lines = content.splitlines()\n",
    "        \n",
    "#         for line_no, line in enumerate(lines, 1):\n",
    "#             # Skip comment lines and import/package declarations\n",
    "#             if (line.strip().startswith(\"//\") or \n",
    "#                 line.strip().startswith(\"/*\") or \n",
    "#                 line.strip().startswith(\"import \") or \n",
    "#                 line.strip().startswith(\"package \")):\n",
    "#                 continue\n",
    "            \n",
    "#             # Look for references to any known symbol\n",
    "#             for symbol_name in all_symbols:\n",
    "#                 # Simple pattern matching with word boundaries\n",
    "#                 pattern = r'\\b' + re.escape(symbol_name) + r'\\b'\n",
    "#                 if re.search(pattern, line):\n",
    "#                     # Check if this is not the definition line\n",
    "#                     if (file_path in self.module_symbols and \n",
    "#                         symbol_name in self.module_symbols[file_path] and \n",
    "#                         self.module_symbols[file_path][symbol_name]['line_no'] == line_no):\n",
    "#                         continue\n",
    "                    \n",
    "#                     if symbol_name not in self.symbol_references:\n",
    "#                         self.symbol_references[symbol_name] = []\n",
    "                    \n",
    "#                     context = self._get_context_around_line(file_path, line_no)\n",
    "#                     self.symbol_references[symbol_name].append((file_path, line_no, context))\n",
    "\n",
    "#     def _find_references_in_go_file(self, file_path: str, content: str) -> None:\n",
    "#         \"\"\"Find references to symbols in a Go file.\"\"\"\n",
    "#         # Get all symbol names from all files to check for references\n",
    "#         all_symbols = set()\n",
    "#         for symbols_dict in self.module_symbols.values():\n",
    "#             all_symbols.update(symbols_dict.keys())\n",
    "        \n",
    "#         # Process content line by line\n",
    "#         lines = content.splitlines()\n",
    "        \n",
    "#         for line_no, line in enumerate(lines, 1):\n",
    "#             # Skip comment lines and import/package declarations\n",
    "#             if (line.strip().startswith(\"//\") or \n",
    "#                 line.strip().startswith(\"/*\") or \n",
    "#                 line.strip().startswith(\"import \") or \n",
    "#                 line.strip().startswith(\"package \")):\n",
    "#                 continue\n",
    "            \n",
    "#             # Look for references to any known symbol\n",
    "#             for symbol_name in all_symbols:\n",
    "#                 # Simple pattern matching with word boundaries\n",
    "#                 pattern = r'\\b' + re.escape(symbol_name) + r'\\b'\n",
    "#                 if re.search(pattern, line):\n",
    "#                     # Check if this is not the definition line\n",
    "#                     if (file_path in self.module_symbols and \n",
    "#                         symbol_name in self.module_symbols[file_path] and \n",
    "#                         self.module_symbols[file_path][symbol_name]['line_no'] == line_no):\n",
    "#                         continue\n",
    "                    \n",
    "#                     if symbol_name not in self.symbol_references:\n",
    "#                         self.symbol_references[symbol_name] = []\n",
    "                    \n",
    "#                     context = self._get_context_around_line(file_path, line_no)\n",
    "#                     self.symbol_references[symbol_name].append((file_path, line_no, context))\n",
    "\n",
    "#     def _build_symbol_index(self) -> None:\n",
    "#         \"\"\"Build a comprehensive index of all symbols and where they're defined/used.\"\"\"\n",
    "#         # Initialize the symbol index\n",
    "#         self.symbol_index = {}\n",
    "        \n",
    "#         # First, add all symbol definitions\n",
    "#         for file_path, symbols in self.module_symbols.items():\n",
    "#             for symbol_name, details in symbols.items():\n",
    "#                 if symbol_name not in self.symbol_index:\n",
    "#                     self.symbol_index[symbol_name] = []\n",
    "                \n",
    "#                 self.symbol_index[symbol_name].append({\n",
    "#                     'file': file_path,\n",
    "#                     'type': 'definition',\n",
    "#                     'symbol_type': details['type'],\n",
    "#                     'line_no': details['line_no'],\n",
    "#                     'context': details.get('context', ''),\n",
    "#                     'docstring': details.get('docstring', '')\n",
    "#                 })\n",
    "        \n",
    "#         # Then, add all references\n",
    "#         for symbol_name, references in self.symbol_references.items():\n",
    "#             if symbol_name not in self.symbol_index:\n",
    "#                 self.symbol_index[symbol_name] = []\n",
    "            \n",
    "#             for file_path, line_no, context in references:\n",
    "#                 # Avoid duplicating references if they're already in definitions\n",
    "#                 if not any(ref['file'] == file_path and ref['line_no'] == line_no and ref['type'] == 'definition' \n",
    "#                           for ref in self.symbol_index.get(symbol_name, [])):\n",
    "#                     self.symbol_index[symbol_name].append({\n",
    "#                         'file': file_path,\n",
    "#                         'type': 'reference',\n",
    "#                         'line_no': line_no,\n",
    "#                         'context': context\n",
    "#                     })\n",
    "\n",
    "#     def build_graph(self) -> nx.DiGraph:\n",
    "#         \"\"\"Build the NetworkX graph with enhanced node and edge information.\"\"\"\n",
    "#         # We've already added basic file and directory nodes during parsing\n",
    "#         # Now add more detailed connections and data\n",
    "        \n",
    "#         # Add nodes for all directories (if not already added)\n",
    "#         for directory in self.directories:\n",
    "#             if not self.graph.has_node(directory):\n",
    "#                 self.graph.add_node(directory, type='directory')\n",
    "            \n",
    "#             # Ensure parent directories exist and are connected\n",
    "#             parts = directory.split(os.sep)\n",
    "#             for i in range(1, len(parts)):\n",
    "#                 parent_path = os.sep.join(parts[:i])\n",
    "#                 if parent_path and not self.graph.has_node(parent_path):\n",
    "#                     self.graph.add_node(parent_path, type='directory')\n",
    "#                     self.directories.add(parent_path)\n",
    "                \n",
    "#                 # Connect parent to child directory\n",
    "#                 if parent_path:\n",
    "#                     child_path = os.sep.join(parts[:i+1])\n",
    "#                     self.graph.add_edge(parent_path, child_path, edge_type='contains_directory')\n",
    "        \n",
    "#         # Add nodes for all files with indices and code snippet nodes\n",
    "#         for file_path, file_idx in self.file_index.items():\n",
    "#             language = self._detect_language(file_path)\n",
    "            \n",
    "#             # Update file node if it exists, create it otherwise\n",
    "#             if self.graph.has_node(file_path):\n",
    "#                 self.graph.nodes[file_path].update({\n",
    "#                     'file_index': file_idx,\n",
    "#                     'directory': os.path.dirname(file_path),\n",
    "#                     'language': language\n",
    "#                 })\n",
    "#             else:\n",
    "#                 self.graph.add_node(file_path, \n",
    "#                                    type='file',\n",
    "#                                    file_index=file_idx,\n",
    "#                                    directory=os.path.dirname(file_path),\n",
    "#                                    language=language)\n",
    "            \n",
    "#             # Connect file to its directory\n",
    "#             directory = os.path.dirname(file_path)\n",
    "#             if directory:\n",
    "#                 # Make sure the directory node exists\n",
    "#                 if not self.graph.has_node(directory):\n",
    "#                     self.graph.add_node(directory, type='directory')\n",
    "#                     self.directories.add(directory)\n",
    "                \n",
    "#                 # Add edge from directory to file if it doesn't exist\n",
    "#                 if not self.graph.has_edge(directory, file_path):\n",
    "#                     self.graph.add_edge(directory, file_path, edge_type='contains_file')\n",
    "            \n",
    "#             # Create snippet nodes for the entire file\n",
    "#             if file_path in self.file_contents:\n",
    "#                 chunks = self._chunk_code(self.file_contents[file_path])\n",
    "#                 for idx, chunk_info in enumerate(chunks):\n",
    "#                     snippet_node = f\"{file_path}::snippet::{idx}\"\n",
    "#                     self.graph.add_node(snippet_node,\n",
    "#                                        type='snippet',\n",
    "#                                        code_snippet=chunk_info['code_snippet'],\n",
    "#                                        start_line=chunk_info['start_line'],\n",
    "#                                        end_line=chunk_info['end_line'],\n",
    "#                                        language=language)\n",
    "#                     # Connect file node to snippet node\n",
    "#                     self.graph.add_edge(file_path, snippet_node, \n",
    "#                                        edge_type='contains_snippet',\n",
    "#                                        start_line=chunk_info['start_line'],\n",
    "#                                        end_line=chunk_info['end_line'])\n",
    "\n",
    "#             # Add nodes for symbols in this file\n",
    "#             for symbol, details in self.module_symbols.get(file_path, {}).items():\n",
    "#                 symbol_node = f\"{file_path}::{symbol}\"\n",
    "#                 self.graph.add_node(symbol_node, \n",
    "#                                    type='symbol',\n",
    "#                                    symbol_type=details['type'],\n",
    "#                                    line_number=details['line_no'],\n",
    "#                                    context=details.get('context', ''),\n",
    "#                                    docstring=details.get('docstring', ''))\n",
    "#                 self.graph.add_edge(file_path, symbol_node, \n",
    "#                                    edge_type='defines',\n",
    "#                                    line_number=details['line_no'])\n",
    "\n",
    "#         # Add edges for imports with line numbers\n",
    "#         for file_path, imports in self.import_relations.items():\n",
    "#             for imp, line_no in imports:\n",
    "#                 # Look for matching files or symbols\n",
    "#                 for target_file, symbols in self.module_symbols.items():\n",
    "#                     if imp in symbols:\n",
    "#                         self.graph.add_edge(file_path, \n",
    "#                                            f\"{target_file}::{imp}\",\n",
    "#                                            edge_type='import',\n",
    "#                                            line_number=line_no)\n",
    "#                     # For Python, handle module imports\n",
    "#                     elif self._detect_language(file_path) == \"python\" and target_file.replace('.py', '').endswith(imp):\n",
    "#                         self.graph.add_edge(file_path, \n",
    "#                                            target_file,\n",
    "#                                            edge_type='import',\n",
    "#                                            line_number=line_no)\n",
    "#                     # For Java, handle package imports\n",
    "#                     elif self._detect_language(file_path) == \"java\" and imp.startswith(os.path.splitext(os.path.basename(target_file))[0]):\n",
    "#                         self.graph.add_edge(file_path, \n",
    "#                                            target_file,\n",
    "#                                            edge_type='import',\n",
    "#                                            line_number=line_no)\n",
    "        \n",
    "#         # Add edges for symbol references\n",
    "#         for symbol, references in self.symbol_references.items():\n",
    "#             for file_path, line_no, context in references:\n",
    "#                 # Find symbol nodes that match this reference\n",
    "#                 for target_file, symbols in self.module_symbols.items():\n",
    "#                     if symbol in symbols:\n",
    "#                         # Create reference edge\n",
    "#                         self.graph.add_edge(file_path, \n",
    "#                                            f\"{target_file}::{symbol}\",\n",
    "#                                            edge_type='references',\n",
    "#                                            line_number=line_no,\n",
    "#                                            context=context)\n",
    "        \n",
    "#         return self.graph\n",
    "\n",
    "#     def export_to_arango(self, url: str, username: str, password: str, db_name: str = \"codebase\", \n",
    "#                          graph_name: str = \"Custom_Flask\", node_collection: str = \"nodes\", \n",
    "#                          edge_collection: str = \"edges\", overwrite: bool = False) -> None:\n",
    "#         \"\"\"\n",
    "#         Export the NetworkX graph to ArangoDB.\n",
    "        \n",
    "#         Args:\n",
    "#             url: ArangoDB server URL\n",
    "#             username: ArangoDB username\n",
    "#             password: ArangoDB password\n",
    "#             db_name: Database name\n",
    "#             graph_name: Graph name\n",
    "#             node_collection: Node collection name\n",
    "#             edge_collection: Edge collection name\n",
    "#             overwrite: Whether to overwrite existing database\n",
    "#         \"\"\"\n",
    "#         # Initialize ArangoDB client\n",
    "#         client = ArangoClient(hosts=url)\n",
    "#         sys_db = client.db('_system', username=username, password=password)\n",
    "        \n",
    "#         # Create or use existing database\n",
    "#         if sys_db.has_database(db_name):\n",
    "#             if overwrite:\n",
    "#                 sys_db.delete_database(db_name)\n",
    "#                 sys_db.create_database(db_name)\n",
    "#                 print(f\"Database '{db_name}' recreated.\")\n",
    "#             else:\n",
    "#                 print(f\"Using existing database '{db_name}'.\")\n",
    "#         else:\n",
    "#             sys_db.create_database(db_name)\n",
    "#             print(f\"Database '{db_name}' created.\")\n",
    "        \n",
    "#         # Connect to the database\n",
    "#         db = client.db(db_name, username=username, password=password)\n",
    "        \n",
    "#         # Create or use existing collections\n",
    "#         if db.has_collection(node_collection):\n",
    "#             nodes = db.collection(node_collection)\n",
    "#             nodes.truncate()\n",
    "#         else:\n",
    "#             nodes = db.create_collection(node_collection)\n",
    "        \n",
    "#         if db.has_collection(edge_collection):\n",
    "#             edges = db.collection(edge_collection)\n",
    "#             edges.truncate()\n",
    "#         else:\n",
    "#             edges = db.create_edge_collection(edge_collection)\n",
    "        \n",
    "#         # Create or use existing graph\n",
    "#         if db.has_graph(graph_name):\n",
    "#             graph = db.graph(graph_name)\n",
    "#         else:\n",
    "#             graph = db.create_graph(graph_name)\n",
    "#             # Define edge definition\n",
    "#             graph.create_edge_definition(\n",
    "#                 edge_collection=edge_collection,\n",
    "#                 from_vertex_collections=[node_collection],\n",
    "#                 to_vertex_collections=[node_collection]\n",
    "#             )\n",
    "        \n",
    "#         # Prepare nodes for ArangoDB (ensuring unique IDs)\n",
    "#         node_mapping = {}  # Maps node names to ArangoDB keys\n",
    "        \n",
    "#         # Add nodes to ArangoDB\n",
    "#         print(\"Adding nodes to ArangoDB...\")\n",
    "#         for node_name, node_attrs in self.graph.nodes(data=True):\n",
    "#             # Create a sanitized key for ArangoDB\n",
    "#             key = re.sub(r'[^a-zA-Z0-9_\\-]', '_', node_name)\n",
    "#             node_mapping[node_name] = key\n",
    "            \n",
    "#             # Include all attributes and the original node name\n",
    "#             node_data = {\n",
    "#                 '_key': key,\n",
    "#                 'original_name': node_name\n",
    "#             }\n",
    "#             node_data.update(node_attrs)\n",
    "            \n",
    "#             # Handle special data types for ArangoDB\n",
    "#             for attr, value in node_data.items():\n",
    "#                 if isinstance(value, (set, tuple)):\n",
    "#                     node_data[attr] = list(value)\n",
    "            \n",
    "#             # Insert the node\n",
    "#             nodes.insert(node_data)\n",
    "        \n",
    "#         # Add edges to ArangoDB\n",
    "#         print(\"Adding edges to ArangoDB...\")\n",
    "#         for src, dst, edge_attrs in self.graph.edges(data=True):\n",
    "#             # Create edge with proper from/to\n",
    "#             edge_data = {\n",
    "#                 '_from': f\"{node_collection}/{node_mapping[src]}\",\n",
    "#                 '_to': f\"{node_collection}/{node_mapping[dst]}\"\n",
    "#             }\n",
    "#             edge_data.update(edge_attrs)\n",
    "            \n",
    "#             # Handle special data types for ArangoDB\n",
    "#             for attr, value in edge_data.items():\n",
    "#                 if isinstance(value, (set, tuple)):\n",
    "#                     edge_data[attr] = list(value)\n",
    "            \n",
    "#             # Insert the edge\n",
    "#             edges.insert(edge_data)\n",
    "        \n",
    "#         print(f\"Exported graph to ArangoDB: {len(self.graph.nodes())} nodes and {len(self.graph.edges())} edges.\")\n",
    "\n",
    "#     def query_database(self, url: str, username: str, password: str, db_name: str = \"codebase\", \n",
    "#                       query: str = None) -> List[Dict]:\n",
    "#         \"\"\"\n",
    "#         Execute a query against the ArangoDB database.\n",
    "        \n",
    "#         Args:\n",
    "#             url: ArangoDB server URL\n",
    "#             username: ArangoDB username\n",
    "#             password: ArangoDB password\n",
    "#             db_name: Database name\n",
    "#             query: AQL query string\n",
    "            \n",
    "#         Returns:\n",
    "#             Query results as a list of dictionaries\n",
    "#         \"\"\"\n",
    "#         client = ArangoClient(hosts=url)\n",
    "#         db = client.db(db_name, username=username, password=password)\n",
    "        \n",
    "#         if query is None:\n",
    "#             # Default query to get basic statistics\n",
    "#             query = \"\"\"\n",
    "#             RETURN {\n",
    "#                 \"node_count\": LENGTH(FOR v IN nodes RETURN v),\n",
    "#                 \"edge_count\": LENGTH(FOR e IN edges RETURN e),\n",
    "#                 \"file_count\": LENGTH(FOR v IN nodes FILTER v.type == 'file' RETURN v),\n",
    "#                 \"directory_count\": LENGTH(FOR v IN nodes FILTER v.type == 'directory' RETURN v),\n",
    "#                 \"symbol_count\": LENGTH(FOR v IN nodes FILTER v.type == 'symbol' RETURN v)\n",
    "#             }\n",
    "#             \"\"\"\n",
    "        \n",
    "#         cursor = db.aql.execute(query)\n",
    "#         return [doc for doc in cursor]\n",
    "\n",
    "#     def export_to_json(self, output_path: str) -> None:\n",
    "#         \"\"\"\n",
    "#         Export the graph data to a JSON file for backup or analysis outside ArangoDB.\n",
    "        \n",
    "#         Args:\n",
    "#             output_path: Path to write the JSON file\n",
    "#         \"\"\"\n",
    "#         data = {\n",
    "#             \"nodes\": [],\n",
    "#             \"edges\": []\n",
    "#         }\n",
    "        \n",
    "#         # Export nodes\n",
    "#         for node_name, attrs in self.graph.nodes(data=True):\n",
    "#             node_data = {\"id\": node_name}\n",
    "#             node_data.update(attrs)\n",
    "#             data[\"nodes\"].append(node_data)\n",
    "        \n",
    "#         # Export edges\n",
    "#         for src, dst, attrs in self.graph.edges(data=True):\n",
    "#             edge_data = {\n",
    "#                 \"source\": src,\n",
    "#                 \"target\": dst\n",
    "#             }\n",
    "#             edge_data.update(attrs)\n",
    "#             data[\"edges\"].append(edge_data)\n",
    "        \n",
    "#         # Write to file\n",
    "#         with open(output_path, 'w', encoding='utf-8') as f:\n",
    "#             json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "#         print(f\"Exported graph to JSON file: {output_path}\")\n",
    "\n",
    "#     def analyze_codebase(self) -> Dict[str, any]:\n",
    "#         \"\"\"\n",
    "#         Perform basic analysis on the codebase and return statistics.\n",
    "        \n",
    "#         Returns:\n",
    "#             Dictionary with analysis results\n",
    "#         \"\"\"\n",
    "#         stats = {\n",
    "#             \"total_files\": len(self.file_index),\n",
    "#             \"total_directories\": len(self.directories),\n",
    "#             \"total_symbols\": sum(len(symbols) for symbols in self.module_symbols.values()),\n",
    "#             \"languages\": {},\n",
    "#             \"file_sizes\": {\n",
    "#                 \"min\": float('inf'),\n",
    "#                 \"max\": 0,\n",
    "#                 \"avg\": 0\n",
    "#             },\n",
    "#             \"symbol_types\": {}\n",
    "#         }\n",
    "        \n",
    "#         # Count files by language\n",
    "#         for file_path in self.file_index:\n",
    "#             lang = self._detect_language(file_path)\n",
    "#             stats[\"languages\"][lang] = stats[\"languages\"].get(lang, 0) + 1\n",
    "            \n",
    "#             # Track file sizes\n",
    "#             file_size = len(self.file_contents.get(file_path, \"\"))\n",
    "#             stats[\"file_sizes\"][\"min\"] = min(stats[\"file_sizes\"][\"min\"], file_size)\n",
    "#             stats[\"file_sizes\"][\"max\"] = max(stats[\"file_sizes\"][\"max\"], file_size)\n",
    "        \n",
    "#         # Calculate average file size\n",
    "#         if stats[\"total_files\"] > 0:\n",
    "#             total_size = sum(len(content) for content in self.file_contents.values())\n",
    "#             stats[\"file_sizes\"][\"avg\"] = total_size / stats[\"total_files\"]\n",
    "#         else:\n",
    "#             stats[\"file_sizes\"][\"min\"] = 0\n",
    "        \n",
    "#         # Count symbols by type\n",
    "#         for symbols in self.module_symbols.values():\n",
    "#             for symbol, details in symbols.items():\n",
    "#                 symbol_type = details.get(\"type\", \"unknown\")\n",
    "#                 stats[\"symbol_types\"][symbol_type] = stats[\"symbol_types\"].get(symbol_type, 0) + 1\n",
    "        \n",
    "#         return stats\n",
    "\n",
    "#     def run_workflow(self, code_path: str, arango_url: str, username: str, password: str, \n",
    "#                     db_name: str = \"codebase\") -> Dict:\n",
    "#         \"\"\"\n",
    "#         Run the complete workflow: parse files, build graph, export to ArangoDB, and analyze.\n",
    "        \n",
    "#         Args:\n",
    "#             code_path: Path to the codebase\n",
    "#             arango_url: ArangoDB server URL\n",
    "#             username: ArangoDB username\n",
    "#             password: ArangoDB password\n",
    "#             db_name: Database name\n",
    "            \n",
    "#         Returns:\n",
    "#             Analysis results\n",
    "#         \"\"\"\n",
    "#         print(f\"Processing codebase at: {code_path}\")\n",
    "        \n",
    "#         # Parse files\n",
    "#         self.parse_files()\n",
    "#         print(f\"Parsed {len(self.file_index)} files and {len(self.directories)} directories\")\n",
    "        \n",
    "#         # Build graph\n",
    "#         self.build_graph()\n",
    "#         print(f\"Built graph with {len(self.graph.nodes())} nodes and {len(self.graph.edges())} edges\")\n",
    "        \n",
    "#         # Export to ArangoDB\n",
    "#         self.export_to_arango(\n",
    "#             url=arango_url,\n",
    "#             username=username,\n",
    "#             password=password,\n",
    "#             db_name=db_name,\n",
    "#             overwrite=True\n",
    "#         )\n",
    "        \n",
    "#         # Analyze codebase\n",
    "#         analysis = self.analyze_codebase()\n",
    "#         print(f\"Analysis complete: {analysis}\")\n",
    "        \n",
    "#         return analysis\n",
    "    \n",
    "#     def validate_graph_and_data(self) -> dict:\n",
    "#         \"\"\"\n",
    "#         Validate the parsed data and graph construction.\n",
    "#         Returns a detailed report on what was found and potential issues.\n",
    "#         \"\"\"\n",
    "#         report = {\n",
    "#             \"files\": {\n",
    "#                 \"count\": len(self.file_index),\n",
    "#                 \"samples\": list(self.file_index.keys())[:5],  # First 5 files\n",
    "#                 \"extensions\": {}\n",
    "#             },\n",
    "#             \"directories\": {\n",
    "#                 \"count\": len(self.directories),\n",
    "#                 \"samples\": list(self.directories)[:5]  # First 5 directories\n",
    "#             },\n",
    "#             \"symbols\": {\n",
    "#                 \"count\": sum(len(symbols) for symbols in self.module_symbols.values()),\n",
    "#                 \"by_type\": {},\n",
    "#                 \"samples\": []\n",
    "#             },\n",
    "#             \"graph\": {\n",
    "#                 \"nodes\": self.graph.number_of_nodes(),\n",
    "#                 \"edges\": self.graph.number_of_edges(),\n",
    "#                 \"node_types\": {},\n",
    "#                 \"edge_types\": {}\n",
    "#             },\n",
    "#             \"possible_issues\": []\n",
    "#         }\n",
    "        \n",
    "#         # Check file extensions\n",
    "#         for file_path in self.file_index:\n",
    "#             _, ext = os.path.splitext(file_path)\n",
    "#             ext = ext.lower()\n",
    "#             report[\"files\"][\"extensions\"][ext] = report[\"files\"][\"extensions\"].get(ext, 0) + 1\n",
    "        \n",
    "#         # Check for supported extensions\n",
    "#         supported_exts = []\n",
    "#         for lang, exts in self.language_extensions.items():\n",
    "#             supported_exts.extend(exts)\n",
    "        \n",
    "#         if set(report[\"files\"][\"extensions\"].keys()).isdisjoint(supported_exts):\n",
    "#             report[\"possible_issues\"].append(\"No files with supported extensions found.\")\n",
    "        \n",
    "#         # Check symbol types\n",
    "#         for file_path, symbols in self.module_symbols.items():\n",
    "#             for symbol_name, details in symbols.items():\n",
    "#                 symbol_type = details['type']\n",
    "#                 report[\"symbols\"][\"by_type\"][symbol_type] = report[\"symbols\"][\"by_type\"].get(symbol_type, 0) + 1\n",
    "                \n",
    "#                 if len(report[\"symbols\"][\"samples\"]) < 5:\n",
    "#                     report[\"symbols\"][\"samples\"].append({\n",
    "#                         \"name\": symbol_name,\n",
    "#                         \"file\": file_path,\n",
    "#                         \"type\": symbol_type,\n",
    "#                         \"line\": details['line_no']\n",
    "#                     })\n",
    "        \n",
    "#         # Check graph node and edge types\n",
    "#         for _, data in self.graph.nodes(data=True):\n",
    "#             node_type = data.get('type', 'unknown')\n",
    "#             report[\"graph\"][\"node_types\"][node_type] = report[\"graph\"][\"node_types\"].get(node_type, 0) + 1\n",
    "        \n",
    "#         for _, _, data in self.graph.edges(data=True):\n",
    "#             edge_type = data.get('edge_type', 'unknown')\n",
    "#             report[\"graph\"][\"edge_types\"][edge_type] = report[\"graph\"][\"edge_types\"].get(edge_type, 0) + 1\n",
    "        \n",
    "#         # Check if nodes match files and directories\n",
    "#         if report[\"graph\"][\"node_types\"].get(\"file\", 0) != report[\"files\"][\"count\"]:\n",
    "#             report[\"possible_issues\"].append(\n",
    "#                 f\"Mismatch between file count ({report['files']['count']}) and file nodes in graph ({report['graph']['node_types'].get('file', 0)})\"\n",
    "#             )\n",
    "        \n",
    "#         if report[\"graph\"][\"node_types\"].get(\"directory\", 0) != report[\"directories\"][\"count\"]:\n",
    "#             report[\"possible_issues\"].append(\n",
    "#                 f\"Mismatch between directory count ({report['directories']['count']}) and directory nodes in graph ({report['graph']['node_types'].get('directory', 0)})\"\n",
    "#             )\n",
    "        \n",
    "#         # Check if symbols have corresponding nodes\n",
    "#         symbol_count = report[\"symbols\"][\"count\"]\n",
    "#         symbol_nodes = report[\"graph\"][\"node_types\"].get(\"symbol\", 0)\n",
    "#         if symbol_count != symbol_nodes:\n",
    "#             report[\"possible_issues\"].append(\n",
    "#                 f\"Mismatch between symbol count ({symbol_count}) and symbol nodes in graph ({symbol_nodes})\"\n",
    "#             )\n",
    "        \n",
    "#         # Validate directory structure\n",
    "#         if report[\"directories\"][\"count\"] > 0 and report[\"files\"][\"count\"] > 0:\n",
    "#             # Check if files are connected to their directories\n",
    "#             contains_file_edges = report[\"graph\"][\"edge_types\"].get(\"contains_file\", 0)\n",
    "#             if contains_file_edges < report[\"files\"][\"count\"]:\n",
    "#                 report[\"possible_issues\"].append(\n",
    "#                     f\"Some files may not be properly connected to their directories ({contains_file_edges} edges for {report['files']['count']} files)\"\n",
    "#                 )\n",
    "        \n",
    "#         return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyvis.network import Network\n",
    "\n",
    "# def visualize_codebase_graph(codebase_path, output_html=\"codebase_graph.html\", limit_nodes=None):\n",
    "#     \"\"\"\n",
    "#     Visualize the codebase graph using pyvis.\n",
    "    \n",
    "#     Args:\n",
    "#         codebase_path: Path to the codebase directory\n",
    "#         output_html: Output HTML file for the visualization/\n",
    "#         limit_nodes: Optional limit on the number of nodes to display (for large codebases)\n",
    "#     \"\"\"\n",
    "#     # Initialize and build the graph\n",
    "#     visualizer = CodebaseVisualizer(codebase_path)\n",
    "#     visualizer.parse_files()\n",
    "#     graph = visualizer.build_graph()\n",
    "    \n",
    "#     # Export graph to JSON (optional)\n",
    "#     visualizer.export_graph_json('codebase_graph.json')\n",
    "    \n",
    "#     # Create a pyvis network\n",
    "#     net = Network(height=\"900px\", width=\"100%\", bgcolor=\"#222222\", font_color=\"white\")\n",
    "    \n",
    "#     # Configure physics\n",
    "#     net.barnes_hut(gravity=-5000, central_gravity=0.3, spring_length=200)\n",
    "    \n",
    "#     # Define node groups and colors\n",
    "#     node_colors = {\n",
    "#         'file': '#4287f5',\n",
    "#         'directory': '#42f5a7',\n",
    "#         'symbol': '#f542cb',\n",
    "#         'snippet': '#f5a742'\n",
    "#     }\n",
    "    \n",
    "#     # If we need to limit nodes for performance\n",
    "#     if limit_nodes and len(graph.nodes()) > limit_nodes:\n",
    "#         # Focus on file and directory nodes, and limit symbol nodes\n",
    "#         important_nodes = [node for node, data in graph.nodes(data=True) \n",
    "#                           if data.get('type') in ['file', 'directory']]\n",
    "        \n",
    "#         # Add some symbol nodes to reach the limit\n",
    "#         symbols = [node for node, data in graph.nodes(data=True) \n",
    "#                   if data.get('type') == 'symbol']\n",
    "        \n",
    "#         # Take a subset of symbols based on connectivity\n",
    "#         symbol_importance = sorted(\n",
    "#             [(node, graph.degree(node)) for node in symbols],\n",
    "#             key=lambda x: x[1],\n",
    "#             reverse=True\n",
    "#         )\n",
    "        \n",
    "#         important_symbols = [node for node, _ in symbol_importance[:limit_nodes - len(important_nodes)]]\n",
    "#         selected_nodes = important_nodes + important_symbols\n",
    "        \n",
    "#         # Create a subgraph\n",
    "#         graph = graph.subgraph(selected_nodes)\n",
    "    \n",
    "#     # Add nodes with appropriate styles\n",
    "#     for node, node_data in graph.nodes(data=True):\n",
    "#         node_type = node_data.get('type', 'unknown')\n",
    "#         label = os.path.basename(node) if '/' in node else node\n",
    "        \n",
    "#         # Truncate very long labels\n",
    "#         if len(label) > 30:\n",
    "#             label = label[:27] + \"...\"\n",
    "        \n",
    "#         # Create hover title with more details\n",
    "#         title = f\"<div style='max-width:300px;'>\"\n",
    "#         title += f\"<b>{node}</b><br>\"\n",
    "#         title += f\"Type: {node_type}<br>\"\n",
    "        \n",
    "#         if node_type == 'file':\n",
    "#             title += f\"Directory: {node_data.get('directory', 'N/A')}<br>\"\n",
    "            \n",
    "#         elif node_type == 'symbol':\n",
    "#             title += f\"Symbol type: {node_data.get('symbol_type', 'N/A')}<br>\"\n",
    "#             title += f\"Line: {node_data.get('line_number', 'N/A')}<br>\"\n",
    "            \n",
    "#             if node_data.get('docstring'):\n",
    "#                 docstring = node_data['docstring']\n",
    "#                 if len(docstring) > 200:\n",
    "#                     docstring = docstring[:197] + \"...\"\n",
    "#                 title += f\"Docstring: {docstring}<br>\"\n",
    "                \n",
    "#         elif node_type == 'snippet':\n",
    "#             title += f\"Lines: {node_data.get('start_line', 'N/A')}-{node_data.get('end_line', 'N/A')}<br>\"\n",
    "            \n",
    "#             if node_data.get('code_snippet'):\n",
    "#                 snippet = node_data['code_snippet'].replace('\\n', '<br>')\n",
    "#                 if len(snippet) > 300:\n",
    "#                     snippet = snippet[:297] + \"...\"\n",
    "#                 title += f\"Code:<br><pre>{snippet}</pre>\"\n",
    "                \n",
    "#         title += \"</div>\"\n",
    "        \n",
    "#         # Determine node size based on type and connections\n",
    "#         size = 15  # Default size\n",
    "#         if node_type == 'directory':\n",
    "#             size = 25\n",
    "#         elif node_type == 'file':\n",
    "#             size = 20\n",
    "#         elif node_type == 'symbol' and node_data.get('symbol_type') == 'class':\n",
    "#             size = 18\n",
    "        \n",
    "#         # Add node with appropriate styling\n",
    "#         net.add_node(\n",
    "#             node, \n",
    "#             label=label, \n",
    "#             title=title,\n",
    "#             color=node_colors.get(node_type, '#999999'),\n",
    "#             size=size,\n",
    "#             shape='dot' if node_type != 'directory' else 'diamond'\n",
    "#         )\n",
    "    \n",
    "#     # Add edges with appropriate styles\n",
    "#     for source, target, edge_data in graph.edges(data=True):\n",
    "#         edge_type = edge_data.get('edge_type', 'unknown')\n",
    "        \n",
    "#         # Style edges differently based on type\n",
    "#         if edge_type == 'import':\n",
    "#             color = '#f5f542'\n",
    "#             width = 2\n",
    "#             dashes = False\n",
    "#         elif edge_type == 'references':\n",
    "#             color = '#f54242'\n",
    "#             width = 1\n",
    "#             dashes = [5, 5]\n",
    "#         elif edge_type == 'defines':\n",
    "#             color = '#42f55a'\n",
    "#             width = 3\n",
    "#             dashes = False\n",
    "#         elif edge_type == 'contains_snippet':\n",
    "#             color = '#42c8f5'\n",
    "#             width = 1\n",
    "#             dashes = [2, 2]\n",
    "#         else:\n",
    "#             color = '#999999'\n",
    "#             width = 1\n",
    "#             dashes = False\n",
    "        \n",
    "#         # Create hover title with edge details\n",
    "#         title = f\"<div><b>{edge_type}</b><br>\"\n",
    "        \n",
    "#         if edge_data.get('line_number'):\n",
    "#             title += f\"Line: {edge_data['line_number']}<br>\"\n",
    "            \n",
    "#         if edge_data.get('context'):\n",
    "#             context = edge_data['context']\n",
    "#             if len(context) > 200:\n",
    "#                 context = context[:197] + \"...\"\n",
    "#             title += f\"Context: {context}\"\n",
    "            \n",
    "#         title += \"</div>\"\n",
    "        \n",
    "#         # Add edge with styling\n",
    "#         net.add_edge(\n",
    "#             source, \n",
    "#             target, \n",
    "#             title=title,\n",
    "#             color=color,\n",
    "#             width=width,\n",
    "#             dashes=dashes\n",
    "#         )\n",
    "    \n",
    "#     # Enable physics, navigation and interaction options\n",
    "#     net.toggle_physics(True)\n",
    "#     net.show_buttons(filter_=['physics'])\n",
    "    \n",
    "#     # Save the visualization\n",
    "#     net.save_graph(output_html)\n",
    "#     print(f\"Graph visualization saved to {output_html}\")\n",
    "    \n",
    "#     return output_html\n",
    "\n",
    "# def visualize_symbol_subgraph(visualizer, symbol_name, output_html=\"symbol_graph.html\"):\n",
    "#     \"\"\"\n",
    "#     Create a focused visualization of a symbol and its relationships.\n",
    "    \n",
    "#     Args:\n",
    "#         visualizer: Initialized CodebaseVisualizer instance\n",
    "#         symbol_name: Name of the symbol to visualize\n",
    "#         output_html: Output HTML file for the visualization\n",
    "#     \"\"\"\n",
    "#     # Get the full graph\n",
    "#     full_graph = visualizer.graph\n",
    "    \n",
    "#     # Find all nodes related to this symbol\n",
    "#     symbol_nodes = [node for node in full_graph.nodes() if f\"::{symbol_name}\" in node]\n",
    "    \n",
    "#     if not symbol_nodes:\n",
    "#         print(f\"Symbol '{symbol_name}' not found in the graph.\")\n",
    "#         return None\n",
    "    \n",
    "#     # Get nodes that are connected to symbol nodes (1-hop neighborhood)\n",
    "#     related_nodes = set(symbol_nodes)\n",
    "#     for node in symbol_nodes:\n",
    "#         # Add predecessors (nodes that reference this symbol)\n",
    "#         related_nodes.update(full_graph.predecessors(node))\n",
    "#         # Add successors (nodes that this symbol references)\n",
    "#         related_nodes.update(full_graph.successors(node))\n",
    "    \n",
    "#     # Create a subgraph\n",
    "#     subgraph = full_graph.subgraph(related_nodes)\n",
    "    \n",
    "#     # Create a pyvis network\n",
    "#     net = Network(height=\"750px\", width=\"100%\", bgcolor=\"#222222\", font_color=\"white\")\n",
    "#     net.barnes_hut(gravity=-2000, central_gravity=0.3, spring_length=150)\n",
    "    \n",
    "#     # Node colors by type\n",
    "#     node_colors = {\n",
    "#         'file': '#4287f5',\n",
    "#         'directory': '#42f5a7',\n",
    "#         'symbol': '#f542cb',\n",
    "#         'snippet': '#f5a742'\n",
    "#     }\n",
    "    \n",
    "#     # Add nodes with appropriate styles\n",
    "#     for node, node_data in subgraph.nodes(data=True):\n",
    "#         node_type = node_data.get('type', 'unknown')\n",
    "#         label = os.path.basename(node) if '/' in node else node\n",
    "        \n",
    "#         # Make the focus symbol nodes larger and highlighted\n",
    "#         if node in symbol_nodes:\n",
    "#             size = 30\n",
    "#             color = '#ff0000'  # Bright red for focus\n",
    "#         else:\n",
    "#             size = 15\n",
    "#             color = node_colors.get(node_type, '#999999')\n",
    "        \n",
    "#         # Add hover information\n",
    "#         title = f\"<div style='max-width:300px;'>\"\n",
    "#         title += f\"<b>{node}</b><br>\"\n",
    "#         title += f\"Type: {node_type}<br>\"\n",
    "        \n",
    "#         if node_type == 'symbol':\n",
    "#             title += f\"Symbol type: {node_data.get('symbol_type', 'N/A')}<br>\"\n",
    "#             if node_data.get('docstring'):\n",
    "#                 title += f\"Docstring: {node_data.get('docstring', '')}<br>\"\n",
    "                \n",
    "#         title += \"</div>\"\n",
    "        \n",
    "#         # Add node with styling\n",
    "#         net.add_node(\n",
    "#             node,\n",
    "#             label=label,\n",
    "#             title=title,\n",
    "#             color=color,\n",
    "#             size=size\n",
    "#         )\n",
    "    \n",
    "#     # Add edges with styles\n",
    "#     for source, target, edge_data in subgraph.edges(data=True):\n",
    "#         edge_type = edge_data.get('edge_type', 'unknown')\n",
    "        \n",
    "#         # Style edges by type\n",
    "#         if edge_type == 'import':\n",
    "#             color = '#f5f542'  # Yellow\n",
    "#         elif edge_type == 'references':\n",
    "#             color = '#f54242'  # Red\n",
    "#         elif edge_type == 'defines':\n",
    "#             color = '#42f55a'  # Green\n",
    "#         else:\n",
    "#             color = '#999999'  # Gray\n",
    "        \n",
    "#         # Add edge with details in hover\n",
    "#         title = f\"{edge_type}\"\n",
    "#         if edge_data.get('line_number'):\n",
    "#             title += f\" (line {edge_data['line_number']})\"\n",
    "            \n",
    "#         net.add_edge(source, target, title=title, color=color)\n",
    "    \n",
    "#     # Enable physics and navigation\n",
    "#     net.toggle_physics(True)\n",
    "#     net.show_buttons(filter_=['physics'])\n",
    "    \n",
    "#     # Save the visualization\n",
    "#     net.save_graph(output_html)\n",
    "#     print(f\"Symbol graph visualization saved to {output_html}\")\n",
    "    \n",
    "#     return output_html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize the entire codebase graph (with node limit for performance)\n",
    "# visualize_codebase_graph(\"directory_name\", limit_nodes=200)\n",
    "\n",
    "# # Visualize a specific symbol (like \"symbol_name\")\n",
    "# visualizer = CodebaseVisualizer(\"directory_name\")\n",
    "# visualizer.parse_files()\n",
    "# G = visualizer.build_graph()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
