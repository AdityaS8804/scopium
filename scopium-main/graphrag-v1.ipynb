{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T06:27:55.540171Z",
     "iopub.status.busy": "2025-03-04T06:27:55.539628Z",
     "iopub.status.idle": "2025-03-04T06:27:57.074713Z",
     "shell.execute_reply": "2025-03-04T06:27:57.073411Z",
     "shell.execute_reply.started": "2025-03-04T06:27:55.540130Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing packages - **DONT TOUCH IT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T06:27:59.586675Z",
     "iopub.status.busy": "2025-03-04T06:27:59.586283Z",
     "iopub.status.idle": "2025-03-04T06:28:01.781957Z",
     "shell.execute_reply": "2025-03-04T06:28:01.780296Z",
     "shell.execute_reply.started": "2025-03-04T06:27:59.586633Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'flask' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/pallets/flask.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T06:28:04.152873Z",
     "iopub.status.busy": "2025-03-04T06:28:04.152522Z",
     "iopub.status.idle": "2025-03-04T06:28:15.047579Z",
     "shell.execute_reply": "2025-03-04T06:28:15.046302Z",
     "shell.execute_reply.started": "2025-03-04T06:28:04.152845Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nx-arangodb in ./.venv/lib/python3.10/site-packages (1.3.0)\n",
      "Requirement already satisfied: networkx<=3.4,>=3.0 in ./.venv/lib/python3.10/site-packages (from nx-arangodb) (3.4)\n",
      "Requirement already satisfied: phenolrs~=0.5 in ./.venv/lib/python3.10/site-packages (from nx-arangodb) (0.5.9)\n",
      "Requirement already satisfied: python-arango~=8.1 in ./.venv/lib/python3.10/site-packages (from nx-arangodb) (8.1.6)\n",
      "Requirement already satisfied: adbnx-adapter~=5.0.5 in ./.venv/lib/python3.10/site-packages (from nx-arangodb) (5.0.6)\n",
      "Requirement already satisfied: requests>=2.27.1 in ./.venv/lib/python3.10/site-packages (from adbnx-adapter~=5.0.5->nx-arangodb) (2.32.3)\n",
      "Requirement already satisfied: rich>=12.5.1 in ./.venv/lib/python3.10/site-packages (from adbnx-adapter~=5.0.5->nx-arangodb) (13.9.4)\n",
      "Requirement already satisfied: setuptools>=45 in ./.venv/lib/python3.10/site-packages (from adbnx-adapter~=5.0.5->nx-arangodb) (75.6.0)\n",
      "Requirement already satisfied: numpy~=1.26 in ./.venv/lib/python3.10/site-packages (from phenolrs~=0.5->nx-arangodb) (1.26.4)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in ./.venv/lib/python3.10/site-packages (from python-arango~=8.1->nx-arangodb) (2.3.0)\n",
      "Requirement already satisfied: requests_toolbelt in ./.venv/lib/python3.10/site-packages (from python-arango~=8.1->nx-arangodb) (1.0.0)\n",
      "Requirement already satisfied: PyJWT in ./.venv/lib/python3.10/site-packages (from python-arango~=8.1->nx-arangodb) (2.10.1)\n",
      "Requirement already satisfied: importlib_metadata>=4.7.1 in ./.venv/lib/python3.10/site-packages (from python-arango~=8.1->nx-arangodb) (8.6.1)\n",
      "Requirement already satisfied: packaging>=23.1 in ./.venv/lib/python3.10/site-packages (from python-arango~=8.1->nx-arangodb) (24.2)\n",
      "Requirement already satisfied: zipp>=3.20 in ./.venv/lib/python3.10/site-packages (from importlib_metadata>=4.7.1->python-arango~=8.1->nx-arangodb) (3.21.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests>=2.27.1->adbnx-adapter~=5.0.5->nx-arangodb) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests>=2.27.1->adbnx-adapter~=5.0.5->nx-arangodb) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests>=2.27.1->adbnx-adapter~=5.0.5->nx-arangodb) (2025.1.31)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.10/site-packages (from rich>=12.5.1->adbnx-adapter~=5.0.5->nx-arangodb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.10/site-packages (from rich>=12.5.1->adbnx-adapter~=5.0.5->nx-arangodb) (2.19.1)\n",
      "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in ./.venv/lib/python3.10/site-packages (from rich>=12.5.1->adbnx-adapter~=5.0.5->nx-arangodb) (4.12.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=12.5.1->adbnx-adapter~=5.0.5->nx-arangodb) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install nx-arangodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T06:28:15.049958Z",
     "iopub.status.busy": "2025-03-04T06:28:15.049522Z",
     "iopub.status.idle": "2025-03-04T06:28:15.324661Z",
     "shell.execute_reply": "2025-03-04T06:28:15.323352Z",
     "shell.execute_reply.started": "2025-03-04T06:28:15.049913Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nvidia-smi\n",
      "zsh:1: command not found: nvcc\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T06:28:16.653376Z",
     "iopub.status.busy": "2025-03-04T06:28:16.652942Z",
     "iopub.status.idle": "2025-03-04T06:28:37.048897Z",
     "shell.execute_reply": "2025-03-04T06:28:37.047530Z",
     "shell.execute_reply.started": "2025-03-04T06:28:16.653341Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.nvidia.com\n",
      "Collecting nx-cugraph-cu12\n",
      "  Downloading https://pypi.nvidia.com/nx-cugraph-cu12/nx_cugraph_cu12-25.2.0-py3-none-any.whl (160 kB)\n",
      "INFO: pip is looking at multiple versions of nx-cugraph-cu12 to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading https://pypi.nvidia.com/nx-cugraph-cu12/nx_cugraph_cu12-24.12.0-py3-none-any.whl (152 kB)\n",
      "  Downloading https://pypi.nvidia.com/nx-cugraph-cu12/nx_cugraph_cu12-24.10.0-py3-none-any.whl (149 kB)\n",
      "  Downloading https://pypi.nvidia.com/nx-cugraph-cu12/nx_cugraph_cu12-24.8.0-py3-none-any.whl (140 kB)\n",
      "  Downloading https://pypi.nvidia.com/nx-cugraph-cu12/nx_cugraph_cu12-24.6.1-py3-none-any.whl (129 kB)\n",
      "  Downloading https://pypi.nvidia.com/nx-cugraph-cu12/nx_cugraph_cu12-24.6.0-py3-none-any.whl (129 kB)\n",
      "  Downloading https://pypi.nvidia.com/nx-cugraph-cu12/nx_cugraph_cu12-24.4.0-py3-none-any.whl (125 kB)\n",
      "  Downloading https://pypi.nvidia.com/nx-cugraph-cu12/nx_cugraph_cu12-24.2.0-py3-none-any.whl (117 kB)\n",
      "INFO: pip is still looking at multiple versions of nx-cugraph-cu12 to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading https://pypi.nvidia.com/nx-cugraph-cu12/nx_cugraph_cu12-23.12.0-py3-none-any.whl (87 kB)\n",
      "  Downloading https://pypi.nvidia.com/nx-cugraph-cu12/nx_cugraph_cu12-23.10.0-py3-none-any.whl (39 kB)\n",
      "\u001b[31mERROR: Cannot install nx-cugraph-cu12==23.10.0, nx-cugraph-cu12==23.12.0, nx-cugraph-cu12==24.10.0, nx-cugraph-cu12==24.12.0, nx-cugraph-cu12==24.2.0, nx-cugraph-cu12==24.4.0, nx-cugraph-cu12==24.6.0, nx-cugraph-cu12==24.6.1, nx-cugraph-cu12==24.8.0 and nx-cugraph-cu12==25.2.0 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "The conflict is caused by:\n",
      "    nx-cugraph-cu12 25.2.0 depends on cupy-cuda12x>=12.0.0\n",
      "    nx-cugraph-cu12 24.12.0 depends on cupy-cuda12x>=12.0.0\n",
      "    nx-cugraph-cu12 24.10.0 depends on cupy-cuda12x>=12.0.0\n",
      "    nx-cugraph-cu12 24.8.0 depends on cupy-cuda12x>=12.0.0\n",
      "    nx-cugraph-cu12 24.6.1 depends on cupy-cuda12x>=12.0.0\n",
      "    nx-cugraph-cu12 24.6.0 depends on cupy-cuda12x>=12.0.0\n",
      "    nx-cugraph-cu12 24.4.0 depends on cupy-cuda12x>=12.0.0\n",
      "    nx-cugraph-cu12 24.2.0 depends on cupy-cuda12x>=12.0.0\n",
      "    nx-cugraph-cu12 23.12.0 depends on cupy-cuda12x>=12.0.0\n",
      "    nx-cugraph-cu12 23.10.0 depends on cupy-cuda12x>=12.0.0\n",
      "\n",
      "To fix this you could try to:\n",
      "1. loosen the range of package versions you've specified\n",
      "2. remove package versions to allow pip to attempt to solve the dependency conflict\n",
      "\n",
      "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 5] Input/output error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/GitHub/scopium/.venv/lib/python3.10/site-packages/IPython/utils/_process_posix.py:156\u001b[0m, in \u001b[0;36mProcessHandler.system\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;66;03m# res is the index of the pattern that caused the match, so we\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# know whether we've finished (if we matched EOF) or not\u001b[39;00m\n\u001b[0;32m--> 156\u001b[0m     res_idx \u001b[38;5;241m=\u001b[39m \u001b[43mchild\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpect_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatterns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28mprint\u001b[39m(child\u001b[38;5;241m.\u001b[39mbefore[out_size:]\u001b[38;5;241m.\u001b[39mdecode(enc, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m'\u001b[39m), end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/GitHub/scopium/.venv/lib/python3.10/site-packages/pexpect/spawnbase.py:383\u001b[0m, in \u001b[0;36mSpawnBase.expect_list\u001b[0;34m(self, pattern_list, timeout, searchwindowsize, async_, **kw)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpect_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GitHub/scopium/.venv/lib/python3.10/site-packages/pexpect/expect.py:169\u001b[0m, in \u001b[0;36mExpecter.expect_loop\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# Still have time left, so read more data\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m incoming \u001b[38;5;241m=\u001b[39m \u001b[43mspawn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_nonblocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspawn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaxread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspawn\u001b[38;5;241m.\u001b[39mdelayafterread \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/GitHub/scopium/.venv/lib/python3.10/site-packages/pexpect/pty_spawn.py:500\u001b[0m, in \u001b[0;36mspawn.read_nonblocking\u001b[0;34m(self, size, timeout)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;66;03m# Because of the select(0) check above, we know that no data\u001b[39;00m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;66;03m# is available right now. But if a non-zero timeout is given\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;66;03m# (possibly timeout=None), we call select() with a timeout.\u001b[39;00m\n\u001b[0;32m--> 500\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (timeout \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(spawn, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mread_nonblocking(size)\n",
      "File \u001b[0;32m~/GitHub/scopium/.venv/lib/python3.10/site-packages/pexpect/pty_spawn.py:450\u001b[0m, in \u001b[0;36mspawn.read_nonblocking.<locals>.select\u001b[0;34m(timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mselect\u001b[39m(timeout):\n\u001b[0;32m--> 450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mselect_ignore_interrupts\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchild_fd\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/GitHub/scopium/.venv/lib/python3.10/site-packages/pexpect/utils.py:143\u001b[0m, in \u001b[0;36mselect_ignore_interrupts\u001b[0;34m(iwtd, owtd, ewtd, timeout)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mselect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43miwtd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mowtd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mewtd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpip install nx-cugraph-cu12 --extra-index-url https://pypi.nvidia.com # Requires CUDA-capable GPU\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GitHub/scopium/.venv/lib/python3.10/site-packages/ipykernel/zmqshell.py:657\u001b[0m, in \u001b[0;36mZMQInteractiveShell.system_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    655\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_exit_code\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m system(cmd)\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 657\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_exit_code\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvar_expand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GitHub/scopium/.venv/lib/python3.10/site-packages/IPython/utils/_process_posix.py:167\u001b[0m, in \u001b[0;36mProcessHandler.system\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    162\u001b[0m         out_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(child\u001b[38;5;241m.\u001b[39mbefore)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# We need to send ^C to the process.  The ascii code for '^C' is 3\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# (the character is known as ETX for 'End of Text', see\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# curses.ascii.ETX).\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m     \u001b[43mchild\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msendline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mchr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# Read and print any more output the program might produce on its\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# way out.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/GitHub/scopium/.venv/lib/python3.10/site-packages/pexpect/pty_spawn.py:578\u001b[0m, in \u001b[0;36mspawn.sendline\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''Wraps send(), sending string ``s`` to child process, with\u001b[39;00m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;124;03m``os.linesep`` automatically appended. Returns number of bytes\u001b[39;00m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;124;03mwritten.  Only a limited number of bytes may be sent for each\u001b[39;00m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;124;03mline in the default terminal mode, see docstring of :meth:`send`.\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    577\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coerce_send_string(s)\n\u001b[0;32m--> 578\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinesep\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GitHub/scopium/.venv/lib/python3.10/site-packages/pexpect/pty_spawn.py:569\u001b[0m, in \u001b[0;36mspawn.send\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log(s, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msend\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    568\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encoder\u001b[38;5;241m.\u001b[39mencode(s, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 569\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchild_fd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 5] Input/output error"
     ]
    }
   ],
   "source": [
    "!pip install nx-cugraph-cu12 --extra-index-url https://pypi.nvidia.com # Requires CUDA-capable GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T06:31:33.136264Z",
     "iopub.status.busy": "2025-03-04T06:31:33.135815Z",
     "iopub.status.idle": "2025-03-04T06:31:48.649399Z",
     "shell.execute_reply": "2025-03-04T06:31:48.648009Z",
     "shell.execute_reply.started": "2025-03-04T06:31:33.136235Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in ./.venv/lib/python3.10/site-packages (0.3.20)\n",
      "Requirement already satisfied: langchain-community in ./.venv/lib/python3.10/site-packages (0.3.19)\n",
      "Requirement already satisfied: langchain-openai in ./.venv/lib/python3.10/site-packages (0.3.7)\n",
      "Requirement already satisfied: langgraph in ./.venv/lib/python3.10/site-packages (0.3.5)\n",
      "Requirement already satisfied: langchain_mistralai in ./.venv/lib/python3.10/site-packages (0.2.7)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.41 in ./.venv/lib/python3.10/site-packages (from langchain) (0.3.41)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in ./.venv/lib/python3.10/site-packages (from langchain) (0.3.6)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in ./.venv/lib/python3.10/site-packages (from langchain) (0.3.11)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in ./.venv/lib/python3.10/site-packages (from langchain) (2.10.6)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./.venv/lib/python3.10/site-packages (from langchain) (2.0.38)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.venv/lib/python3.10/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.venv/lib/python3.10/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in ./.venv/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./.venv/lib/python3.10/site-packages (from langchain-community) (3.11.13)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in ./.venv/lib/python3.10/site-packages (from langchain-community) (9.0.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in ./.venv/lib/python3.10/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in ./.venv/lib/python3.10/site-packages (from langchain-community) (2.8.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in ./.venv/lib/python3.10/site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in ./.venv/lib/python3.10/site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.58.1 in ./.venv/lib/python3.10/site-packages (from langchain-openai) (1.65.1)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in ./.venv/lib/python3.10/site-packages (from langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.10 in ./.venv/lib/python3.10/site-packages (from langgraph) (2.0.16)\n",
      "Requirement already satisfied: langgraph-prebuilt<0.2,>=0.1.1 in ./.venv/lib/python3.10/site-packages (from langgraph) (0.1.1)\n",
      "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.42 in ./.venv/lib/python3.10/site-packages (from langgraph) (0.1.53)\n",
      "Requirement already satisfied: tokenizers<1,>=0.15.1 in ./.venv/lib/python3.10/site-packages (from langchain_mistralai) (0.21.0)\n",
      "Requirement already satisfied: httpx<1,>=0.25.2 in ./.venv/lib/python3.10/site-packages (from langchain_mistralai) (0.28.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.venv/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./.venv/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.10/site-packages (from httpx<1,>=0.25.2->langchain_mistralai) (4.8.0)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.10/site-packages (from httpx<1,>=0.25.2->langchain_mistralai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.10/site-packages (from httpx<1,>=0.25.2->langchain_mistralai) (1.0.7)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.10/site-packages (from httpx<1,>=0.25.2->langchain_mistralai) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.25.2->langchain_mistralai) (0.14.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.venv/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./.venv/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./.venv/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (4.12.2)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.1.0 in ./.venv/lib/python3.10/site-packages (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph) (1.1.0)\n",
      "Requirement already satisfied: orjson>=3.10.1 in ./.venv/lib/python3.10/site-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./.venv/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in ./.venv/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.10/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./.venv/lib/python3.10/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (0.8.2)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.10/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./.venv/lib/python3.10/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./.venv/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in ./.venv/lib/python3.10/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./.venv/lib/python3.10/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in ./.venv/lib/python3.10/site-packages (from tokenizers<1,>=0.15.1->langchain_mistralai) (0.29.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./.venv/lib/python3.10/site-packages (from anyio->httpx<1,>=0.25.2->langchain_mistralai) (1.2.2)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain_mistralai) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain_mistralai) (2025.2.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.41->langchain) (3.0.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.venv/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade langchain langchain-community langchain-openai langgraph langchain_mistralai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T06:31:48.651596Z",
     "iopub.status.busy": "2025-03-04T06:31:48.651210Z",
     "iopub.status.idle": "2025-03-04T06:31:53.301722Z",
     "shell.execute_reply": "2025-03-04T06:31:53.300318Z",
     "shell.execute_reply.started": "2025-03-04T06:31:48.651547Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: networkx==3.4 in ./.venv/lib/python3.10/site-packages (3.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install networkx==3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tree-sitter in ./.venv/lib/python3.10/site-packages (0.24.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tree-sitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'tree-sitter-cpp' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/tree-sitter/tree-sitter-cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T06:32:31.887003Z",
     "iopub.status.busy": "2025-03-04T06:32:31.886594Z",
     "iopub.status.idle": "2025-03-04T06:32:31.893200Z",
     "shell.execute_reply": "2025-03-04T06:32:31.891936Z",
     "shell.execute_reply.started": "2025-03-04T06:32:31.886967Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:32:44 +0530] [INFO]: NetworkX-cuGraph is unavailable: No module named 'cupy'.\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import ast\n",
    "from typing import Dict, Set, List, Tuple, Optional,Any\n",
    "import json\n",
    "from arango import ArangoClient\n",
    "import nx_arangodb as nxadb\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.graphs import ArangoGraph\n",
    "from langchain_community.chains.graph_qa.arangodb import ArangoGraphQAChain\n",
    "from langchain_core.tools import tool\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T06:32:40.946548Z",
     "iopub.status.busy": "2025-03-04T06:32:40.946222Z",
     "iopub.status.idle": "2025-03-04T06:32:41.131038Z",
     "shell.execute_reply": "2025-03-04T06:32:41.129930Z",
     "shell.execute_reply.started": "2025-03-04T06:32:40.946523Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import ast\n",
    "# import networkx as nx\n",
    "# from typing import Dict, Set, List, Tuple, Optional\n",
    "# import json\n",
    "# from arango import ArangoClient\n",
    "# import re\n",
    "\n",
    "# class CodebaseVisualizer:\n",
    "#     def __init__(self, root_dir: str):\n",
    "#         self.root_dir = root_dir\n",
    "#         self.graph = nx.DiGraph()\n",
    "#         self.file_contents: Dict[str, str] = {}\n",
    "#         self.import_relations: Dict[str, List[Tuple[str, int]]] = {}  # file -> [(module, line_no)]\n",
    "#         self.module_symbols: Dict[str, Dict[str, Dict[str, any]]] = {}  # file -> {symbol -> {type, line_no, context}}\n",
    "#         self.symbol_references: Dict[str, List[Tuple[str, int]]] = {}  # symbol -> [(file, line_no)]\n",
    "#         self.file_index: Dict[str, int] = {}  # Maps files to indices\n",
    "#         self.current_index = 0\n",
    "#         self.directories: Set[str] = set()\n",
    "#         # Add a new index for all symbols to quickly locate them\n",
    "#         self.symbol_index: Dict[str, List[Dict]] = {}  # symbol -> [{file, type, line_no, context}]\n",
    "\n",
    "#     def _get_next_index(self) -> int:\n",
    "#         \"\"\"Get next available index for file indexing.\"\"\"\n",
    "#         self.current_index += 1\n",
    "#         return self.current_index\n",
    "\n",
    "#     def _chunk_code(self, code: str, lines_per_chunk: int = 20) -> List[Dict]:\n",
    "#         \"\"\"\n",
    "#         Chunk the given code into snippets.\n",
    "#         Returns a list of dictionaries with 'code_snippet', 'start_line', and 'end_line'.\n",
    "#         \"\"\"\n",
    "#         lines = code.splitlines()\n",
    "#         chunks = []\n",
    "#         for i in range(0, len(lines), lines_per_chunk):\n",
    "#             chunk_lines = lines[i:i + lines_per_chunk]\n",
    "#             chunk = {\n",
    "#                 'code_snippet': '\\n'.join(chunk_lines),\n",
    "#                 'start_line': i + 1,\n",
    "#                 'end_line': i + len(chunk_lines)\n",
    "#             }\n",
    "#             chunks.append(chunk)\n",
    "#         return chunks\n",
    "\n",
    "#     def _get_context_around_line(self, file_path: str, line_no: int, context_lines: int = 3) -> str:\n",
    "#         \"\"\"Extract context around a specific line in a file.\"\"\"\n",
    "#         if file_path not in self.file_contents:\n",
    "#             return \"\"\n",
    "        \n",
    "#         lines = self.file_contents[file_path].splitlines()\n",
    "#         start = max(0, line_no - context_lines - 1)\n",
    "#         end = min(len(lines), line_no + context_lines)\n",
    "        \n",
    "#         context = \"\\n\".join(lines[start:end])\n",
    "#         return context\n",
    "\n",
    "#     def parse_files(self) -> None:\n",
    "#         \"\"\"Parse all Python files in the directory and build relationships.\"\"\"\n",
    "#         # First pass: Index all files and create directory nodes\n",
    "#         for root, dirs, files in os.walk(self.root_dir):\n",
    "#             # Add directory node\n",
    "#             rel_dir = os.path.relpath(root, self.root_dir)\n",
    "#             if rel_dir != '.':\n",
    "#                 self.directories.add(rel_dir)\n",
    "#                 self.graph.add_node(rel_dir, type='directory')\n",
    "\n",
    "#             # Index Python files\n",
    "#             for file in files:\n",
    "#                 if file.endswith('.py'):\n",
    "#                     file_path = os.path.join(root, file)\n",
    "#                     rel_path = os.path.relpath(file_path, self.root_dir)\n",
    "#                     self.file_index[rel_path] = self._get_next_index()\n",
    "                    \n",
    "#                     try:\n",
    "#                         with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#                             content = f.read()\n",
    "#                             self.file_contents[rel_path] = content\n",
    "#                             self._analyze_file(rel_path, content)\n",
    "#                     except Exception as e:\n",
    "#                         print(f\"Error parsing {file_path}: {e}\")\n",
    "        \n",
    "#         # Second pass: Find symbol references across files\n",
    "#         self._find_symbol_references()\n",
    "        \n",
    "#         # Build the symbol index after all analyses\n",
    "#         self._build_symbol_index()\n",
    "\n",
    "#     def _analyze_file(self, file_path: str, content: str) -> None:\n",
    "#         \"\"\"Analyze a single file for imports and symbols with line numbers and context.\"\"\"\n",
    "#         try:\n",
    "#             tree = ast.parse(content)\n",
    "#             imports = []\n",
    "#             symbols = {}\n",
    "\n",
    "#             for node in ast.walk(tree):\n",
    "#                 # Track imports\n",
    "#                 if isinstance(node, (ast.Import, ast.ImportFrom)):\n",
    "#                     if isinstance(node, ast.Import):\n",
    "#                         for name in node.names:\n",
    "#                             imports.append((name.name, node.lineno))\n",
    "#                     else:  # ImportFrom\n",
    "#                         module = node.module if node.module else ''\n",
    "#                         for name in node.names:\n",
    "#                             imports.append((f\"{module}.{name.name}\" if module else name.name, node.lineno))\n",
    "\n",
    "#                 # Track defined symbols with line numbers and context\n",
    "#                 elif isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.Assign)):\n",
    "#                     if isinstance(node, (ast.FunctionDef, ast.ClassDef)):\n",
    "#                         symbol_name = node.name\n",
    "#                         symbol_type = 'class' if isinstance(node, ast.ClassDef) else 'function'\n",
    "#                         line_no = node.lineno\n",
    "#                         context = self._extract_node_source(content, node)\n",
    "                        \n",
    "#                         symbols[symbol_name] = {\n",
    "#                             'type': symbol_type,\n",
    "#                             'line_no': line_no,\n",
    "#                             'context': context,\n",
    "#                             'docstring': ast.get_docstring(node)\n",
    "#                         }\n",
    "#                     elif isinstance(node, ast.Assign):\n",
    "#                         # Handle variable assignments\n",
    "#                         for target in node.targets:\n",
    "#                             if isinstance(target, ast.Name):\n",
    "#                                 symbol_name = target.id\n",
    "#                                 line_no = node.lineno\n",
    "#                                 context = self._extract_node_source(content, node)\n",
    "                                \n",
    "#                                 symbols[symbol_name] = {\n",
    "#                                     'type': 'variable',\n",
    "#                                     'line_no': line_no,\n",
    "#                                     'context': context\n",
    "#                                 }\n",
    "\n",
    "#             self.import_relations[file_path] = imports\n",
    "#             self.module_symbols[file_path] = symbols\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error analyzing {file_path}: {e}\")\n",
    "\n",
    "#     def _extract_node_source(self, source: str, node) -> str:\n",
    "#         \"\"\"Extract the source code for an AST node.\"\"\"\n",
    "#         try:\n",
    "#             lines = source.splitlines()\n",
    "#             if hasattr(node, 'lineno') and hasattr(node, 'end_lineno'):\n",
    "#                 start = node.lineno - 1\n",
    "#                 end = getattr(node, 'end_lineno', start + 1)\n",
    "#                 return '\\n'.join(lines[start:end])\n",
    "#             return \"\"\n",
    "#         except Exception:\n",
    "#             return \"\"\n",
    "\n",
    "#     def _find_symbol_references(self) -> None:\n",
    "#         \"\"\"Find references to symbols across all files.\"\"\"\n",
    "#         for file_path, content in self.file_contents.items():\n",
    "#             try:\n",
    "#                 tree = ast.parse(content)\n",
    "#                 self._process_file_for_references(file_path, tree, content)\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error finding references in {file_path}: {e}\")\n",
    "\n",
    "#     def _process_file_for_references(self, file_path: str, tree, source: str) -> None:\n",
    "#         \"\"\"Process a file's AST to find references to symbols.\"\"\"\n",
    "#         for node in ast.walk(tree):\n",
    "#             # Find variable references\n",
    "#             if isinstance(node, ast.Name) and isinstance(node.ctx, ast.Load):\n",
    "#                 symbol_name = node.id\n",
    "#                 line_no = node.lineno\n",
    "                \n",
    "#                 # Track reference with context\n",
    "#                 if symbol_name not in self.symbol_references:\n",
    "#                     self.symbol_references[symbol_name] = []\n",
    "                \n",
    "#                 context = self._get_context_around_line(file_path, line_no)\n",
    "#                 self.symbol_references[symbol_name].append((file_path, line_no, context))\n",
    "            \n",
    "#             # Find attribute references (e.g., obj.method())\n",
    "#             elif isinstance(node, ast.Attribute) and isinstance(node.ctx, ast.Load):\n",
    "#                 attr_name = node.attr\n",
    "#                 line_no = node.lineno\n",
    "                \n",
    "#                 if attr_name not in self.symbol_references:\n",
    "#                     self.symbol_references[attr_name] = []\n",
    "                \n",
    "#                 context = self._get_context_around_line(file_path, line_no)\n",
    "#                 self.symbol_references[attr_name].append((file_path, line_no, context))\n",
    "\n",
    "#     def _build_symbol_index(self) -> None:\n",
    "#         \"\"\"Build a comprehensive index of all symbols and where they're defined/used.\"\"\"\n",
    "#         # Initialize the symbol index\n",
    "#         self.symbol_index = {}\n",
    "        \n",
    "#         # First, add all symbol definitions\n",
    "#         for file_path, symbols in self.module_symbols.items():\n",
    "#             for symbol_name, details in symbols.items():\n",
    "#                 if symbol_name not in self.symbol_index:\n",
    "#                     self.symbol_index[symbol_name] = []\n",
    "                \n",
    "#                 self.symbol_index[symbol_name].append({\n",
    "#                     'file': file_path,\n",
    "#                     'type': 'definition',\n",
    "#                     'symbol_type': details['type'],\n",
    "#                     'line_no': details['line_no'],\n",
    "#                     'context': details.get('context', ''),\n",
    "#                     'docstring': details.get('docstring', '')\n",
    "#                 })\n",
    "        \n",
    "#         # Then, add all references\n",
    "#         for symbol_name, references in self.symbol_references.items():\n",
    "#             if symbol_name not in self.symbol_index:\n",
    "#                 self.symbol_index[symbol_name] = []\n",
    "            \n",
    "#             for file_path, line_no, context in references:\n",
    "#                 # Avoid duplicating references if they're already in definitions\n",
    "#                 if not any(ref['file'] == file_path and ref['line_no'] == line_no and ref['type'] == 'definition' \n",
    "#                           for ref in self.symbol_index.get(symbol_name, [])):\n",
    "#                     self.symbol_index[symbol_name].append({\n",
    "#                         'file': file_path,\n",
    "#                         'type': 'reference',\n",
    "#                         'line_no': line_no,\n",
    "#                         'context': context\n",
    "#                     })\n",
    "\n",
    "#     def build_graph(self) -> nx.DiGraph:\n",
    "#         \"\"\"Build the NetworkX graph with enhanced node and edge information.\"\"\"\n",
    "#         # Start with a directed graph for clarity in relationships\n",
    "#         dot_graph = nx.DiGraph()\n",
    "        \n",
    "#         # Add nodes for all files with indices and code snippet nodes\n",
    "#         for file_path, file_idx in self.file_index.items():\n",
    "#             dot_graph.add_node(file_path, \n",
    "#                                type='file',\n",
    "#                                file_index=file_idx,\n",
    "#                                directory=os.path.dirname(file_path))\n",
    "            \n",
    "#             # Create snippet nodes for the entire file\n",
    "#             if file_path in self.file_contents:\n",
    "#                 chunks = self._chunk_code(self.file_contents[file_path])\n",
    "#                 for idx, chunk_info in enumerate(chunks):\n",
    "#                     snippet_node = f\"{file_path}::snippet::{idx}\"\n",
    "#                     dot_graph.add_node(snippet_node,\n",
    "#                                        type='snippet',\n",
    "#                                        code_snippet=chunk_info['code_snippet'],\n",
    "#                                        start_line=chunk_info['start_line'],\n",
    "#                                        end_line=chunk_info['end_line'])\n",
    "#                     # Connect file node to snippet node\n",
    "#                     dot_graph.add_edge(file_path, snippet_node, \n",
    "#                                        edge_type='contains_snippet',\n",
    "#                                        start_line=chunk_info['start_line'],\n",
    "#                                        end_line=chunk_info['end_line'])\n",
    "\n",
    "#             # Add nodes for symbols in this file\n",
    "#             for symbol, details in self.module_symbols.get(file_path, {}).items():\n",
    "#                 symbol_node = f\"{file_path}::{symbol}\"\n",
    "#                 dot_graph.add_node(symbol_node, \n",
    "#                                    type='symbol',\n",
    "#                                    symbol_type=details['type'],\n",
    "#                                    line_number=details['line_no'],\n",
    "#                                    context=details.get('context', ''),\n",
    "#                                    docstring=details.get('docstring', ''))\n",
    "#                 dot_graph.add_edge(file_path, symbol_node, \n",
    "#                                    edge_type='defines',\n",
    "#                                    line_number=details['line_no'])\n",
    "\n",
    "#         # Add directory nodes\n",
    "#         for directory in self.directories:\n",
    "#             dot_graph.add_node(directory, type='directory')\n",
    "\n",
    "#         # Add edges for imports with line numbers\n",
    "#         for file_path, imports in self.import_relations.items():\n",
    "#             for imp, line_no in imports:\n",
    "#                 # Look for matching files or symbols\n",
    "#                 for target_file, symbols in self.module_symbols.items():\n",
    "#                     if imp in symbols:\n",
    "#                         dot_graph.add_edge(file_path, \n",
    "#                                            f\"{target_file}::{imp}\",\n",
    "#                                            edge_type='import',\n",
    "#                                            line_number=line_no)\n",
    "#                     elif target_file.replace('.py', '').endswith(imp):\n",
    "#                         dot_graph.add_edge(file_path, \n",
    "#                                            target_file,\n",
    "#                                            edge_type='import',\n",
    "#                                            line_number=line_no)\n",
    "        \n",
    "#         # Add edges for symbol references with line numbers and context\n",
    "#         for symbol, references in self.symbol_references.items():\n",
    "#             for file_path, symbols in self.module_symbols.items():\n",
    "#                 if symbol in symbols:\n",
    "#                     symbol_node = f\"{file_path}::{symbol}\"\n",
    "                    \n",
    "#                     # Connect symbol to all its references\n",
    "#                     for ref_file, ref_line, context in references:\n",
    "#                         if ref_file != file_path:  # Only add cross-file references\n",
    "#                             dot_graph.add_edge(ref_file, \n",
    "#                                               symbol_node,\n",
    "#                                               edge_type='references',\n",
    "#                                               line_number=ref_line,\n",
    "#                                               context=context)\n",
    "        \n",
    "#         # Save the built graph in self.graph for later export\n",
    "#         self.graph = dot_graph\n",
    "#         return dot_graph\n",
    "\n",
    "#     def find_symbol_usages(self, symbol_name: str) -> List[Dict]:\n",
    "#         \"\"\"Find all usages of a symbol in the codebase with context.\"\"\"\n",
    "#         # Use the symbol index for fast lookup\n",
    "#         if symbol_name in self.symbol_index:\n",
    "#             return self.symbol_index[symbol_name]\n",
    "#         return []\n",
    "\n",
    "#     def find_symbol_by_partial_name(self, partial_name: str) -> Dict[str, List[Dict]]:\n",
    "#         \"\"\"Find symbols that match the partial name (case-insensitive).\"\"\"\n",
    "#         results = {}\n",
    "#         partial_name_lower = partial_name.lower()\n",
    "        \n",
    "#         for symbol_name, occurrences in self.symbol_index.items():\n",
    "#             if partial_name_lower in symbol_name.lower():\n",
    "#                 results[symbol_name] = occurrences\n",
    "        \n",
    "#         return results\n",
    "\n",
    "#     def get_symbol_definitions(self, symbol_name: str) -> List[Dict]:\n",
    "#         \"\"\"Get all definitions of a symbol across the codebase.\"\"\"\n",
    "#         if symbol_name in self.symbol_index:\n",
    "#             return [item for item in self.symbol_index[symbol_name] if item['type'] == 'definition']\n",
    "#         return []\n",
    "\n",
    "#     def get_symbol_references(self, symbol_name: str) -> List[Dict]:\n",
    "#         \"\"\"Get all references to a symbol across the codebase.\"\"\"\n",
    "#         if symbol_name in self.symbol_index:\n",
    "#             return [item for item in self.symbol_index[symbol_name] if item['type'] == 'reference']\n",
    "#         return []\n",
    "\n",
    "#     def infer_symbol_location(self, symbol_query: str) -> Dict:\n",
    "#         \"\"\"\n",
    "#         Infer the location and details of a symbol based on a query.\n",
    "#         This function makes it easier for LLMs to find the right symbol.\n",
    "#         \"\"\"\n",
    "#         # First, try exact match\n",
    "#         if symbol_query in self.symbol_index:\n",
    "#             definitions = self.get_symbol_definitions(symbol_query)\n",
    "#             if definitions:\n",
    "#                 return {\n",
    "#                     'found': True,\n",
    "#                     'exact_match': True,\n",
    "#                     'symbol_name': symbol_query,\n",
    "#                     'definitions': definitions,\n",
    "#                     'references': self.get_symbol_references(symbol_query)\n",
    "#                 }\n",
    "        \n",
    "#         # Try partial match\n",
    "#         partial_matches = self.find_symbol_by_partial_name(symbol_query)\n",
    "#         if partial_matches:\n",
    "#             # Sort matches by relevance (length of partial match compared to full name)\n",
    "#             sorted_matches = sorted(\n",
    "#                 partial_matches.items(), \n",
    "#                 key=lambda x: (abs(len(x[0]) - len(symbol_query)), x[0])\n",
    "#             )\n",
    "            \n",
    "#             # Get the most relevant matches\n",
    "#             relevant_symbols = {}\n",
    "#             for symbol_name, occurrences in sorted_matches[:5]:  # Take top 5 matches\n",
    "#                 definitions = [item for item in occurrences if item['type'] == 'definition']\n",
    "#                 references = [item for item in occurrences if item['type'] == 'reference']\n",
    "                \n",
    "#                 if definitions:  # Only include symbols with definitions\n",
    "#                     relevant_symbols[symbol_name] = {\n",
    "#                         'definitions': definitions,\n",
    "#                         'references': references\n",
    "#                     }\n",
    "            \n",
    "#             if relevant_symbols:\n",
    "#                 return {\n",
    "#                     'found': True,\n",
    "#                     'exact_match': False,\n",
    "#                     'partial_matches': relevant_symbols\n",
    "#                 }\n",
    "        \n",
    "#         # No matches found\n",
    "#         return {\n",
    "#             'found': False,\n",
    "#             'message': f\"No symbols matching '{symbol_query}' found in the codebase.\"\n",
    "#         }\n",
    "\n",
    "#     def export_file_index(self, output_path: str = 'file_index.txt') -> None:\n",
    "#         \"\"\"Export the file index mapping to a text file.\"\"\"\n",
    "#         with open(output_path, 'w') as f:\n",
    "#             # Write header\n",
    "#             f.write(\"File Index Mapping\\n\")\n",
    "#             f.write(\"=================\\n\\n\")\n",
    "            \n",
    "#             # Sort by index for better readability\n",
    "#             sorted_items = sorted(self.file_index.items(), key=lambda x: x[1])\n",
    "            \n",
    "#             # Write each file and its index\n",
    "#             for file_path, index in sorted_items:\n",
    "#                 f.write(f\"{index}: {file_path}\\n\")\n",
    "                \n",
    "#                 # If there are symbols in this file, list them with line numbers\n",
    "#                 if file_path in self.module_symbols:\n",
    "#                     f.write(\"  Symbols:\\n\")\n",
    "#                     for symbol, details in self.module_symbols[file_path].items():\n",
    "#                         symbol_type = details['type']\n",
    "#                         line_no = details['line_no']\n",
    "#                         f.write(f\"    - {symbol} ({symbol_type}) at line {line_no}\\n\")\n",
    "#                     f.write(\"\\n\")\n",
    "\n",
    "#     def export_symbol_index(self, output_path: str = 'symbol_index.json') -> None:\n",
    "#         \"\"\"Export the symbol index to a JSON file for quick lookups.\"\"\"\n",
    "#         with open(output_path, 'w') as f:\n",
    "#             json.dump(self.symbol_index, f, indent=2)\n",
    "\n",
    "#     def _analyze_symbol_purpose(self, symbol_name, usages):\n",
    "#         \"\"\"Analyze the purpose of a symbol based on its usage patterns.\"\"\"\n",
    "#         # Get the definition if available\n",
    "#         definitions = [u for u in usages if u['type'] == 'definition']\n",
    "#         references = [u for u in usages if u['type'] == 'reference']\n",
    "        \n",
    "#         purpose = \"\"\n",
    "        \n",
    "#         # Check if we have a definition with docstring\n",
    "#         if definitions and definitions[0].get('docstring'):\n",
    "#             purpose += f\"{definitions[0]['docstring']}\\n\\n\"\n",
    "        \n",
    "#         # If it's a function, try to infer what it does from usage\n",
    "#         if definitions and definitions[0].get('symbol_type') == 'function':\n",
    "#             # Collect contexts where it's used\n",
    "#             contexts = [ref.get('context', '') for ref in references if ref.get('context')]\n",
    "            \n",
    "#             # Analyze contexts for patterns\n",
    "#             if contexts:\n",
    "#                 common_patterns = self._find_common_usage_patterns(contexts, symbol_name)\n",
    "                \n",
    "#                 purpose += \"Based on usage patterns, this function appears to:\\n\"\n",
    "                \n",
    "#                 if any(\"assert\" in ctx for ctx in contexts):\n",
    "#                     purpose += \"- Be used in test assertions to verify behavior\\n\"\n",
    "                \n",
    "#                 if any(\"if\" in ctx and symbol_name in ctx for ctx in contexts):\n",
    "#                     purpose += \"- Be used as a condition in control flow statements\\n\"\n",
    "                \n",
    "#                 if common_patterns:\n",
    "#                     for pattern in common_patterns[:3]:  # Top 3 patterns\n",
    "#                         purpose += f\"- {pattern}\\n\"\n",
    "        \n",
    "#         # If we couldn't infer much, provide a generic description\n",
    "#         if not purpose or len(purpose.strip()) < 10:\n",
    "#             if definitions and definitions[0].get('symbol_type') == 'function':\n",
    "#                 context = definitions[0].get('context', '')\n",
    "                \n",
    "#                 # Look for parameters to understand what it takes\n",
    "#                 params = self._extract_function_params(context)\n",
    "                \n",
    "#                 purpose += f\"This function appears to check or validate something\"\n",
    "#                 if params:\n",
    "#                     purpose += f\" related to {', '.join(params)}\"\n",
    "#                 purpose += \".\\n\"\n",
    "        \n",
    "#         return purpose\n",
    "\n",
    "#     def _find_common_usage_patterns(self, contexts, symbol_name):\n",
    "#         \"\"\"Find common patterns in the usage contexts.\"\"\"\n",
    "#         patterns = []\n",
    "        \n",
    "#         # Check if it's used with certain objects/methods frequently\n",
    "#         if any(f\".{symbol_name}\" in ctx for ctx in contexts):\n",
    "#             patterns.append(\"Be a method called on objects\")\n",
    "        \n",
    "#         # Check if it's used for configuration or setup\n",
    "#         if any(\"config\" in ctx.lower() or \"setup\" in ctx.lower() for ctx in contexts):\n",
    "#             patterns.append(\"Be involved in configuration or setup\")\n",
    "        \n",
    "#         # Check if it's used for logging\n",
    "#         if any(\"log\" in ctx.lower() for ctx in contexts):\n",
    "#             patterns.append(\"Be related to logging functionality\")\n",
    "        \n",
    "#         # Check if it's used in exception handling\n",
    "#         if any(\"except\" in ctx.lower() or \"try\" in ctx.lower() for ctx in contexts):\n",
    "#             patterns.append(\"Be involved in exception handling\")\n",
    "        \n",
    "#         return patterns\n",
    "\n",
    "#     def _extract_function_params(self, context):\n",
    "#         \"\"\"Extract parameter names from a function definition.\"\"\"\n",
    "#         params = []\n",
    "#         if context:\n",
    "#             # Simple regex-based extraction\n",
    "#             match = re.search(r'def\\s+\\w+\\s*\\((.*?)\\)', context, re.DOTALL)\n",
    "#             if match:\n",
    "#                 param_string = match.group(1)\n",
    "#                 # Split by comma and clean up\n",
    "#                 raw_params = [p.strip() for p in param_string.split(',')]\n",
    "#                 # Extract just the parameter name (before any : or =)\n",
    "#                 params = [re.split(r'[=:]', p)[0].strip() for p in raw_params if p]\n",
    "#                 # Remove self if it's there\n",
    "#                 if params and params[0] == 'self':\n",
    "#                     params = params[1:]\n",
    "#         return params\n",
    "\n",
    "#     def _general_codebase_analysis(self, query):\n",
    "#         \"\"\"Provide a general analysis based on the query.\"\"\"\n",
    "#         response = f\"# Analysis for Query: {query}\\n\\n\"\n",
    "        \n",
    "#         # Check if the query is asking about structure\n",
    "#         if any(term in query.lower() for term in ['structure', 'organization', 'layout']):\n",
    "#             response += \"## Codebase Structure\\n\\n\"\n",
    "#             # Count files by directory\n",
    "#             files_by_dir = {}\n",
    "#             for file_path in self.file_index:\n",
    "#                 directory = os.path.dirname(file_path)\n",
    "#                 if directory not in files_by_dir:\n",
    "#                     files_by_dir[directory] = []\n",
    "#                 files_by_dir[directory].append(file_path)\n",
    "            \n",
    "#             response += f\"The codebase contains {len(self.file_index)} Python files across {len(files_by_dir)} directories.\\n\\n\"\n",
    "            \n",
    "#             # Show top-level directories\n",
    "#             response += \"Main directories:\\n\"\n",
    "#             for directory, files in sorted(files_by_dir.items(), key=lambda x: len(x[1]), reverse=True)[:10]:\n",
    "#                 dir_name = directory if directory else '(root)'\n",
    "#                 response += f\"- {dir_name}: {len(files)} files\\n\"\n",
    "        \n",
    "#         # Check if the query is asking about specific functionality\n",
    "#         functionality_terms = ['handle', 'process', 'create', 'generate', 'calculate']\n",
    "#         for term in functionality_terms:\n",
    "#             if term in query.lower():\n",
    "#                 # Search for functions with this term\n",
    "#                 matching_functions = []\n",
    "#                 for file_path, symbols in self.module_symbols.items():\n",
    "#                     for symbol, details in symbols.items():\n",
    "#                         if details['type'] == 'function' and term in symbol.lower():\n",
    "#                             matching_functions.append((file_path, symbol, details))\n",
    "                \n",
    "#                 if matching_functions:\n",
    "#                     response += f\"\\n## Functions Related to '{term}'\\n\\n\"\n",
    "#                     for file_path, symbol, details in matching_functions[:5]:  # Show top 5\n",
    "#                         response += f\"- `{symbol}` in {file_path}:{details['line_no']}\\n\"\n",
    "#                     if len(matching_functions) > 5:\n",
    "#                         response += f\"... and {len(matching_functions) - 5} more functions\\n\"\n",
    "        \n",
    "#         return response\n",
    "\n",
    "#     def export_graph_json(self, output_path: str = 'codebase_graph.json') -> None:\n",
    "#         \"\"\"Export the graph structure to JSON.\"\"\"\n",
    "#         graph_data = {\n",
    "#             'nodes': [\n",
    "#                 {\n",
    "#                     'id': node,\n",
    "#                     'type': data['type'],\n",
    "#                     'file_index': data.get('file_index'),\n",
    "#                     'directory': data.get('directory'),\n",
    "#                     'symbol_type': data.get('symbol_type'),\n",
    "#                     'line_number': data.get('line_number'),\n",
    "#                     'code_snippet': data.get('code_snippet'),\n",
    "#                     'start_line': data.get('start_line'),\n",
    "#                     'end_line': data.get('end_line'),\n",
    "#                     'context': data.get('context'),\n",
    "#                     'docstring': data.get('docstring')\n",
    "#                 } \n",
    "#                 for node, data in self.graph.nodes(data=True)\n",
    "#             ],\n",
    "#             'links': [\n",
    "#                 {\n",
    "#                     'source': source,\n",
    "#                     'target': target,\n",
    "#                     'type': data.get('edge_type'),\n",
    "#                     'line_number': data.get('line_number'),\n",
    "#                     'start_line': data.get('start_line'),\n",
    "#                     'end_line': data.get('end_line'),\n",
    "#                     'context': data.get('context')\n",
    "#                 } \n",
    "#                 for source, target, data in self.graph.edges(data=True)\n",
    "#             ]\n",
    "#         }\n",
    "        \n",
    "#         with open(output_path, 'w') as f:\n",
    "#             json.dump(graph_data, f, indent=2)\n",
    "            \n",
    "#     def export_inference_data(self, output_path: str = 'symbol_inference.json') -> None:\n",
    "#         \"\"\"\n",
    "#         Export inference-friendly data about all symbols in the codebase.\n",
    "#         This creates a dedicated lookup file optimized for LLMs to find symbols.\n",
    "#         \"\"\"\n",
    "#         inference_data = {\n",
    "#             'symbols': {},\n",
    "#             'metadata': {\n",
    "#                 'total_symbols': len(self.symbol_index),\n",
    "#                 'total_files': len(self.file_index),\n",
    "#                 'total_directories': len(self.directories)\n",
    "#             }\n",
    "#         }\n",
    "        \n",
    "#         # Process each symbol with its definitions and references\n",
    "#         for symbol_name, occurrences in self.symbol_index.items():\n",
    "#             definitions = [item for item in occurrences if item['type'] == 'definition']\n",
    "#             references = [item for item in occurrences if item['type'] == 'reference']\n",
    "            \n",
    "#             if definitions:  # Only include symbols with at least one definition\n",
    "#                 # Create a symbol entry with all relevant information\n",
    "#                 symbol_entry = {\n",
    "#                     'definition_files': list(set(d['file'] for d in definitions)),\n",
    "#                     'definition_count': len(definitions),\n",
    "#                     'reference_count': len(references),\n",
    "#                     'definitions': definitions,\n",
    "#                     # Include just summarized references to keep file size manageable\n",
    "#                     'reference_summary': {\n",
    "#                         f['file']: len([r for r in references if r['file'] == f['file']])\n",
    "#                         for f in sorted(set(({'file': r['file']} for r in references)), key=lambda x: x['file'])\n",
    "#                     },\n",
    "#                     'symbol_types': list(set(d.get('symbol_type', 'unknown') for d in definitions)),\n",
    "#                     'purpose': self._analyze_symbol_purpose(symbol_name, occurrences) if definitions else \"\"\n",
    "#                 }\n",
    "                \n",
    "#                 inference_data['symbols'][symbol_name] = symbol_entry\n",
    "        \n",
    "#         with open(output_path, 'w') as f:\n",
    "#             json.dump(inference_data, f, indent=2)\n",
    "\n",
    "#     def export_to_arango(self,\n",
    "#                          db_name: str = 'codebase',\n",
    "#                          username: str = 'root',\n",
    "#                          password: str = 'passwd',\n",
    "#                          host: str = 'http://localhost:8529') -> None:\n",
    "#         \"\"\"Export the graph into ArangoDB.\"\"\"\n",
    "#         client = ArangoClient(hosts=host)\n",
    "#         db = client.db(username=username, password=password, verify=True)\n",
    "\n",
    "#         # Delete the existing graph and its collections if they exist.\n",
    "#         graph_name = \"FlaskRepv1\"\n",
    "        \n",
    "#         # Import networkx_to_arangodb if it's available\n",
    "#         try:\n",
    "#             import networkx_to_arangodb as nxadb # type: ignore\n",
    "#             G_adb = nxadb.Graph(\n",
    "#                 name=graph_name,\n",
    "#                 db=db,\n",
    "#                 incoming_graph_data=self.graph,\n",
    "#                 write_batch_size=50000,\n",
    "#                 overwrite_graph=True\n",
    "#             )\n",
    "            \n",
    "#             self.G_adb = G_adb\n",
    "#             print(\"Graph successfully exported to ArangoDB.\")\n",
    "#             return G_adb\n",
    "#         except ImportError:\n",
    "#             print(\"networkx_to_arangodb module not found. Please install it to export to ArangoDB.\")\n",
    "#             return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import networkx as nx\n",
    "from typing import Dict, Set, List, Tuple, Optional, Union\n",
    "import json\n",
    "from arango import ArangoClient\n",
    "import re\n",
    "import glob\n",
    "\n",
    "class CodebaseVisualizer:\n",
    "    def __init__(self, root_dir: str, supported_languages=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.graph = nx.DiGraph()\n",
    "        self.file_contents: Dict[str, str] = {}\n",
    "        self.import_relations: Dict[str, List[Tuple[str, int]]] = {}  # file -> [(module, line_no)]\n",
    "        self.module_symbols: Dict[str, Dict[str, Dict[str, any]]] = {}  # file -> {symbol -> {type, line_no, context}}\n",
    "        self.symbol_references: Dict[str, List[Tuple[str, int, str]]] = {}  # symbol -> [(file, line_no, context)]\n",
    "        self.file_index: Dict[str, int] = {}  # Maps files to indices\n",
    "        self.current_index = 0\n",
    "        self.directories: Set[str] = set()\n",
    "        # Add a new index for all symbols to quickly locate them\n",
    "        self.symbol_index: Dict[str, List[Dict]] = {}  # symbol -> [{file, type, line_no, context}]\n",
    "        \n",
    "        # Define supported languages\n",
    "        self.supported_languages = supported_languages or [\"python\", \"cpp\", \"java\", \"go\"]\n",
    "        \n",
    "        # Language file extensions mapping\n",
    "        self.language_extensions = {\n",
    "            \"python\": [\".py\"],\n",
    "            \"cpp\": [\".c\", \".cpp\", \".h\", \".hpp\", \".cc\", \".cxx\", \".hxx\"],\n",
    "            \"java\": [\".java\"],\n",
    "            \"go\": [\".go\"]\n",
    "        }\n",
    "\n",
    "    def _get_next_index(self) -> int:\n",
    "        \"\"\"Get next available index for file indexing.\"\"\"\n",
    "        self.current_index += 1\n",
    "        return self.current_index\n",
    "\n",
    "    def _chunk_code(self, code: str, lines_per_chunk: int = 20) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Chunk the given code into snippets.\n",
    "        Returns a list of dictionaries with 'code_snippet', 'start_line', and 'end_line'.\n",
    "        \"\"\"\n",
    "        lines = code.splitlines()\n",
    "        chunks = []\n",
    "        for i in range(0, len(lines), lines_per_chunk):\n",
    "            chunk_lines = lines[i:i + lines_per_chunk]\n",
    "            chunk = {\n",
    "                'code_snippet': '\\n'.join(chunk_lines),\n",
    "                'start_line': i + 1,\n",
    "                'end_line': i + len(chunk_lines)\n",
    "            }\n",
    "            chunks.append(chunk)\n",
    "        return chunks\n",
    "\n",
    "    def _get_context_around_line(self, file_path: str, line_no: int, context_lines: int = 3) -> str:\n",
    "        \"\"\"Extract context around a specific line in a file.\"\"\"\n",
    "        if file_path not in self.file_contents:\n",
    "            return \"\"\n",
    "        \n",
    "        lines = self.file_contents[file_path].splitlines()\n",
    "        start = max(0, line_no - context_lines - 1)\n",
    "        end = min(len(lines), line_no + context_lines)\n",
    "        \n",
    "        context = \"\\n\".join(lines[start:end])\n",
    "        return context\n",
    "\n",
    "    def _detect_language(self, file_path: str) -> str:\n",
    "        \"\"\"Detect the programming language of a file based on its extension.\"\"\"\n",
    "        _, ext = os.path.splitext(file_path)\n",
    "        ext = ext.lower()\n",
    "        \n",
    "        for language, extensions in self.language_extensions.items():\n",
    "            if ext in extensions:\n",
    "                return language\n",
    "                \n",
    "        return \"unknown\"\n",
    "\n",
    "    def parse_files(self) -> None:\n",
    "        \"\"\"Parse all files in the directory and build relationships.\"\"\"\n",
    "        # First pass: Index all files and create directory nodes\n",
    "        for root, dirs, files in os.walk(self.root_dir):\n",
    "            # Add directory node\n",
    "            rel_dir = os.path.relpath(root, self.root_dir)\n",
    "            if rel_dir != '.':\n",
    "                self.directories.add(rel_dir)\n",
    "                self.graph.add_node(rel_dir, type='directory')\n",
    "                \n",
    "                # Add edge from parent directory to this directory (if not root)\n",
    "                parent_dir = os.path.dirname(rel_dir)\n",
    "                if parent_dir and parent_dir != '.':\n",
    "                    self.graph.add_edge(parent_dir, rel_dir, edge_type='contains_directory')\n",
    "\n",
    "            # Index files of supported languages\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                rel_path = os.path.relpath(file_path, self.root_dir)\n",
    "                file_language = self._detect_language(file_path)\n",
    "                \n",
    "                if file_language in self.supported_languages:\n",
    "                    self.file_index[rel_path] = self._get_next_index()\n",
    "                    \n",
    "                    # Add node for this file\n",
    "                    self.graph.add_node(rel_path, type='file', file_index=self.file_index[rel_path], language=file_language)\n",
    "                    \n",
    "                    # Connect file to its directory\n",
    "                    if rel_dir != '.':\n",
    "                        self.graph.add_edge(rel_dir, rel_path, edge_type='contains_file')\n",
    "                    \n",
    "                    try:\n",
    "                        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                            content = f.read()\n",
    "                            self.file_contents[rel_path] = content\n",
    "                            self._analyze_file(rel_path, content, file_language)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error parsing {file_path}: {e}\")\n",
    "        \n",
    "        # Second pass: Find symbol references across files\n",
    "        for file_path, content in self.file_contents.items():\n",
    "            file_language = self._detect_language(file_path)\n",
    "            self._find_references_in_file(file_path, content, file_language)\n",
    "        \n",
    "        # Build the symbol index after all analyses\n",
    "        self._build_symbol_index()\n",
    "\n",
    "    def _analyze_file(self, file_path: str, content: str, language: str) -> None:\n",
    "        \"\"\"Analyze a file for imports and symbols with line numbers and context.\"\"\"\n",
    "        if language == \"python\":\n",
    "            self._analyze_python_file(file_path, content)\n",
    "        elif language == \"cpp\":\n",
    "            self._analyze_cpp_file(file_path, content)\n",
    "        elif language == \"java\":\n",
    "            self._analyze_java_file(file_path, content)\n",
    "        elif language == \"go\":\n",
    "            self._analyze_go_file(file_path, content)\n",
    "\n",
    "    def _analyze_python_file(self, file_path: str, content: str) -> None:\n",
    "        \"\"\"Analyze a Python file for imports and symbols.\"\"\"\n",
    "        try:\n",
    "            tree = ast.parse(content)\n",
    "            imports = []\n",
    "            symbols = {}\n",
    "\n",
    "            for node in ast.walk(tree):\n",
    "                # Track imports\n",
    "                if isinstance(node, (ast.Import, ast.ImportFrom)):\n",
    "                    if isinstance(node, ast.Import):\n",
    "                        for name in node.names:\n",
    "                            imports.append((name.name, node.lineno))\n",
    "                    else:  # ImportFrom\n",
    "                        module = node.module if node.module else ''\n",
    "                        for name in node.names:\n",
    "                            imports.append((f\"{module}.{name.name}\" if module else name.name, node.lineno))\n",
    "\n",
    "                # Track defined symbols with line numbers and context\n",
    "                elif isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.Assign)):\n",
    "                    if isinstance(node, (ast.FunctionDef, ast.ClassDef)):\n",
    "                        symbol_name = node.name\n",
    "                        symbol_type = 'class' if isinstance(node, ast.ClassDef) else 'function'\n",
    "                        line_no = node.lineno\n",
    "                        context = self._extract_python_node_source(content, node)\n",
    "                        \n",
    "                        symbols[symbol_name] = {\n",
    "                            'type': symbol_type,\n",
    "                            'line_no': line_no,\n",
    "                            'context': context,\n",
    "                            'docstring': ast.get_docstring(node)\n",
    "                        }\n",
    "                    elif isinstance(node, ast.Assign):\n",
    "                        # Handle variable assignments\n",
    "                        for target in node.targets:\n",
    "                            if isinstance(target, ast.Name):\n",
    "                                symbol_name = target.id\n",
    "                                line_no = node.lineno\n",
    "                                context = self._extract_python_node_source(content, node)\n",
    "                                \n",
    "                                symbols[symbol_name] = {\n",
    "                                    'type': 'variable',\n",
    "                                    'line_no': line_no,\n",
    "                                    'context': context\n",
    "                                }\n",
    "\n",
    "            self.import_relations[file_path] = imports\n",
    "            self.module_symbols[file_path] = symbols\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing Python file {file_path}: {e}\")\n",
    "\n",
    "    def _extract_python_node_source(self, source: str, node) -> str:\n",
    "        \"\"\"Extract the source code for a Python AST node.\"\"\"\n",
    "        try:\n",
    "            lines = source.splitlines()\n",
    "            if hasattr(node, 'lineno') and hasattr(node, 'end_lineno'):\n",
    "                start = node.lineno - 1\n",
    "                end = getattr(node, 'end_lineno', start + 1)\n",
    "                return '\\n'.join(lines[start:end])\n",
    "            return \"\"\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "\n",
    "    def _analyze_cpp_file(self, file_path: str, content: str) -> None:\n",
    "        \"\"\"Analyze a C/C++ file for includes and symbols.\"\"\"\n",
    "        imports = []\n",
    "        symbols = {}\n",
    "        \n",
    "        # Process content line by line\n",
    "        lines = content.splitlines()\n",
    "        \n",
    "        # Regular expressions for C/C++ code analysis\n",
    "        include_pattern = re.compile(r'#include\\s+[<\"]([^>\"]+)[>\"]')\n",
    "        class_pattern = re.compile(r'(?:class|struct)\\s+(\\w+)')\n",
    "        function_pattern = re.compile(r'(\\w+)\\s*\\([^)]*\\)\\s*(?:const|override|final|noexcept)?\\s*(?:{|;)')\n",
    "        namespace_pattern = re.compile(r'namespace\\s+(\\w+)')\n",
    "        \n",
    "        for line_no, line in enumerate(lines, 1):\n",
    "            # Find include statements\n",
    "            include_match = include_pattern.search(line)\n",
    "            if include_match:\n",
    "                imports.append((include_match.group(1), line_no))\n",
    "            \n",
    "            # Find class/struct definitions\n",
    "            class_match = class_pattern.search(line)\n",
    "            if class_match:\n",
    "                class_name = class_match.group(1)\n",
    "                context = self._get_context_around_line(file_path, line_no)\n",
    "                symbols[class_name] = {\n",
    "                    'type': 'class',\n",
    "                    'line_no': line_no,\n",
    "                    'context': context\n",
    "                }\n",
    "            \n",
    "            # Find function definitions (simplified)\n",
    "            function_match = function_pattern.search(line)\n",
    "            if function_match and not line.strip().startswith('#') and not line.strip().startswith('//'):\n",
    "                function_name = function_match.group(1)\n",
    "                # Skip some common keywords that might be mistaken for functions\n",
    "                if function_name not in ['if', 'while', 'for', 'switch', 'return']:\n",
    "                    context = self._get_context_around_line(file_path, line_no)\n",
    "                    symbols[function_name] = {\n",
    "                        'type': 'function',\n",
    "                        'line_no': line_no,\n",
    "                        'context': context\n",
    "                    }\n",
    "            \n",
    "            # Find namespace definitions\n",
    "            namespace_match = namespace_pattern.search(line)\n",
    "            if namespace_match:\n",
    "                namespace_name = namespace_match.group(1)\n",
    "                context = self._get_context_around_line(file_path, line_no)\n",
    "                symbols[namespace_name] = {\n",
    "                    'type': 'namespace',\n",
    "                    'line_no': line_no,\n",
    "                    'context': context\n",
    "                }\n",
    "        \n",
    "        self.import_relations[file_path] = imports\n",
    "        self.module_symbols[file_path] = symbols\n",
    "\n",
    "    def _analyze_java_file(self, file_path: str, content: str) -> None:\n",
    "        \"\"\"Analyze a Java file for imports and symbols.\"\"\"\n",
    "        imports = []\n",
    "        symbols = {}\n",
    "        \n",
    "        # Process content line by line\n",
    "        lines = content.splitlines()\n",
    "        \n",
    "        # Regular expressions for Java code analysis\n",
    "        package_pattern = re.compile(r'package\\s+([\\w.]+)')\n",
    "        import_pattern = re.compile(r'import\\s+([\\w.]+(?:\\.\\*)?)')\n",
    "        class_pattern = re.compile(r'(?:public|private|protected)?\\s*(?:abstract|final)?\\s*class\\s+(\\w+)')\n",
    "        interface_pattern = re.compile(r'(?:public|private|protected)?\\s*interface\\s+(\\w+)')\n",
    "        method_pattern = re.compile(r'(?:public|private|protected)?\\s*(?:static|final|abstract)?\\s*(?:[\\w<>[\\],\\s]+)\\s+(\\w+)\\s*\\([^)]*\\)')\n",
    "        \n",
    "        for line_no, line in enumerate(lines, 1):\n",
    "            # Find package declaration\n",
    "            package_match = package_pattern.search(line)\n",
    "            if package_match:\n",
    "                package_name = package_match.group(1)\n",
    "                imports.append((package_name, line_no))\n",
    "            \n",
    "            # Find import statements\n",
    "            import_match = import_pattern.search(line)\n",
    "            if import_match:\n",
    "                import_name = import_match.group(1)\n",
    "                imports.append((import_name, line_no))\n",
    "            \n",
    "            # Find class definitions\n",
    "            class_match = class_pattern.search(line)\n",
    "            if class_match:\n",
    "                class_name = class_match.group(1)\n",
    "                context = self._get_context_around_line(file_path, line_no)\n",
    "                symbols[class_name] = {\n",
    "                    'type': 'class',\n",
    "                    'line_no': line_no,\n",
    "                    'context': context\n",
    "                }\n",
    "            \n",
    "            # Find interface definitions\n",
    "            interface_match = interface_pattern.search(line)\n",
    "            if interface_match:\n",
    "                interface_name = interface_match.group(1)\n",
    "                context = self._get_context_around_line(file_path, line_no)\n",
    "                symbols[interface_name] = {\n",
    "                    'type': 'interface',\n",
    "                    'line_no': line_no,\n",
    "                    'context': context\n",
    "                }\n",
    "            \n",
    "            # Find method definitions\n",
    "            method_match = method_pattern.search(line)\n",
    "            if method_match:\n",
    "                method_name = method_match.group(1)\n",
    "                # Skip some common keywords that might be mistaken for methods\n",
    "                if method_name not in ['if', 'while', 'for', 'switch', 'return']:\n",
    "                    context = self._get_context_around_line(file_path, line_no)\n",
    "                    symbols[method_name] = {\n",
    "                        'type': 'method',\n",
    "                        'line_no': line_no,\n",
    "                        'context': context\n",
    "                    }\n",
    "        \n",
    "        self.import_relations[file_path] = imports\n",
    "        self.module_symbols[file_path] = symbols\n",
    "\n",
    "    def _analyze_go_file(self, file_path: str, content: str) -> None:\n",
    "        \"\"\"Analyze a Go file for imports and symbols.\"\"\"\n",
    "        imports = []\n",
    "        symbols = {}\n",
    "        \n",
    "        # Process content line by line\n",
    "        lines = content.splitlines()\n",
    "        \n",
    "        # Regular expressions for Go code analysis\n",
    "        package_pattern = re.compile(r'package\\s+(\\w+)')\n",
    "        import_single_pattern = re.compile(r'import\\s+\"([^\"]+)\"')\n",
    "        import_multi_start_pattern = re.compile(r'import\\s+\\(')\n",
    "        import_multi_line_pattern = re.compile(r'\\s*\"([^\"]+)\"')\n",
    "        func_pattern = re.compile(r'func\\s+(?:\\([^)]+\\)\\s+)?(\\w+)')\n",
    "        struct_pattern = re.compile(r'type\\s+(\\w+)\\s+struct')\n",
    "        interface_pattern = re.compile(r'type\\s+(\\w+)\\s+interface')\n",
    "        \n",
    "        in_import_block = False\n",
    "        \n",
    "        for line_no, line in enumerate(lines, 1):\n",
    "            # Find package declaration\n",
    "            package_match = package_pattern.search(line)\n",
    "            if package_match:\n",
    "                package_name = package_match.group(1)\n",
    "                imports.append((f\"package {package_name}\", line_no))\n",
    "            \n",
    "            # Handle single-line imports\n",
    "            import_match = import_single_pattern.search(line)\n",
    "            if import_match:\n",
    "                import_name = import_match.group(1)\n",
    "                imports.append((import_name, line_no))\n",
    "            \n",
    "            # Handle multi-line imports\n",
    "            if import_multi_start_pattern.search(line):\n",
    "                in_import_block = True\n",
    "                continue\n",
    "            \n",
    "            if in_import_block:\n",
    "                if line.strip() == ')':\n",
    "                    in_import_block = False\n",
    "                    continue\n",
    "                    \n",
    "                import_line_match = import_multi_line_pattern.search(line)\n",
    "                if import_line_match:\n",
    "                    import_name = import_line_match.group(1)\n",
    "                    imports.append((import_name, line_no))\n",
    "            \n",
    "            # Find function definitions\n",
    "            func_match = func_pattern.search(line)\n",
    "            if func_match:\n",
    "                func_name = func_match.group(1)\n",
    "                context = self._get_context_around_line(file_path, line_no)\n",
    "                symbols[func_name] = {\n",
    "                    'type': 'function',\n",
    "                    'line_no': line_no,\n",
    "                    'context': context\n",
    "                }\n",
    "            \n",
    "            # Find struct definitions\n",
    "            struct_match = struct_pattern.search(line)\n",
    "            if struct_match:\n",
    "                struct_name = struct_match.group(1)\n",
    "                context = self._get_context_around_line(file_path, line_no)\n",
    "                symbols[struct_name] = {\n",
    "                    'type': 'struct',\n",
    "                    'line_no': line_no,\n",
    "                    'context': context\n",
    "                }\n",
    "            \n",
    "            # Find interface definitions\n",
    "            interface_match = interface_pattern.search(line)\n",
    "            if interface_match:\n",
    "                interface_name = interface_match.group(1)\n",
    "                context = self._get_context_around_line(file_path, line_no)\n",
    "                symbols[interface_name] = {\n",
    "                    'type': 'interface',\n",
    "                    'line_no': line_no,\n",
    "                    'context': context\n",
    "                }\n",
    "        \n",
    "        self.import_relations[file_path] = imports\n",
    "        self.module_symbols[file_path] = symbols\n",
    "\n",
    "    def _find_references_in_file(self, file_path: str, content: str, language: str) -> None:\n",
    "        \"\"\"Find references to symbols in a file based on its language.\"\"\"\n",
    "        if language == \"python\":\n",
    "            self._find_references_in_python_file(file_path, content)\n",
    "        elif language == \"cpp\":\n",
    "            self._find_references_in_cpp_file(file_path, content)\n",
    "        elif language == \"java\":\n",
    "            self._find_references_in_java_file(file_path, content)\n",
    "        elif language == \"go\":\n",
    "            self._find_references_in_go_file(file_path, content)\n",
    "\n",
    "    def _find_references_in_python_file(self, file_path: str, content: str) -> None:\n",
    "        \"\"\"Find references to symbols in a Python file.\"\"\"\n",
    "        try:\n",
    "            tree = ast.parse(content)\n",
    "            \n",
    "            for node in ast.walk(tree):\n",
    "                # Find variable references\n",
    "                if isinstance(node, ast.Name) and isinstance(node.ctx, ast.Load):\n",
    "                    symbol_name = node.id\n",
    "                    line_no = node.lineno\n",
    "                    \n",
    "                    # Track reference with context\n",
    "                    if symbol_name not in self.symbol_references:\n",
    "                        self.symbol_references[symbol_name] = []\n",
    "                    \n",
    "                    context = self._get_context_around_line(file_path, line_no)\n",
    "                    self.symbol_references[symbol_name].append((file_path, line_no, context))\n",
    "                \n",
    "                # Find attribute references (e.g., obj.method())\n",
    "                elif isinstance(node, ast.Attribute) and isinstance(node.ctx, ast.Load):\n",
    "                    attr_name = node.attr\n",
    "                    line_no = node.lineno\n",
    "                    \n",
    "                    if attr_name not in self.symbol_references:\n",
    "                        self.symbol_references[attr_name] = []\n",
    "                    \n",
    "                    context = self._get_context_around_line(file_path, line_no)\n",
    "                    self.symbol_references[attr_name].append((file_path, line_no, context))\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error finding references in Python file {file_path}: {e}\")\n",
    "\n",
    "    def _find_references_in_cpp_file(self, file_path: str, content: str) -> None:\n",
    "        \"\"\"Find references to symbols in a C/C++ file.\"\"\"\n",
    "        # Get all symbol names from all files to check for references\n",
    "        all_symbols = set()\n",
    "        for symbols_dict in self.module_symbols.values():\n",
    "            all_symbols.update(symbols_dict.keys())\n",
    "        \n",
    "        # Process content line by line\n",
    "        lines = content.splitlines()\n",
    "        \n",
    "        for line_no, line in enumerate(lines, 1):\n",
    "            # Look for references to any known symbol\n",
    "            for symbol_name in all_symbols:\n",
    "                # Simple pattern matching (would be more robust with proper C++ parsing)\n",
    "                pattern = r'\\b' + re.escape(symbol_name) + r'\\b'\n",
    "                if re.search(pattern, line):\n",
    "                    # Check if this is not the definition line\n",
    "                    if (file_path in self.module_symbols and \n",
    "                        symbol_name in self.module_symbols[file_path] and \n",
    "                        self.module_symbols[file_path][symbol_name]['line_no'] == line_no):\n",
    "                        continue\n",
    "                    \n",
    "                    if symbol_name not in self.symbol_references:\n",
    "                        self.symbol_references[symbol_name] = []\n",
    "                    \n",
    "                    context = self._get_context_around_line(file_path, line_no)\n",
    "                    self.symbol_references[symbol_name].append((file_path, line_no, context))\n",
    "\n",
    "    def _find_references_in_java_file(self, file_path: str, content: str) -> None:\n",
    "        \"\"\"Find references to symbols in a Java file.\"\"\"\n",
    "        # Get all symbol names from all files to check for references\n",
    "        all_symbols = set()\n",
    "        for symbols_dict in self.module_symbols.values():\n",
    "            all_symbols.update(symbols_dict.keys())\n",
    "        \n",
    "        # Process content line by line\n",
    "        lines = content.splitlines()\n",
    "        \n",
    "        for line_no, line in enumerate(lines, 1):\n",
    "            # Skip comment lines and import/package declarations\n",
    "            if (line.strip().startswith(\"//\") or \n",
    "                line.strip().startswith(\"/*\") or \n",
    "                line.strip().startswith(\"import \") or \n",
    "                line.strip().startswith(\"package \")):\n",
    "                continue\n",
    "            \n",
    "            # Look for references to any known symbol\n",
    "            for symbol_name in all_symbols:\n",
    "                # Simple pattern matching with word boundaries\n",
    "                pattern = r'\\b' + re.escape(symbol_name) + r'\\b'\n",
    "                if re.search(pattern, line):\n",
    "                    # Check if this is not the definition line\n",
    "                    if (file_path in self.module_symbols and \n",
    "                        symbol_name in self.module_symbols[file_path] and \n",
    "                        self.module_symbols[file_path][symbol_name]['line_no'] == line_no):\n",
    "                        continue\n",
    "                    \n",
    "                    if symbol_name not in self.symbol_references:\n",
    "                        self.symbol_references[symbol_name] = []\n",
    "                    \n",
    "                    context = self._get_context_around_line(file_path, line_no)\n",
    "                    self.symbol_references[symbol_name].append((file_path, line_no, context))\n",
    "\n",
    "    def _find_references_in_go_file(self, file_path: str, content: str) -> None:\n",
    "        \"\"\"Find references to symbols in a Go file.\"\"\"\n",
    "        # Get all symbol names from all files to check for references\n",
    "        all_symbols = set()\n",
    "        for symbols_dict in self.module_symbols.values():\n",
    "            all_symbols.update(symbols_dict.keys())\n",
    "        \n",
    "        # Process content line by line\n",
    "        lines = content.splitlines()\n",
    "        \n",
    "        for line_no, line in enumerate(lines, 1):\n",
    "            # Skip comment lines and import/package declarations\n",
    "            if (line.strip().startswith(\"//\") or \n",
    "                line.strip().startswith(\"/*\") or \n",
    "                line.strip().startswith(\"import \") or \n",
    "                line.strip().startswith(\"package \")):\n",
    "                continue\n",
    "            \n",
    "            # Look for references to any known symbol\n",
    "            for symbol_name in all_symbols:\n",
    "                # Simple pattern matching with word boundaries\n",
    "                pattern = r'\\b' + re.escape(symbol_name) + r'\\b'\n",
    "                if re.search(pattern, line):\n",
    "                    # Check if this is not the definition line\n",
    "                    if (file_path in self.module_symbols and \n",
    "                        symbol_name in self.module_symbols[file_path] and \n",
    "                        self.module_symbols[file_path][symbol_name]['line_no'] == line_no):\n",
    "                        continue\n",
    "                    \n",
    "                    if symbol_name not in self.symbol_references:\n",
    "                        self.symbol_references[symbol_name] = []\n",
    "                    \n",
    "                    context = self._get_context_around_line(file_path, line_no)\n",
    "                    self.symbol_references[symbol_name].append((file_path, line_no, context))\n",
    "\n",
    "    def _build_symbol_index(self) -> None:\n",
    "        \"\"\"Build a comprehensive index of all symbols and where they're defined/used.\"\"\"\n",
    "        # Initialize the symbol index\n",
    "        self.symbol_index = {}\n",
    "        \n",
    "        # First, add all symbol definitions\n",
    "        for file_path, symbols in self.module_symbols.items():\n",
    "            for symbol_name, details in symbols.items():\n",
    "                if symbol_name not in self.symbol_index:\n",
    "                    self.symbol_index[symbol_name] = []\n",
    "                \n",
    "                self.symbol_index[symbol_name].append({\n",
    "                    'file': file_path,\n",
    "                    'type': 'definition',\n",
    "                    'symbol_type': details['type'],\n",
    "                    'line_no': details['line_no'],\n",
    "                    'context': details.get('context', ''),\n",
    "                    'docstring': details.get('docstring', '')\n",
    "                })\n",
    "        \n",
    "        # Then, add all references\n",
    "        for symbol_name, references in self.symbol_references.items():\n",
    "            if symbol_name not in self.symbol_index:\n",
    "                self.symbol_index[symbol_name] = []\n",
    "            \n",
    "            for file_path, line_no, context in references:\n",
    "                # Avoid duplicating references if they're already in definitions\n",
    "                if not any(ref['file'] == file_path and ref['line_no'] == line_no and ref['type'] == 'definition' \n",
    "                          for ref in self.symbol_index.get(symbol_name, [])):\n",
    "                    self.symbol_index[symbol_name].append({\n",
    "                        'file': file_path,\n",
    "                        'type': 'reference',\n",
    "                        'line_no': line_no,\n",
    "                        'context': context\n",
    "                    })\n",
    "\n",
    "    def build_graph(self) -> nx.DiGraph:\n",
    "        \"\"\"Build the NetworkX graph with enhanced node and edge information.\"\"\"\n",
    "        # We've already added basic file and directory nodes during parsing\n",
    "        # Now add more detailed connections and data\n",
    "        \n",
    "        # Add nodes for all directories (if not already added)\n",
    "        for directory in self.directories:\n",
    "            if not self.graph.has_node(directory):\n",
    "                self.graph.add_node(directory, type='directory')\n",
    "            \n",
    "            # Ensure parent directories exist and are connected\n",
    "            parts = directory.split(os.sep)\n",
    "            for i in range(1, len(parts)):\n",
    "                parent_path = os.sep.join(parts[:i])\n",
    "                if parent_path and not self.graph.has_node(parent_path):\n",
    "                    self.graph.add_node(parent_path, type='directory')\n",
    "                    self.directories.add(parent_path)\n",
    "                \n",
    "                # Connect parent to child directory\n",
    "                if parent_path:\n",
    "                    child_path = os.sep.join(parts[:i+1])\n",
    "                    self.graph.add_edge(parent_path, child_path, edge_type='contains_directory')\n",
    "        \n",
    "        # Add nodes for all files with indices and code snippet nodes\n",
    "        for file_path, file_idx in self.file_index.items():\n",
    "            language = self._detect_language(file_path)\n",
    "            \n",
    "            # Update file node if it exists, create it otherwise\n",
    "            if self.graph.has_node(file_path):\n",
    "                self.graph.nodes[file_path].update({\n",
    "                    'file_index': file_idx,\n",
    "                    'directory': os.path.dirname(file_path),\n",
    "                    'language': language\n",
    "                })\n",
    "            else:\n",
    "                self.graph.add_node(file_path, \n",
    "                                   type='file',\n",
    "                                   file_index=file_idx,\n",
    "                                   directory=os.path.dirname(file_path),\n",
    "                                   language=language)\n",
    "            \n",
    "            # Connect file to its directory\n",
    "            directory = os.path.dirname(file_path)\n",
    "            if directory:\n",
    "                # Make sure the directory node exists\n",
    "                if not self.graph.has_node(directory):\n",
    "                    self.graph.add_node(directory, type='directory')\n",
    "                    self.directories.add(directory)\n",
    "                \n",
    "                # Add edge from directory to file if it doesn't exist\n",
    "                if not self.graph.has_edge(directory, file_path):\n",
    "                    self.graph.add_edge(directory, file_path, edge_type='contains_file')\n",
    "            \n",
    "            # Create snippet nodes for the entire file\n",
    "            if file_path in self.file_contents:\n",
    "                chunks = self._chunk_code(self.file_contents[file_path])\n",
    "                for idx, chunk_info in enumerate(chunks):\n",
    "                    snippet_node = f\"{file_path}::snippet::{idx}\"\n",
    "                    self.graph.add_node(snippet_node,\n",
    "                                       type='snippet',\n",
    "                                       code_snippet=chunk_info['code_snippet'],\n",
    "                                       start_line=chunk_info['start_line'],\n",
    "                                       end_line=chunk_info['end_line'],\n",
    "                                       language=language)\n",
    "                    # Connect file node to snippet node\n",
    "                    self.graph.add_edge(file_path, snippet_node, \n",
    "                                       edge_type='contains_snippet',\n",
    "                                       start_line=chunk_info['start_line'],\n",
    "                                       end_line=chunk_info['end_line'])\n",
    "\n",
    "            # Add nodes for symbols in this file\n",
    "            for symbol, details in self.module_symbols.get(file_path, {}).items():\n",
    "                symbol_node = f\"{file_path}::{symbol}\"\n",
    "                self.graph.add_node(symbol_node, \n",
    "                                   type='symbol',\n",
    "                                   symbol_type=details['type'],\n",
    "                                   line_number=details['line_no'],\n",
    "                                   context=details.get('context', ''),\n",
    "                                   docstring=details.get('docstring', ''))\n",
    "                self.graph.add_edge(file_path, symbol_node, \n",
    "                                   edge_type='defines',\n",
    "                                   line_number=details['line_no'])\n",
    "\n",
    "        # Add edges for imports with line numbers\n",
    "        for file_path, imports in self.import_relations.items():\n",
    "            for imp, line_no in imports:\n",
    "                # Look for matching files or symbols\n",
    "                for target_file, symbols in self.module_symbols.items():\n",
    "                    if imp in symbols:\n",
    "                        self.graph.add_edge(file_path, \n",
    "                                           f\"{target_file}::{imp}\",\n",
    "                                           edge_type='import',\n",
    "                                           line_number=line_no)\n",
    "                    # For Python, handle module imports\n",
    "                    elif self._detect_language(file_path) == \"python\" and target_file.replace('.py', '').endswith(imp):\n",
    "                        self.graph.add_edge(file_path, \n",
    "                                           target_file,\n",
    "                                           edge_type='import',\n",
    "                                           line_number=line_no)\n",
    "                    # For Java, handle package imports\n",
    "                    elif self._detect_language(file_path) == \"java\" and imp.startswith(os.path.splitext(os.path.basename(target_file))[0]):\n",
    "                        self.graph.add_edge(file_path, \n",
    "                                           target_file,\n",
    "                                           edge_type='import',\n",
    "                                           line_number=line_no)\n",
    "        \n",
    "        # Add edges for symbol references\n",
    "        for symbol, references in self.symbol_references.items():\n",
    "            for file_path, line_no, context in references:\n",
    "                # Find symbol nodes that match this reference\n",
    "                for target_file, symbols in self.module_symbols.items():\n",
    "                    if symbol in symbols:\n",
    "                        # Create reference edge\n",
    "                        self.graph.add_edge(file_path, \n",
    "                                           f\"{target_file}::{symbol}\",\n",
    "                                           edge_type='references',\n",
    "                                           line_number=line_no,\n",
    "                                           context=context)\n",
    "        \n",
    "        return self.graph\n",
    "\n",
    "    def export_to_arango(self, url: str, username: str, password: str, db_name: str = \"codebase\", \n",
    "                         graph_name: str = \"Custom_Flask\", node_collection: str = \"nodes\", \n",
    "                         edge_collection: str = \"edges\", overwrite: bool = False) -> None:\n",
    "        \"\"\"\n",
    "        Export the NetworkX graph to ArangoDB.\n",
    "        \n",
    "        Args:\n",
    "            url: ArangoDB server URL\n",
    "            username: ArangoDB username\n",
    "            password: ArangoDB password\n",
    "            db_name: Database name\n",
    "            graph_name: Graph name\n",
    "            node_collection: Node collection name\n",
    "            edge_collection: Edge collection name\n",
    "            overwrite: Whether to overwrite existing database\n",
    "        \"\"\"\n",
    "        # Initialize ArangoDB client\n",
    "        client = ArangoClient(hosts=url)\n",
    "        sys_db = client.db('_system', username=username, password=password)\n",
    "        \n",
    "        # Create or use existing database\n",
    "        if sys_db.has_database(db_name):\n",
    "            if overwrite:\n",
    "                sys_db.delete_database(db_name)\n",
    "                sys_db.create_database(db_name)\n",
    "                print(f\"Database '{db_name}' recreated.\")\n",
    "            else:\n",
    "                print(f\"Using existing database '{db_name}'.\")\n",
    "        else:\n",
    "            sys_db.create_database(db_name)\n",
    "            print(f\"Database '{db_name}' created.\")\n",
    "        \n",
    "        # Connect to the database\n",
    "        db = client.db(db_name, username=username, password=password)\n",
    "        \n",
    "        # Create or use existing collections\n",
    "        if db.has_collection(node_collection):\n",
    "            nodes = db.collection(node_collection)\n",
    "            nodes.truncate()\n",
    "        else:\n",
    "            nodes = db.create_collection(node_collection)\n",
    "        \n",
    "        if db.has_collection(edge_collection):\n",
    "            edges = db.collection(edge_collection)\n",
    "            edges.truncate()\n",
    "        else:\n",
    "            edges = db.create_edge_collection(edge_collection)\n",
    "        \n",
    "        # Create or use existing graph\n",
    "        if db.has_graph(graph_name):\n",
    "            graph = db.graph(graph_name)\n",
    "        else:\n",
    "            graph = db.create_graph(graph_name)\n",
    "            # Define edge definition\n",
    "            graph.create_edge_definition(\n",
    "                edge_collection=edge_collection,\n",
    "                from_vertex_collections=[node_collection],\n",
    "                to_vertex_collections=[node_collection]\n",
    "            )\n",
    "        \n",
    "        # Prepare nodes for ArangoDB (ensuring unique IDs)\n",
    "        node_mapping = {}  # Maps node names to ArangoDB keys\n",
    "        \n",
    "        # Add nodes to ArangoDB\n",
    "        print(\"Adding nodes to ArangoDB...\")\n",
    "        for node_name, node_attrs in self.graph.nodes(data=True):\n",
    "            # Create a sanitized key for ArangoDB\n",
    "            key = re.sub(r'[^a-zA-Z0-9_\\-]', '_', node_name)\n",
    "            node_mapping[node_name] = key\n",
    "            \n",
    "            # Include all attributes and the original node name\n",
    "            node_data = {\n",
    "                '_key': key,\n",
    "                'original_name': node_name\n",
    "            }\n",
    "            node_data.update(node_attrs)\n",
    "            \n",
    "            # Handle special data types for ArangoDB\n",
    "            for attr, value in node_data.items():\n",
    "                if isinstance(value, (set, tuple)):\n",
    "                    node_data[attr] = list(value)\n",
    "            \n",
    "            # Insert the node\n",
    "            nodes.insert(node_data)\n",
    "        \n",
    "        # Add edges to ArangoDB\n",
    "        print(\"Adding edges to ArangoDB...\")\n",
    "        for src, dst, edge_attrs in self.graph.edges(data=True):\n",
    "            # Create edge with proper from/to\n",
    "            edge_data = {\n",
    "                '_from': f\"{node_collection}/{node_mapping[src]}\",\n",
    "                '_to': f\"{node_collection}/{node_mapping[dst]}\"\n",
    "            }\n",
    "            edge_data.update(edge_attrs)\n",
    "            \n",
    "            # Handle special data types for ArangoDB\n",
    "            for attr, value in edge_data.items():\n",
    "                if isinstance(value, (set, tuple)):\n",
    "                    edge_data[attr] = list(value)\n",
    "            \n",
    "            # Insert the edge\n",
    "            edges.insert(edge_data)\n",
    "        \n",
    "        print(f\"Exported graph to ArangoDB: {len(self.graph.nodes())} nodes and {len(self.graph.edges())} edges.\")\n",
    "\n",
    "    def query_database(self, url: str, username: str, password: str, db_name: str = \"codebase\", \n",
    "                      query: str = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Execute a query against the ArangoDB database.\n",
    "        \n",
    "        Args:\n",
    "            url: ArangoDB server URL\n",
    "            username: ArangoDB username\n",
    "            password: ArangoDB password\n",
    "            db_name: Database name\n",
    "            query: AQL query string\n",
    "            \n",
    "        Returns:\n",
    "            Query results as a list of dictionaries\n",
    "        \"\"\"\n",
    "        client = ArangoClient(hosts=url)\n",
    "        db = client.db(db_name, username=username, password=password)\n",
    "        \n",
    "        if query is None:\n",
    "            # Default query to get basic statistics\n",
    "            query = \"\"\"\n",
    "            RETURN {\n",
    "                \"node_count\": LENGTH(FOR v IN nodes RETURN v),\n",
    "                \"edge_count\": LENGTH(FOR e IN edges RETURN e),\n",
    "                \"file_count\": LENGTH(FOR v IN nodes FILTER v.type == 'file' RETURN v),\n",
    "                \"directory_count\": LENGTH(FOR v IN nodes FILTER v.type == 'directory' RETURN v),\n",
    "                \"symbol_count\": LENGTH(FOR v IN nodes FILTER v.type == 'symbol' RETURN v)\n",
    "            }\n",
    "            \"\"\"\n",
    "        \n",
    "        cursor = db.aql.execute(query)\n",
    "        return [doc for doc in cursor]\n",
    "\n",
    "    def export_to_json(self, output_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Export the graph data to a JSON file for backup or analysis outside ArangoDB.\n",
    "        \n",
    "        Args:\n",
    "            output_path: Path to write the JSON file\n",
    "        \"\"\"\n",
    "        data = {\n",
    "            \"nodes\": [],\n",
    "            \"edges\": []\n",
    "        }\n",
    "        \n",
    "        # Export nodes\n",
    "        for node_name, attrs in self.graph.nodes(data=True):\n",
    "            node_data = {\"id\": node_name}\n",
    "            node_data.update(attrs)\n",
    "            data[\"nodes\"].append(node_data)\n",
    "        \n",
    "        # Export edges\n",
    "        for src, dst, attrs in self.graph.edges(data=True):\n",
    "            edge_data = {\n",
    "                \"source\": src,\n",
    "                \"target\": dst\n",
    "            }\n",
    "            edge_data.update(attrs)\n",
    "            data[\"edges\"].append(edge_data)\n",
    "        \n",
    "        # Write to file\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"Exported graph to JSON file: {output_path}\")\n",
    "\n",
    "    def analyze_codebase(self) -> Dict[str, any]:\n",
    "        \"\"\"\n",
    "        Perform basic analysis on the codebase and return statistics.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with analysis results\n",
    "        \"\"\"\n",
    "        stats = {\n",
    "            \"total_files\": len(self.file_index),\n",
    "            \"total_directories\": len(self.directories),\n",
    "            \"total_symbols\": sum(len(symbols) for symbols in self.module_symbols.values()),\n",
    "            \"languages\": {},\n",
    "            \"file_sizes\": {\n",
    "                \"min\": float('inf'),\n",
    "                \"max\": 0,\n",
    "                \"avg\": 0\n",
    "            },\n",
    "            \"symbol_types\": {}\n",
    "        }\n",
    "        \n",
    "        # Count files by language\n",
    "        for file_path in self.file_index:\n",
    "            lang = self._detect_language(file_path)\n",
    "            stats[\"languages\"][lang] = stats[\"languages\"].get(lang, 0) + 1\n",
    "            \n",
    "            # Track file sizes\n",
    "            file_size = len(self.file_contents.get(file_path, \"\"))\n",
    "            stats[\"file_sizes\"][\"min\"] = min(stats[\"file_sizes\"][\"min\"], file_size)\n",
    "            stats[\"file_sizes\"][\"max\"] = max(stats[\"file_sizes\"][\"max\"], file_size)\n",
    "        \n",
    "        # Calculate average file size\n",
    "        if stats[\"total_files\"] > 0:\n",
    "            total_size = sum(len(content) for content in self.file_contents.values())\n",
    "            stats[\"file_sizes\"][\"avg\"] = total_size / stats[\"total_files\"]\n",
    "        else:\n",
    "            stats[\"file_sizes\"][\"min\"] = 0\n",
    "        \n",
    "        # Count symbols by type\n",
    "        for symbols in self.module_symbols.values():\n",
    "            for symbol, details in symbols.items():\n",
    "                symbol_type = details.get(\"type\", \"unknown\")\n",
    "                stats[\"symbol_types\"][symbol_type] = stats[\"symbol_types\"].get(symbol_type, 0) + 1\n",
    "        \n",
    "        return stats\n",
    "\n",
    "    def run_workflow(self, code_path: str, arango_url: str, username: str, password: str, \n",
    "                    db_name: str = \"codebase\") -> Dict:\n",
    "        \"\"\"\n",
    "        Run the complete workflow: parse files, build graph, export to ArangoDB, and analyze.\n",
    "        \n",
    "        Args:\n",
    "            code_path: Path to the codebase\n",
    "            arango_url: ArangoDB server URL\n",
    "            username: ArangoDB username\n",
    "            password: ArangoDB password\n",
    "            db_name: Database name\n",
    "            \n",
    "        Returns:\n",
    "            Analysis results\n",
    "        \"\"\"\n",
    "        print(f\"Processing codebase at: {code_path}\")\n",
    "        \n",
    "        # Parse files\n",
    "        self.parse_files()\n",
    "        print(f\"Parsed {len(self.file_index)} files and {len(self.directories)} directories\")\n",
    "        \n",
    "        # Build graph\n",
    "        self.build_graph()\n",
    "        print(f\"Built graph with {len(self.graph.nodes())} nodes and {len(self.graph.edges())} edges\")\n",
    "        \n",
    "        # Export to ArangoDB\n",
    "        self.export_to_arango(\n",
    "            url=arango_url,\n",
    "            username=username,\n",
    "            password=password,\n",
    "            db_name=db_name,\n",
    "            overwrite=True\n",
    "        )\n",
    "        \n",
    "        # Analyze codebase\n",
    "        analysis = self.analyze_codebase()\n",
    "        print(f\"Analysis complete: {analysis}\")\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def validate_graph_and_data(self) -> dict:\n",
    "        \"\"\"\n",
    "        Validate the parsed data and graph construction.\n",
    "        Returns a detailed report on what was found and potential issues.\n",
    "        \"\"\"\n",
    "        report = {\n",
    "            \"files\": {\n",
    "                \"count\": len(self.file_index),\n",
    "                \"samples\": list(self.file_index.keys())[:5],  # First 5 files\n",
    "                \"extensions\": {}\n",
    "            },\n",
    "            \"directories\": {\n",
    "                \"count\": len(self.directories),\n",
    "                \"samples\": list(self.directories)[:5]  # First 5 directories\n",
    "            },\n",
    "            \"symbols\": {\n",
    "                \"count\": sum(len(symbols) for symbols in self.module_symbols.values()),\n",
    "                \"by_type\": {},\n",
    "                \"samples\": []\n",
    "            },\n",
    "            \"graph\": {\n",
    "                \"nodes\": self.graph.number_of_nodes(),\n",
    "                \"edges\": self.graph.number_of_edges(),\n",
    "                \"node_types\": {},\n",
    "                \"edge_types\": {}\n",
    "            },\n",
    "            \"possible_issues\": []\n",
    "        }\n",
    "        \n",
    "        # Check file extensions\n",
    "        for file_path in self.file_index:\n",
    "            _, ext = os.path.splitext(file_path)\n",
    "            ext = ext.lower()\n",
    "            report[\"files\"][\"extensions\"][ext] = report[\"files\"][\"extensions\"].get(ext, 0) + 1\n",
    "        \n",
    "        # Check for supported extensions\n",
    "        supported_exts = []\n",
    "        for lang, exts in self.language_extensions.items():\n",
    "            supported_exts.extend(exts)\n",
    "        \n",
    "        if set(report[\"files\"][\"extensions\"].keys()).isdisjoint(supported_exts):\n",
    "            report[\"possible_issues\"].append(\"No files with supported extensions found.\")\n",
    "        \n",
    "        # Check symbol types\n",
    "        for file_path, symbols in self.module_symbols.items():\n",
    "            for symbol_name, details in symbols.items():\n",
    "                symbol_type = details['type']\n",
    "                report[\"symbols\"][\"by_type\"][symbol_type] = report[\"symbols\"][\"by_type\"].get(symbol_type, 0) + 1\n",
    "                \n",
    "                if len(report[\"symbols\"][\"samples\"]) < 5:\n",
    "                    report[\"symbols\"][\"samples\"].append({\n",
    "                        \"name\": symbol_name,\n",
    "                        \"file\": file_path,\n",
    "                        \"type\": symbol_type,\n",
    "                        \"line\": details['line_no']\n",
    "                    })\n",
    "        \n",
    "        # Check graph node and edge types\n",
    "        for _, data in self.graph.nodes(data=True):\n",
    "            node_type = data.get('type', 'unknown')\n",
    "            report[\"graph\"][\"node_types\"][node_type] = report[\"graph\"][\"node_types\"].get(node_type, 0) + 1\n",
    "        \n",
    "        for _, _, data in self.graph.edges(data=True):\n",
    "            edge_type = data.get('edge_type', 'unknown')\n",
    "            report[\"graph\"][\"edge_types\"][edge_type] = report[\"graph\"][\"edge_types\"].get(edge_type, 0) + 1\n",
    "        \n",
    "        # Check if nodes match files and directories\n",
    "        if report[\"graph\"][\"node_types\"].get(\"file\", 0) != report[\"files\"][\"count\"]:\n",
    "            report[\"possible_issues\"].append(\n",
    "                f\"Mismatch between file count ({report['files']['count']}) and file nodes in graph ({report['graph']['node_types'].get('file', 0)})\"\n",
    "            )\n",
    "        \n",
    "        if report[\"graph\"][\"node_types\"].get(\"directory\", 0) != report[\"directories\"][\"count\"]:\n",
    "            report[\"possible_issues\"].append(\n",
    "                f\"Mismatch between directory count ({report['directories']['count']}) and directory nodes in graph ({report['graph']['node_types'].get('directory', 0)})\"\n",
    "            )\n",
    "        \n",
    "        # Check if symbols have corresponding nodes\n",
    "        symbol_count = report[\"symbols\"][\"count\"]\n",
    "        symbol_nodes = report[\"graph\"][\"node_types\"].get(\"symbol\", 0)\n",
    "        if symbol_count != symbol_nodes:\n",
    "            report[\"possible_issues\"].append(\n",
    "                f\"Mismatch between symbol count ({symbol_count}) and symbol nodes in graph ({symbol_nodes})\"\n",
    "            )\n",
    "        \n",
    "        # Validate directory structure\n",
    "        if report[\"directories\"][\"count\"] > 0 and report[\"files\"][\"count\"] > 0:\n",
    "            # Check if files are connected to their directories\n",
    "            contains_file_edges = report[\"graph\"][\"edge_types\"].get(\"contains_file\", 0)\n",
    "            if contains_file_edges < report[\"files\"][\"count\"]:\n",
    "                report[\"possible_issues\"].append(\n",
    "                    f\"Some files may not be properly connected to their directories ({contains_file_edges} edges for {report['files']['count']} files)\"\n",
    "                )\n",
    "        \n",
    "        return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph has 2974 nodes and 8795 edges\n",
      "{\n",
      "  \"files\": {\n",
      "    \"count\": 83,\n",
      "    \"samples\": [\n",
      "      \"tests/test_basic.py\",\n",
      "      \"tests/conftest.py\",\n",
      "      \"tests/test_converters.py\",\n",
      "      \"tests/test_logging.py\",\n",
      "      \"tests/test_signals.py\"\n",
      "    ],\n",
      "    \"extensions\": {\n",
      "      \".py\": 83\n",
      "    }\n",
      "  },\n",
      "  \"directories\": {\n",
      "    \"count\": 68,\n",
      "    \"samples\": [\n",
      "      \"docs/patterns\",\n",
      "      \"examples/javascript/tests\",\n",
      "      \"examples/tutorial/flaskr/templates/blog\",\n",
      "      \"examples/javascript/js_example\",\n",
      "      \"tests/test_apps/blueprintapp\"\n",
      "    ]\n",
      "  },\n",
      "  \"symbols\": {\n",
      "    \"count\": 1893,\n",
      "    \"by_type\": {\n",
      "      \"variable\": 660,\n",
      "      \"function\": 1097,\n",
      "      \"class\": 136\n",
      "    },\n",
      "    \"samples\": [\n",
      "      {\n",
      "        \"name\": \"require_cpython_gc\",\n",
      "        \"file\": \"tests/test_basic.py\",\n",
      "        \"type\": \"variable\",\n",
      "        \"line\": 24\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"test_options_work\",\n",
      "        \"file\": \"tests/test_basic.py\",\n",
      "        \"type\": \"function\",\n",
      "        \"line\": 30\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"test_options_on_multiple_rules\",\n",
      "        \"file\": \"tests/test_basic.py\",\n",
      "        \"type\": \"function\",\n",
      "        \"line\": 40\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"test_method_route\",\n",
      "        \"file\": \"tests/test_basic.py\",\n",
      "        \"type\": \"function\",\n",
      "        \"line\": 54\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"test_method_route_no_methods\",\n",
      "        \"file\": \"tests/test_basic.py\",\n",
      "        \"type\": \"function\",\n",
      "        \"line\": 65\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"graph\": {\n",
      "    \"nodes\": 2974,\n",
      "    \"edges\": 8795,\n",
      "    \"node_types\": {\n",
      "      \"directory\": 68,\n",
      "      \"file\": 83,\n",
      "      \"snippet\": 930,\n",
      "      \"symbol\": 1893\n",
      "    },\n",
      "    \"edge_types\": {\n",
      "      \"contains_file\": 83,\n",
      "      \"contains_directory\": 60,\n",
      "      \"contains_snippet\": 930,\n",
      "      \"references\": 6719,\n",
      "      \"defines\": 942,\n",
      "      \"import\": 61\n",
      "    }\n",
      "  },\n",
      "  \"possible_issues\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "visualizer = CodebaseVisualizer(root_dir=\"flask\")\n",
    "visualizer.parse_files()\n",
    "G = visualizer.build_graph()\n",
    "print(f\"Graph has {len(G.nodes())} nodes and {len(G.edges())} edges\")\n",
    "\n",
    "report = visualizer.validate_graph_and_data()\n",
    "print(json.dumps(report, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T06:32:46.868174Z",
     "iopub.status.busy": "2025-03-04T06:32:46.867712Z",
     "iopub.status.idle": "2025-03-04T06:32:46.872269Z",
     "shell.execute_reply": "2025-03-04T06:32:46.871147Z",
     "shell.execute_reply.started": "2025-03-04T06:32:46.868142Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#visualizer.export_to_arango(db_name=\"FlaskRepv1\",username=\"root\",password=\"cUZ0YaNdcwfUTw6VjRny\",host=\"https://d2eeb8083350.arangodb.cloud:8529\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T06:32:47.073946Z",
     "iopub.status.busy": "2025-03-04T06:32:47.073567Z",
     "iopub.status.idle": "2025-03-04T06:32:47.078449Z",
     "shell.execute_reply": "2025-03-04T06:32:47.077323Z",
     "shell.execute_reply.started": "2025-03-04T06:32:47.073914Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# visualizer = CodebaseVisualizer(\"flask\")\n",
    "# visualizer.parse_files()\n",
    "# G = visualizer.build_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T06:32:48.213329Z",
     "iopub.status.busy": "2025-03-04T06:32:48.212975Z",
     "iopub.status.idle": "2025-03-04T06:32:48.219590Z",
     "shell.execute_reply": "2025-03-04T06:32:48.218354Z",
     "shell.execute_reply.started": "2025-03-04T06:32:48.213302Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# class GraphVisualizer:\n",
    "#     def __init__(self, graph: nx.Graph):\n",
    "#         self.graph = graph\n",
    "#         self.pos = None\n",
    "        \n",
    "#     def set_layout(self, layout_type: str = 'spring', **layout_params) -> None:\n",
    "#         \"\"\"\n",
    "#         Set the layout for the graph visualization.\n",
    "        \n",
    "#         Args:\n",
    "#             layout_type: Type of layout ('spring', 'circular', 'kamada_kawai', \n",
    "#                         'random', 'shell', 'spectral')\n",
    "#             layout_params: Additional parameters for the layout algorithm\n",
    "#         \"\"\"\n",
    "#         layout_funcs = {\n",
    "#             'spring': nx.spring_layout,\n",
    "#             'circular': nx.circular_layout,\n",
    "#             'kamada_kawai': nx.kamada_kawai_layout,\n",
    "#             'random': nx.random_layout,\n",
    "#             'shell': nx.shell_layout,\n",
    "#             'spectral': nx.spectral_layout\n",
    "#         }\n",
    "        \n",
    "#         if layout_type not in layout_funcs:\n",
    "#             raise ValueError(f\"Unsupported layout type. Choose from: {list(layout_funcs.keys())}\")\n",
    "            \n",
    "#         self.pos = layout_funcs[layout_type](self.graph, **layout_params)\n",
    "    \n",
    "#     def _get_node_colors(self) -> Dict[str, str]:\n",
    "#         \"\"\"Extract node colors from graph attributes or generate defaults.\"\"\"\n",
    "#         colors = {}\n",
    "#         for node in self.graph.nodes():\n",
    "#             # Check for color in node attributes\n",
    "#             attrs = self.graph.nodes[node]\n",
    "#             if 'fillcolor' in attrs:\n",
    "#                 colors[node] = attrs['fillcolor']\n",
    "#             elif 'color' in attrs:\n",
    "#                 colors[node] = attrs['color']\n",
    "#             else:\n",
    "#                 colors[node] = 'lightblue'  # default color\n",
    "#         return colors\n",
    "    \n",
    "#     def _get_node_sizes(self) -> Dict[str, float]:\n",
    "#         \"\"\"Extract or compute node sizes.\"\"\"\n",
    "#         sizes = {}\n",
    "#         for node in self.graph.nodes():\n",
    "#             attrs = self.graph.nodes[node]\n",
    "#             if 'size' in attrs:\n",
    "#                 sizes[node] = attrs['size']\n",
    "#             else:\n",
    "#                 # Default size based on node degree\n",
    "#                 sizes[node] = 1000 * (1 + self.graph.degree(node) / 10)\n",
    "#         return sizes\n",
    "    \n",
    "#     def _get_edge_colors(self) -> Dict[Tuple[str, str], str]:\n",
    "#         \"\"\"Extract edge colors from graph attributes or generate defaults.\"\"\"\n",
    "#         colors = {}\n",
    "#         for u, v in self.graph.edges():\n",
    "#             edge_data = self.graph.get_edge_data(u, v)\n",
    "#             if 'color' in edge_data:\n",
    "#                 colors[(u, v)] = edge_data['color']\n",
    "#             else:\n",
    "#                 colors[(u, v)] = 'gray'  # default color\n",
    "#         return colors\n",
    "    \n",
    "#     def _get_node_labels(self) -> Dict[str, str]:\n",
    "#         \"\"\"Extract node labels from graph attributes.\"\"\"\n",
    "#         labels = {}\n",
    "#         for node in self.graph.nodes():\n",
    "#             attrs = self.graph.nodes[node]\n",
    "#             if 'label' in attrs:\n",
    "#                 labels[node] = attrs['label']\n",
    "#             else:\n",
    "#                 labels[node] = str(node)\n",
    "#         return labels\n",
    "    \n",
    "#     def _get_edge_labels(self) -> Dict[Tuple[str, str], str]:\n",
    "#         \"\"\"Extract edge labels from graph attributes.\"\"\"\n",
    "#         labels = {}\n",
    "#         for u, v in self.graph.edges():\n",
    "#             edge_data = self.graph.get_edge_data(u, v)\n",
    "#             if 'label' in edge_data:\n",
    "#                 labels[(u, v)] = edge_data['label']\n",
    "#         return labels\n",
    "\n",
    "#     def visualize(self, \n",
    "#                  figsize: Tuple[int, int] = (12, 8),\n",
    "#                  node_size: Optional[Dict[str, float]] = None,\n",
    "#                  node_color: Optional[Dict[str, str]] = None,\n",
    "#                  edge_color: Optional[Dict[Tuple[str, str], str]] = None,\n",
    "#                  with_labels: bool = True,\n",
    "#                  font_size: int = 8,\n",
    "#                  title: Optional[str] = None,\n",
    "#                  show_edge_labels: bool = True,\n",
    "#                  alpha: float = 0.7,\n",
    "#                  save_path: Optional[str] = None) -> None:\n",
    "#         \"\"\"\n",
    "#         Visualize the graph with customizable options.\n",
    "        \n",
    "#         Args:\n",
    "#             figsize: Size of the figure (width, height)\n",
    "#             node_size: Dictionary mapping nodes to their sizes\n",
    "#             node_color: Dictionary mapping nodes to their colors\n",
    "#             edge_color: Dictionary mapping edges to their colors\n",
    "#             with_labels: Whether to show node labels\n",
    "#             font_size: Size of the font for labels\n",
    "#             title: Title of the graph\n",
    "#             show_edge_labels: Whether to show edge labels\n",
    "#             alpha: Transparency of nodes\n",
    "#             save_path: Path to save the visualization (if None, displays instead)\n",
    "#         \"\"\"\n",
    "#         if self.pos is None:\n",
    "#             self.set_layout('spring')\n",
    "            \n",
    "#         plt.figure(figsize=figsize)\n",
    "        \n",
    "#         # Get or use provided node attributes\n",
    "#         node_colors = node_color if node_color is not None else self._get_node_colors()\n",
    "#         node_sizes = node_size if node_size is not None else self._get_node_sizes()\n",
    "#         edge_colors = edge_color if edge_color is not None else self._get_edge_colors()\n",
    "        \n",
    "#         # Draw nodes\n",
    "#         nx.draw_networkx_nodes(self.graph, self.pos,\n",
    "#                              node_color=[node_colors[node] for node in self.graph.nodes()],\n",
    "#                              node_size=[node_sizes[node] for node in self.graph.nodes()],\n",
    "#                              alpha=alpha)\n",
    "        \n",
    "#         # Draw edges\n",
    "#         for (u, v) in self.graph.edges():\n",
    "#             nx.draw_networkx_edges(self.graph, self.pos,\n",
    "#                                  edgelist=[(u, v)],\n",
    "#                                  edge_color=edge_colors.get((u, v), 'gray'),\n",
    "#                                  alpha=0.5)\n",
    "        \n",
    "#         # Add labels if requested\n",
    "#         if with_labels:\n",
    "#             labels = self._get_node_labels()\n",
    "#             nx.draw_networkx_labels(self.graph, self.pos, labels,\n",
    "#                                   font_size=font_size)\n",
    "        \n",
    "#         # Add edge labels if requested\n",
    "#         if show_edge_labels:\n",
    "#             edge_labels = self._get_edge_labels()\n",
    "#             if edge_labels:\n",
    "#                 nx.draw_networkx_edge_labels(self.graph, self.pos,\n",
    "#                                            edge_labels=edge_labels,\n",
    "#                                            font_size=font_size-2)\n",
    "        \n",
    "#         if title:\n",
    "#             plt.title(title)\n",
    "        \n",
    "#         plt.axis('off')\n",
    "        \n",
    "#         if save_path:\n",
    "#             plt.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "#             plt.close()\n",
    "#         else:\n",
    "#             plt.show()\n",
    "\n",
    "# # Example usage:\n",
    "# '''\n",
    "# # Create a sample graph\n",
    "# G = nx.Graph()\n",
    "# G.add_nodes_from([\n",
    "#     (1, {'fillcolor': 'lightblue', 'label': 'Node 1'}),\n",
    "#     (2, {'fillcolor': 'lightgreen', 'label': 'Node 2'}),\n",
    "#     (3, {'fillcolor': 'lightred', 'label': 'Node 3'})\n",
    "# ])\n",
    "# G.add_edges_from([\n",
    "#     (1, 2, {'color': 'blue', 'label': 'Edge 1-2'}),\n",
    "#     (2, 3, {'color': 'red', 'label': 'Edge 2-3'})\n",
    "# ])\n",
    "# '''\n",
    "# # Create visualizer and display graph\n",
    "# visualizer = GraphVisualizer(G)\n",
    "# visualizer.set_layout('spring', k=2)  # k controls the spacing between nodes\n",
    "# visualizer.visualize(\n",
    "#     figsize=(10, 8),\n",
    "#     font_size=10,\n",
    "#     title=\"Sample Graph Visualization\",\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T06:32:50.079458Z",
     "iopub.status.busy": "2025-03-04T06:32:50.078973Z",
     "iopub.status.idle": "2025-03-04T06:32:50.103278Z",
     "shell.execute_reply": "2025-03-04T06:32:50.101877Z",
     "shell.execute_reply.started": "2025-03-04T06:32:50.079408Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pyvis.network import Network\n",
    "\n",
    "def visualize_codebase_graph(codebase_path, output_html=\"codebase_graph.html\", limit_nodes=None):\n",
    "    \"\"\"\n",
    "    Visualize the codebase graph using pyvis.\n",
    "    \n",
    "    Args:\n",
    "        codebase_path: Path to the codebase directory\n",
    "        output_html: Output HTML file for the visualization/\n",
    "        limit_nodes: Optional limit on the number of nodes to display (for large codebases)\n",
    "    \"\"\"\n",
    "    # Initialize and build the graph\n",
    "    visualizer = CodebaseVisualizer(codebase_path)\n",
    "    visualizer.parse_files()\n",
    "    graph = visualizer.build_graph()\n",
    "    \n",
    "    # Export graph to JSON (optional)\n",
    "    visualizer.export_graph_json('codebase_graph.json')\n",
    "    \n",
    "    # Create a pyvis network\n",
    "    net = Network(height=\"900px\", width=\"100%\", bgcolor=\"#222222\", font_color=\"white\")\n",
    "    \n",
    "    # Configure physics\n",
    "    net.barnes_hut(gravity=-5000, central_gravity=0.3, spring_length=200)\n",
    "    \n",
    "    # Define node groups and colors\n",
    "    node_colors = {\n",
    "        'file': '#4287f5',\n",
    "        'directory': '#42f5a7',\n",
    "        'symbol': '#f542cb',\n",
    "        'snippet': '#f5a742'\n",
    "    }\n",
    "    \n",
    "    # If we need to limit nodes for performance\n",
    "    if limit_nodes and len(graph.nodes()) > limit_nodes:\n",
    "        # Focus on file and directory nodes, and limit symbol nodes\n",
    "        important_nodes = [node for node, data in graph.nodes(data=True) \n",
    "                          if data.get('type') in ['file', 'directory']]\n",
    "        \n",
    "        # Add some symbol nodes to reach the limit\n",
    "        symbols = [node for node, data in graph.nodes(data=True) \n",
    "                  if data.get('type') == 'symbol']\n",
    "        \n",
    "        # Take a subset of symbols based on connectivity\n",
    "        symbol_importance = sorted(\n",
    "            [(node, graph.degree(node)) for node in symbols],\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        important_symbols = [node for node, _ in symbol_importance[:limit_nodes - len(important_nodes)]]\n",
    "        selected_nodes = important_nodes + important_symbols\n",
    "        \n",
    "        # Create a subgraph\n",
    "        graph = graph.subgraph(selected_nodes)\n",
    "    \n",
    "    # Add nodes with appropriate styles\n",
    "    for node, node_data in graph.nodes(data=True):\n",
    "        node_type = node_data.get('type', 'unknown')\n",
    "        label = os.path.basename(node) if '/' in node else node\n",
    "        \n",
    "        # Truncate very long labels\n",
    "        if len(label) > 30:\n",
    "            label = label[:27] + \"...\"\n",
    "        \n",
    "        # Create hover title with more details\n",
    "        title = f\"<div style='max-width:300px;'>\"\n",
    "        title += f\"<b>{node}</b><br>\"\n",
    "        title += f\"Type: {node_type}<br>\"\n",
    "        \n",
    "        if node_type == 'file':\n",
    "            title += f\"Directory: {node_data.get('directory', 'N/A')}<br>\"\n",
    "            \n",
    "        elif node_type == 'symbol':\n",
    "            title += f\"Symbol type: {node_data.get('symbol_type', 'N/A')}<br>\"\n",
    "            title += f\"Line: {node_data.get('line_number', 'N/A')}<br>\"\n",
    "            \n",
    "            if node_data.get('docstring'):\n",
    "                docstring = node_data['docstring']\n",
    "                if len(docstring) > 200:\n",
    "                    docstring = docstring[:197] + \"...\"\n",
    "                title += f\"Docstring: {docstring}<br>\"\n",
    "                \n",
    "        elif node_type == 'snippet':\n",
    "            title += f\"Lines: {node_data.get('start_line', 'N/A')}-{node_data.get('end_line', 'N/A')}<br>\"\n",
    "            \n",
    "            if node_data.get('code_snippet'):\n",
    "                snippet = node_data['code_snippet'].replace('\\n', '<br>')\n",
    "                if len(snippet) > 300:\n",
    "                    snippet = snippet[:297] + \"...\"\n",
    "                title += f\"Code:<br><pre>{snippet}</pre>\"\n",
    "                \n",
    "        title += \"</div>\"\n",
    "        \n",
    "        # Determine node size based on type and connections\n",
    "        size = 15  # Default size\n",
    "        if node_type == 'directory':\n",
    "            size = 25\n",
    "        elif node_type == 'file':\n",
    "            size = 20\n",
    "        elif node_type == 'symbol' and node_data.get('symbol_type') == 'class':\n",
    "            size = 18\n",
    "        \n",
    "        # Add node with appropriate styling\n",
    "        net.add_node(\n",
    "            node, \n",
    "            label=label, \n",
    "            title=title,\n",
    "            color=node_colors.get(node_type, '#999999'),\n",
    "            size=size,\n",
    "            shape='dot' if node_type != 'directory' else 'diamond'\n",
    "        )\n",
    "    \n",
    "    # Add edges with appropriate styles\n",
    "    for source, target, edge_data in graph.edges(data=True):\n",
    "        edge_type = edge_data.get('edge_type', 'unknown')\n",
    "        \n",
    "        # Style edges differently based on type\n",
    "        if edge_type == 'import':\n",
    "            color = '#f5f542'\n",
    "            width = 2\n",
    "            dashes = False\n",
    "        elif edge_type == 'references':\n",
    "            color = '#f54242'\n",
    "            width = 1\n",
    "            dashes = [5, 5]\n",
    "        elif edge_type == 'defines':\n",
    "            color = '#42f55a'\n",
    "            width = 3\n",
    "            dashes = False\n",
    "        elif edge_type == 'contains_snippet':\n",
    "            color = '#42c8f5'\n",
    "            width = 1\n",
    "            dashes = [2, 2]\n",
    "        else:\n",
    "            color = '#999999'\n",
    "            width = 1\n",
    "            dashes = False\n",
    "        \n",
    "        # Create hover title with edge details\n",
    "        title = f\"<div><b>{edge_type}</b><br>\"\n",
    "        \n",
    "        if edge_data.get('line_number'):\n",
    "            title += f\"Line: {edge_data['line_number']}<br>\"\n",
    "            \n",
    "        if edge_data.get('context'):\n",
    "            context = edge_data['context']\n",
    "            if len(context) > 200:\n",
    "                context = context[:197] + \"...\"\n",
    "            title += f\"Context: {context}\"\n",
    "            \n",
    "        title += \"</div>\"\n",
    "        \n",
    "        # Add edge with styling\n",
    "        net.add_edge(\n",
    "            source, \n",
    "            target, \n",
    "            title=title,\n",
    "            color=color,\n",
    "            width=width,\n",
    "            dashes=dashes\n",
    "        )\n",
    "    \n",
    "    # Enable physics, navigation and interaction options\n",
    "    net.toggle_physics(True)\n",
    "    net.show_buttons(filter_=['physics'])\n",
    "    \n",
    "    # Save the visualization\n",
    "    net.save_graph(output_html)\n",
    "    print(f\"Graph visualization saved to {output_html}\")\n",
    "    \n",
    "    return output_html\n",
    "\n",
    "def visualize_symbol_subgraph(visualizer, symbol_name, output_html=\"symbol_graph.html\"):\n",
    "    \"\"\"\n",
    "    Create a focused visualization of a symbol and its relationships.\n",
    "    \n",
    "    Args:\n",
    "        visualizer: Initialized CodebaseVisualizer instance\n",
    "        symbol_name: Name of the symbol to visualize\n",
    "        output_html: Output HTML file for the visualization\n",
    "    \"\"\"\n",
    "    # Get the full graph\n",
    "    full_graph = visualizer.graph\n",
    "    \n",
    "    # Find all nodes related to this symbol\n",
    "    symbol_nodes = [node for node in full_graph.nodes() if f\"::{symbol_name}\" in node]\n",
    "    \n",
    "    if not symbol_nodes:\n",
    "        print(f\"Symbol '{symbol_name}' not found in the graph.\")\n",
    "        return None\n",
    "    \n",
    "    # Get nodes that are connected to symbol nodes (1-hop neighborhood)\n",
    "    related_nodes = set(symbol_nodes)\n",
    "    for node in symbol_nodes:\n",
    "        # Add predecessors (nodes that reference this symbol)\n",
    "        related_nodes.update(full_graph.predecessors(node))\n",
    "        # Add successors (nodes that this symbol references)\n",
    "        related_nodes.update(full_graph.successors(node))\n",
    "    \n",
    "    # Create a subgraph\n",
    "    subgraph = full_graph.subgraph(related_nodes)\n",
    "    \n",
    "    # Create a pyvis network\n",
    "    net = Network(height=\"750px\", width=\"100%\", bgcolor=\"#222222\", font_color=\"white\")\n",
    "    net.barnes_hut(gravity=-2000, central_gravity=0.3, spring_length=150)\n",
    "    \n",
    "    # Node colors by type\n",
    "    node_colors = {\n",
    "        'file': '#4287f5',\n",
    "        'directory': '#42f5a7',\n",
    "        'symbol': '#f542cb',\n",
    "        'snippet': '#f5a742'\n",
    "    }\n",
    "    \n",
    "    # Add nodes with appropriate styles\n",
    "    for node, node_data in subgraph.nodes(data=True):\n",
    "        node_type = node_data.get('type', 'unknown')\n",
    "        label = os.path.basename(node) if '/' in node else node\n",
    "        \n",
    "        # Make the focus symbol nodes larger and highlighted\n",
    "        if node in symbol_nodes:\n",
    "            size = 30\n",
    "            color = '#ff0000'  # Bright red for focus\n",
    "        else:\n",
    "            size = 15\n",
    "            color = node_colors.get(node_type, '#999999')\n",
    "        \n",
    "        # Add hover information\n",
    "        title = f\"<div style='max-width:300px;'>\"\n",
    "        title += f\"<b>{node}</b><br>\"\n",
    "        title += f\"Type: {node_type}<br>\"\n",
    "        \n",
    "        if node_type == 'symbol':\n",
    "            title += f\"Symbol type: {node_data.get('symbol_type', 'N/A')}<br>\"\n",
    "            if node_data.get('docstring'):\n",
    "                title += f\"Docstring: {node_data.get('docstring', '')}<br>\"\n",
    "                \n",
    "        title += \"</div>\"\n",
    "        \n",
    "        # Add node with styling\n",
    "        net.add_node(\n",
    "            node,\n",
    "            label=label,\n",
    "            title=title,\n",
    "            color=color,\n",
    "            size=size\n",
    "        )\n",
    "    \n",
    "    # Add edges with styles\n",
    "    for source, target, edge_data in subgraph.edges(data=True):\n",
    "        edge_type = edge_data.get('edge_type', 'unknown')\n",
    "        \n",
    "        # Style edges by type\n",
    "        if edge_type == 'import':\n",
    "            color = '#f5f542'  # Yellow\n",
    "        elif edge_type == 'references':\n",
    "            color = '#f54242'  # Red\n",
    "        elif edge_type == 'defines':\n",
    "            color = '#42f55a'  # Green\n",
    "        else:\n",
    "            color = '#999999'  # Gray\n",
    "        \n",
    "        # Add edge with details in hover\n",
    "        title = f\"{edge_type}\"\n",
    "        if edge_data.get('line_number'):\n",
    "            title += f\" (line {edge_data['line_number']})\"\n",
    "            \n",
    "        net.add_edge(source, target, title=title, color=color)\n",
    "    \n",
    "    # Enable physics and navigation\n",
    "    net.toggle_physics(True)\n",
    "    net.show_buttons(filter_=['physics'])\n",
    "    \n",
    "    # Save the visualization\n",
    "    net.save_graph(output_html)\n",
    "    print(f\"Symbol graph visualization saved to {output_html}\")\n",
    "    \n",
    "    return output_html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T11:54:20.555980Z",
     "iopub.status.busy": "2025-03-01T11:54:20.555647Z",
     "iopub.status.idle": "2025-03-01T11:54:22.195505Z",
     "shell.execute_reply": "2025-03-01T11:54:22.194779Z",
     "shell.execute_reply.started": "2025-03-01T11:54:20.555958Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'visualize_codebase_graph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Visualize the entire codebase graph (with node limit for performance)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mvisualize_codebase_graph\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflask\u001b[39m\u001b[38;5;124m\"\u001b[39m, limit_nodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Visualize a specific symbol (like \"has_level_handler\")\u001b[39;00m\n\u001b[1;32m      5\u001b[0m visualizer \u001b[38;5;241m=\u001b[39m CodebaseVisualizer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflask\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'visualize_codebase_graph' is not defined"
     ]
    }
   ],
   "source": [
    "# # Visualize the entire codebase graph (with node limit for performance)\n",
    "# visualize_codebase_graph(\"flask\", limit_nodes=200)\n",
    "\n",
    "# # Visualize a specific symbol (like \"has_level_handler\")\n",
    "# visualizer = CodebaseVisualizer(\"flask\")\n",
    "# visualizer.parse_files()\n",
    "# G = visualizer.build_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T11:54:26.520322Z",
     "iopub.status.busy": "2025-03-01T11:54:26.519967Z",
     "iopub.status.idle": "2025-03-01T11:54:26.947993Z",
     "shell.execute_reply": "2025-03-01T11:54:26.947235Z",
     "shell.execute_reply.started": "2025-03-01T11:54:26.520291Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<StandardDatabase _system>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "db = ArangoClient(hosts=\"https://d2eeb8083350.arangodb.cloud:8529\").db(username=\"root\", password=\"cUZ0YaNdcwfUTw6VjRny\", verify=True)\n",
    "\n",
    "print(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T06:29:42.589733Z",
     "iopub.status.busy": "2025-03-04T06:29:42.589269Z",
     "iopub.status.idle": "2025-03-04T06:29:42.599518Z",
     "shell.execute_reply": "2025-03-04T06:29:42.597248Z",
     "shell.execute_reply.started": "2025-03-04T06:29:42.589696Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:38:16 +0530] [INFO]: Overwriting graph 'FlaskRepv1'\n",
      "[19:38:17 +0530] [INFO]: Graph 'FlaskRepv1' exists.\n",
      "[19:38:17 +0530] [INFO]: Default node type set to 'FlaskRepv1_node'\n",
      "[2025/03/09 19:38:19 +0530] [41950] [INFO] - adbnx_adapter: Instantiated ADBNX_Adapter with database '_system'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ReadTimeout",
     "evalue": "HTTPSConnectionPool(host='d2eeb8083350.arangodb.cloud', port=8529): Read timed out. (read timeout=60)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/GitHub/scopium/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/GitHub/scopium/.venv/lib/python3.10/site-packages/urllib3/connection.py:516\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/socket.py:717\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 717\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py:1307\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1304\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1305\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1306\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py:1163\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mTimeoutError\u001b[0m: The read operation timed out",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m~/GitHub/scopium/.venv/lib/python3.10/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/GitHub/scopium/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:841\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    839\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[0;32m--> 841\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    844\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/GitHub/scopium/.venv/lib/python3.10/site-packages/urllib3/util/retry.py:474\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_method_retryable(method):\n\u001b[0;32m--> 474\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/GitHub/scopium/.venv/lib/python3.10/site-packages/urllib3/util/util.py:39\u001b[0m, in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/GitHub/scopium/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m~/GitHub/scopium/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 536\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_timeout\u001b[49m\u001b[43m(\u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/GitHub/scopium/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:367\u001b[0m, in \u001b[0;36mHTTPConnectionPool._raise_timeout\u001b[0;34m(self, err, url, timeout_value)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(err, SocketTimeout):\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeoutError(\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28mself\u001b[39m, url, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRead timed out. (read timeout=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    369\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;66;03m# See the above comment about EAGAIN in Python 3.\u001b[39;00m\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m: HTTPSConnectionPool(host='d2eeb8083350.arangodb.cloud', port=8529): Read timed out. (read timeout=60)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mReadTimeout\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m G_adb \u001b[38;5;241m=\u001b[39m \u001b[43mnxadb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGraph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFlaskRepv1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mincoming_graph_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrite_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# feel free to modify\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverwrite_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(G_adb)\n",
      "File \u001b[0;32m~/GitHub/scopium/.venv/lib/python3.10/site-packages/nx_arangodb/classes/graph.py:236\u001b[0m, in \u001b[0;36mGraph.__init__\u001b[0;34m(self, incoming_graph_data, name, default_node_type, edge_type_key, edge_type_func, edge_collections_attributes, db, read_parallelism, read_batch_size, write_batch_size, write_async, symmetrize_edges, use_arango_views, overwrite_graph, *args, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__set_arangodb_backend_config()\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(incoming_graph_data, nx\u001b[38;5;241m.\u001b[39mGraph):\n\u001b[0;32m--> 236\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_nx_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mincoming_graph_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrite_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrite_async\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loaded_incoming_graph_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/GitHub/scopium/.venv/lib/python3.10/site-packages/nx_arangodb/classes/graph.py:489\u001b[0m, in \u001b[0;36mGraph._load_nx_graph\u001b[0;34m(self, nx_graph, write_batch_size, write_async)\u001b[0m\n\u001b[1;32m    486\u001b[0m     controller \u001b[38;5;241m=\u001b[39m SmartController\n\u001b[1;32m    487\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing smart field \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msmart_field\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for node keys\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 489\u001b[0m \u001b[43mADBNX_Adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontroller\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetworkx_to_arangodb\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madb_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnx_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_async\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_async\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GitHub/scopium/.venv/lib/python3.10/site-packages/adbnx_adapter/adapter.py:369\u001b[0m, in \u001b[0;36mADBNX_Adapter.networkx_to_arangodb\u001b[0;34m(self, name, nx_graph, edge_definitions, orphan_collections, overwrite_graph, batch_size, use_async, **adb_import_kwargs)\u001b[0m\n\u001b[1;32m    364\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__insert_adb_docs(\n\u001b[1;32m    365\u001b[0m                 spinner_progress, adb_docs, use_async, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39madb_import_kwargs\n\u001b[1;32m    366\u001b[0m             )\n\u001b[1;32m    368\u001b[0m     \u001b[38;5;66;03m# Insert remaining nodes\u001b[39;00m\n\u001b[0;32m--> 369\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__insert_adb_docs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspinner_progress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madb_docs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_async\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madb_import_kwargs\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;66;03m##################\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;66;03m# NetworkX Edges #\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;66;03m##################\u001b[39;00m\n\u001b[1;32m    377\u001b[0m from_node_id: NxId\n",
      "File \u001b[0;32m~/GitHub/scopium/.venv/lib/python3.10/site-packages/adbnx_adapter/adapter.py:750\u001b[0m, in \u001b[0;36mADBNX_Adapter.__insert_adb_docs\u001b[0;34m(self, spinner_progress, adb_docs, use_async, **adb_import_kwargs)\u001b[0m\n\u001b[1;32m    747\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mADB Import: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(doc_list)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    748\u001b[0m spinner_progress_task \u001b[38;5;241m=\u001b[39m spinner_progress\u001b[38;5;241m.\u001b[39madd_task(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, action\u001b[38;5;241m=\u001b[39maction)\n\u001b[0;32m--> 750\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_bulk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madb_import_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    751\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(result)\n\u001b[1;32m    753\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m adb_docs[col]\n",
      "File \u001b[0;32m~/GitHub/scopium/.venv/lib/python3.10/site-packages/arango/collection.py:2524\u001b[0m, in \u001b[0;36mCollection.import_bulk\u001b[0;34m(self, documents, halt_on_error, details, from_prefix, to_prefix, overwrite, on_duplicate, sync, batch_size)\u001b[0m\n\u001b[1;32m   2515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2516\u001b[0m     request \u001b[38;5;241m=\u001b[39m Request(\n\u001b[1;32m   2517\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2518\u001b[0m         endpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/_api/import\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2521\u001b[0m         write\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m   2522\u001b[0m     )\n\u001b[0;32m-> 2524\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_handler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2525\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2526\u001b[0m     results \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/GitHub/scopium/.venv/lib/python3.10/site-packages/arango/api.py:74\u001b[0m, in \u001b[0;36mApiGroup._execute\u001b[0;34m(self, request, response_handler)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_execute\u001b[39m(\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28mself\u001b[39m, request: Request, response_handler: Callable[[Response], T]\n\u001b[1;32m     65\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Result[T]:\n\u001b[1;32m     66\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Execute an API.\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m    :param request: HTTP request.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m    :return: API execution result.\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_handler\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GitHub/scopium/.venv/lib/python3.10/site-packages/arango/executor.py:109\u001b[0m, in \u001b[0;36mAsyncApiExecutor.execute\u001b[0;34m(self, request, response_handler)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m     request\u001b[38;5;241m.\u001b[39mheaders[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx-arango-async\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 109\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mis_success:\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m AsyncExecuteError(resp, request)\n",
      "File \u001b[0;32m~/GitHub/scopium/.venv/lib/python3.10/site-packages/arango/connection.py:311\u001b[0m, in \u001b[0;36mBasicConnection.send_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Send an HTTP request to ArangoDB server.\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \n\u001b[1;32m    305\u001b[0m \u001b[38;5;124;03m:param request: HTTP request.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;124;03m:rtype: arango.response.Response\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    310\u001b[0m host_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_host_resolver\u001b[38;5;241m.\u001b[39mget_host_index()\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_auth\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GitHub/scopium/.venv/lib/python3.10/site-packages/arango/connection.py:156\u001b[0m, in \u001b[0;36mBaseConnection.process_request\u001b[0;34m(self, host_index, request, auth)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m tries \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_host_resolver\u001b[38;5;241m.\u001b[39mmax_tries:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 156\u001b[0m         resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_http\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m            \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sessions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mhost_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_url_prefixes\u001b[49m\u001b[43m[\u001b[49m\u001b[43mhost_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m            \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_response(resp, request\u001b[38;5;241m.\u001b[39mdeserialize)\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "File \u001b[0;32m~/GitHub/scopium/.venv/lib/python3.10/site-packages/arango/http.py:230\u001b[0m, in \u001b[0;36mDefaultHTTPClient.send_request\u001b[0;34m(self, session, method, url, headers, params, data, auth)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msend_request\u001b[39m(\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    203\u001b[0m     session: Session,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    209\u001b[0m     auth: Optional[Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    210\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Response:\n\u001b[1;32m    211\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Send an HTTP request.\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \n\u001b[1;32m    213\u001b[0m \u001b[38;5;124;03m    :param session: Requests session object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03m    :rtype: arango.response.Response\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 230\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    240\u001b[0m         method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m    241\u001b[0m         url\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    245\u001b[0m         raw_body\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mtext,\n\u001b[1;32m    246\u001b[0m     )\n",
      "File \u001b[0;32m~/GitHub/scopium/.venv/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/GitHub/scopium/.venv/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/GitHub/scopium/.venv/lib/python3.10/site-packages/requests/adapters.py:713\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, ReadTimeoutError):\n\u001b[0;32m--> 713\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeout(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    714\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, _InvalidHeader):\n\u001b[1;32m    715\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidHeader(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[0;31mReadTimeout\u001b[0m: HTTPSConnectionPool(host='d2eeb8083350.arangodb.cloud', port=8529): Read timed out. (read timeout=60)"
     ]
    }
   ],
   "source": [
    "\n",
    "G_adb = nxadb.Graph(\n",
    "    name=\"FlaskRepv1\",\n",
    "    db=db,\n",
    "    incoming_graph_data=G,\n",
    "    write_batch_size=50000, # feel free to modify\n",
    "    overwrite_graph=True\n",
    ")\n",
    "\n",
    "print(G_adb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run from loaded database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T06:36:49.703703Z",
     "iopub.status.busy": "2025-03-04T06:36:49.703340Z",
     "iopub.status.idle": "2025-03-04T06:36:49.748066Z",
     "shell.execute_reply": "2025-03-04T06:36:49.746918Z",
     "shell.execute_reply.started": "2025-03-04T06:36:49.703672Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<StandardDatabase _system>\n"
     ]
    }
   ],
   "source": [
    "db = ArangoClient(hosts=\"https://d2eeb8083350.arangodb.cloud:8529\").db(username=\"root\", password=\"cUZ0YaNdcwfUTw6VjRny\", verify=True)\n",
    "\n",
    "print(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T06:37:00.648492Z",
     "iopub.status.busy": "2025-03-04T06:37:00.648116Z",
     "iopub.status.idle": "2025-03-04T06:37:00.795106Z",
     "shell.execute_reply": "2025-03-04T06:37:00.794025Z",
     "shell.execute_reply.started": "2025-03-04T06:37:00.648462Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20:27:38 +0530] [INFO]: Graph 'FlaskRepv1' exists.\n",
      "[20:27:39 +0530] [INFO]: Default node type set to 'FlaskRepv1_node'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nx_arangodb.classes.graph.Graph'>\n"
     ]
    }
   ],
   "source": [
    "G_adb = nxadb.Graph(\n",
    "    name=\"FlaskRepv1\",\n",
    "    db=db,\n",
    "    #incoming_graph_data=G,\n",
    "    #write_batch_size=50000 # feel free to modify\n",
    ")\n",
    "\n",
    "print(type(G_adb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T06:37:03.596462Z",
     "iopub.status.busy": "2025-03-04T06:37:03.596069Z",
     "iopub.status.idle": "2025-03-04T06:37:03.725552Z",
     "shell.execute_reply": "2025-03-04T06:37:03.724584Z",
     "shell.execute_reply.started": "2025-03-04T06:37:03.596432Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "arango_graph = ArangoGraph(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Graph Schema': [{'graph_name': 'CodebaseGraph', 'edge_definitions': [{'edge_collection': 'CodebaseGraph_node_to_CodebaseGraph_node', 'from_vertex_collections': ['CodebaseGraph_node'], 'to_vertex_collections': ['CodebaseGraph_node']}]}, {'graph_name': 'FlaskRepv1_node_to_FlaskRespv1_node', 'edge_definitions': [{'edge_collection': 'FlaskRepv1_node_to_FlaskRespv1_node_node_to_FlaskRepv1_node_to_FlaskRespv1_node_node', 'from_vertex_collections': ['FlaskRepv1_node_to_FlaskRespv1_node_node'], 'to_vertex_collections': ['FlaskRepv1_node_to_FlaskRespv1_node_node']}]}, {'graph_name': 'FlaskRepv1_node', 'edge_definitions': [{'edge_collection': 'FlaskRepv1_node_node_to_FlaskRepv1_node_node', 'from_vertex_collections': ['FlaskRepv1_node_node'], 'to_vertex_collections': ['FlaskRepv1_node_node']}]}, {'graph_name': 'code_graph', 'edge_definitions': [{'edge_collection': 'code_edges', 'from_vertex_collections': ['code_nodes'], 'to_vertex_collections': ['code_nodes']}]}, {'graph_name': 'FlaskRespv1', 'edge_definitions': [{'edge_collection': 'FlaskRespv1_node_to_FlaskRespv1_node', 'from_vertex_collections': ['FlaskRespv1_node'], 'to_vertex_collections': ['FlaskRespv1_node']}]}, {'graph_name': 'FlaskRepv2', 'edge_definitions': [{'edge_collection': 'FlaskRepv2_node_to_FlaskRepv2_node', 'from_vertex_collections': ['FlaskRepv2_node'], 'to_vertex_collections': ['FlaskRepv2_node']}]}, {'graph_name': 'Flaskv3', 'edge_definitions': [{'edge_collection': 'Flaskv3_edges', 'from_vertex_collections': ['Flaskv3_nodes'], 'to_vertex_collections': ['Flaskv3_nodes']}]}, {'graph_name': 'Flaskv2', 'edge_definitions': [{'edge_collection': 'Flaskv2_node_to_Flaskv2_node', 'from_vertex_collections': ['Flaskv2_node'], 'to_vertex_collections': ['Flaskv2_node']}]}, {'graph_name': 'FlaskRepv1', 'edge_definitions': [{'edge_collection': 'FlaskRepv1_node_to_FlaskRepv1_node', 'from_vertex_collections': ['FlaskRepv1_node'], 'to_vertex_collections': ['FlaskRepv1_node']}]}], 'Collection Schema': [{'collection_name': 'Flaskv2_node', 'collection_type': 'document', 'document_properties': [{'name': '_key', 'type': 'str'}, {'name': '_id', 'type': 'str'}, {'name': '_rev', 'type': 'str'}, {'name': 'ast_type', 'type': 'str'}, {'name': 'file_path', 'type': 'str'}, {'name': 'rel_path', 'type': 'str'}, {'name': 'module_name', 'type': 'str'}, {'name': 'dir_depth', 'type': 'int'}, {'name': 'imported_by_count', 'type': 'int'}, {'name': 'name', 'type': 'str'}], 'example_document': {'_key': '0', '_id': 'Flaskv2_node/0', '_rev': '_jVNoMc2---', 'ast_type': 'File', 'file_path': 'flask/src/flask/testing.py', 'rel_path': '../src/flask/testing.py', 'module_name': '...src.flask.testing', 'dir_depth': 3, 'imported_by_count': 0, 'name': 'testing.py'}}, {'collection_name': 'FlaskRepv1_node', 'collection_type': 'document', 'document_properties': [{'name': '_key', 'type': 'str'}, {'name': '_id', 'type': 'str'}, {'name': '_rev', 'type': 'str'}, {'name': 'type', 'type': 'str'}], 'example_document': {'_key': '0', '_id': 'FlaskRepv1_node/0', '_rev': '_jVrCqTK---', 'type': 'directory'}}, {'collection_name': 'Flaskv3_nodes', 'collection_type': 'document', 'document_properties': [{'name': '_key', 'type': 'str'}, {'name': '_id', 'type': 'str'}, {'name': '_rev', 'type': 'str'}, {'name': 'type', 'type': 'str'}, {'name': 'path', 'type': 'str'}, {'name': 'docstring', 'type': 'str'}, {'name': 'code_snippet', 'type': 'str'}], 'example_document': {'_key': '0', '_id': 'Flaskv3_nodes/0', '_rev': '_jVQPDwa---', 'type': 'module', 'path': 'flask/src/flask/debughelpers.py', 'docstring': '', 'code_snippet': 'from __future__ import annotations\\n\\nimport typing as t\\n\\nfrom jinja2.loaders import BaseLoader\\nfrom werkzeug.routing import RequestRedirect\\n\\nfrom .blueprints import Blueprint\\nfrom .globals import request_ctx\\nfrom .sansio.app import App\\n\\nif t.TYPE_CHECKING:\\n    from .sansio.scaffold import Scaffold\\n    from .wrappers import Request\\n\\n\\nclass UnexpectedUnicodeError(AssertionError, UnicodeError):\\n    \"\"\"Raised in places where we want some better error reporting for\\n    unexpected unicode or binary dat...'}}, {'collection_name': 'Flaskv2_node_to_Flaskv2_node', 'collection_type': 'edge', 'edge_properties': [{'name': '_key', 'type': 'str'}, {'name': '_id', 'type': 'str'}, {'name': '_from', 'type': 'str'}, {'name': '_to', 'type': 'str'}, {'name': '_rev', 'type': 'str'}, {'name': 'relation', 'type': 'str'}], 'example_edge': {'_key': '0', '_id': 'Flaskv2_node_to_Flaskv2_node/0', '_from': 'Flaskv2_node/0', '_to': 'Flaskv2_node/1', '_rev': '_jVNoNBi--h', 'relation': 'contains'}}, {'collection_name': 'Flaskv3_edges', 'collection_type': 'edge', 'edge_properties': [{'name': '_key', 'type': 'str'}, {'name': '_id', 'type': 'str'}, {'name': '_from', 'type': 'str'}, {'name': '_to', 'type': 'str'}, {'name': '_rev', 'type': 'str'}, {'name': 'relationship', 'type': 'str'}, {'name': 'weight', 'type': 'int'}], 'example_edge': {'_key': '0', '_id': 'Flaskv3_edges/0', '_from': 'Flaskv3_nodes/0', '_to': 'Flaskv3_nodes/1', '_rev': '_jVQPEBi---', 'relationship': 'contains', 'weight': 1}}, {'collection_name': 'FlaskRepv1_node_to_FlaskRepv1_node', 'collection_type': 'edge', 'edge_properties': [{'name': '_key', 'type': 'str'}, {'name': '_id', 'type': 'str'}, {'name': '_from', 'type': 'str'}, {'name': '_to', 'type': 'str'}, {'name': '_rev', 'type': 'str'}, {'name': 'edge_type', 'type': 'str'}], 'example_edge': {'_key': '0', '_id': 'FlaskRepv1_node_to_FlaskRepv1_node/0', '_from': 'FlaskRepv1_node/0', '_to': 'FlaskRepv1_node/1', '_rev': '_jVrCq5K---', 'edge_type': 'contains_directory'}}]}\n"
     ]
    }
   ],
   "source": [
    "print( arango_graph.schema )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvis.network import Network\n",
    "\n",
    "def visualize_codebase_graph(codebase_path, output_html=\"codebase_graph.html\", limit_nodes=None):\n",
    "    \"\"\"\n",
    "    Visualize the codebase graph using pyvis.\n",
    "    \n",
    "    Args:\n",
    "        codebase_path: Path to the codebase directory\n",
    "        output_html: Output HTML file for the visualization/\n",
    "        limit_nodes: Optional limit on the number of nodes to display (for large codebases)\n",
    "    \"\"\"\n",
    "    # Initialize and build the graph\n",
    "    visualizer = CodebaseVisualizer(codebase_path)\n",
    "    visualizer.parse_files()\n",
    "    graph = visualizer.build_graph()\n",
    "    \n",
    "    # Export graph to JSON (optional)\n",
    "    visualizer.export_graph_json('codebase_graph.json')\n",
    "    \n",
    "    # Create a pyvis network\n",
    "    net = Network(height=\"900px\", width=\"100%\", bgcolor=\"#222222\", font_color=\"white\")\n",
    "    \n",
    "    # Configure physics\n",
    "    net.barnes_hut(gravity=-5000, central_gravity=0.3, spring_length=200)\n",
    "    \n",
    "    # Define node groups and colors\n",
    "    node_colors = {\n",
    "        'file': '#4287f5',\n",
    "        'directory': '#42f5a7',\n",
    "        'symbol': '#f542cb',\n",
    "        'snippet': '#f5a742'\n",
    "    }\n",
    "    \n",
    "    # If we need to limit nodes for performance\n",
    "    if limit_nodes and len(graph.nodes()) > limit_nodes:\n",
    "        # Focus on file and directory nodes, and limit symbol nodes\n",
    "        important_nodes = [node for node, data in graph.nodes(data=True) \n",
    "                          if data.get('type') in ['file', 'directory']]\n",
    "        \n",
    "        # Add some symbol nodes to reach the limit\n",
    "        symbols = [node for node, data in graph.nodes(data=True) \n",
    "                  if data.get('type') == 'symbol']\n",
    "        \n",
    "        # Take a subset of symbols based on connectivity\n",
    "        symbol_importance = sorted(\n",
    "            [(node, graph.degree(node)) for node in symbols],\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        important_symbols = [node for node, _ in symbol_importance[:limit_nodes - len(important_nodes)]]\n",
    "        selected_nodes = important_nodes + important_symbols\n",
    "        \n",
    "        # Create a subgraph\n",
    "        graph = graph.subgraph(selected_nodes)\n",
    "    \n",
    "    # Add nodes with appropriate styles\n",
    "    for node, node_data in graph.nodes(data=True):\n",
    "        node_type = node_data.get('type', 'unknown')\n",
    "        label = os.path.basename(node) if '/' in node else node\n",
    "        \n",
    "        # Truncate very long labels\n",
    "        if len(label) > 30:\n",
    "            label = label[:27] + \"...\"\n",
    "        \n",
    "        # Create hover title with more details\n",
    "        title = f\"<div style='max-width:300px;'>\"\n",
    "        title += f\"<b>{node}</b><br>\"\n",
    "        title += f\"Type: {node_type}<br>\"\n",
    "        \n",
    "        if node_type == 'file':\n",
    "            title += f\"Directory: {node_data.get('directory', 'N/A')}<br>\"\n",
    "            \n",
    "        elif node_type == 'symbol':\n",
    "            title += f\"Symbol type: {node_data.get('symbol_type', 'N/A')}<br>\"\n",
    "            title += f\"Line: {node_data.get('line_number', 'N/A')}<br>\"\n",
    "            \n",
    "            if node_data.get('docstring'):\n",
    "                docstring = node_data['docstring']\n",
    "                if len(docstring) > 200:\n",
    "                    docstring = docstring[:197] + \"...\"\n",
    "                title += f\"Docstring: {docstring}<br>\"\n",
    "                \n",
    "        elif node_type == 'snippet':\n",
    "            title += f\"Lines: {node_data.get('start_line', 'N/A')}-{node_data.get('end_line', 'N/A')}<br>\"\n",
    "            \n",
    "            if node_data.get('code_snippet'):\n",
    "                snippet = node_data['code_snippet'].replace('\\n', '<br>')\n",
    "                if len(snippet) > 300:\n",
    "                    snippet = snippet[:297] + \"...\"\n",
    "                title += f\"Code:<br><pre>{snippet}</pre>\"\n",
    "                \n",
    "        title += \"</div>\"\n",
    "        \n",
    "        # Determine node size based on type and connections\n",
    "        size = 15  # Default size\n",
    "        if node_type == 'directory':\n",
    "            size = 25\n",
    "        elif node_type == 'file':\n",
    "            size = 20\n",
    "        elif node_type == 'symbol' and node_data.get('symbol_type') == 'class':\n",
    "            size = 18\n",
    "        \n",
    "        # Add node with appropriate styling\n",
    "        net.add_node(\n",
    "            node, \n",
    "            label=label, \n",
    "            title=title,\n",
    "            color=node_colors.get(node_type, '#999999'),\n",
    "            size=size,\n",
    "            shape='dot' if node_type != 'directory' else 'diamond'\n",
    "        )\n",
    "    \n",
    "    # Add edges with appropriate styles\n",
    "    for source, target, edge_data in graph.edges(data=True):\n",
    "        edge_type = edge_data.get('edge_type', 'unknown')\n",
    "        \n",
    "        # Style edges differently based on type\n",
    "        if edge_type == 'import':\n",
    "            color = '#f5f542'\n",
    "            width = 2\n",
    "            dashes = False\n",
    "        elif edge_type == 'references':\n",
    "            color = '#f54242'\n",
    "            width = 1\n",
    "            dashes = [5, 5]\n",
    "        elif edge_type == 'defines':\n",
    "            color = '#42f55a'\n",
    "            width = 3\n",
    "            dashes = False\n",
    "        elif edge_type == 'contains_snippet':\n",
    "            color = '#42c8f5'\n",
    "            width = 1\n",
    "            dashes = [2, 2]\n",
    "        else:\n",
    "            color = '#999999'\n",
    "            width = 1\n",
    "            dashes = False\n",
    "        \n",
    "        # Create hover title with edge details\n",
    "        title = f\"<div><b>{edge_type}</b><br>\"\n",
    "        \n",
    "        if edge_data.get('line_number'):\n",
    "            title += f\"Line: {edge_data['line_number']}<br>\"\n",
    "            \n",
    "        if edge_data.get('context'):\n",
    "            context = edge_data['context']\n",
    "            if len(context) > 200:\n",
    "                context = context[:197] + \"...\"\n",
    "            title += f\"Context: {context}\"\n",
    "            \n",
    "        title += \"</div>\"\n",
    "        \n",
    "        # Add edge with styling\n",
    "        net.add_edge(\n",
    "            source, \n",
    "            target, \n",
    "            title=title,\n",
    "            color=color,\n",
    "            width=width,\n",
    "            dashes=dashes\n",
    "        )\n",
    "    \n",
    "    # Enable physics, navigation and interaction options\n",
    "    net.toggle_physics(True)\n",
    "    net.show_buttons(filter_=['physics'])\n",
    "    \n",
    "    # Save the visualization\n",
    "    net.save_graph(output_html)\n",
    "    print(f\"Graph visualization saved to {output_html}\")\n",
    "    \n",
    "    return output_html\n",
    "\n",
    "def visualize_symbol_subgraph(visualizer, symbol_name, output_html=\"symbol_graph.html\"):\n",
    "    \"\"\"\n",
    "    Create a focused visualization of a symbol and its relationships.\n",
    "    \n",
    "    Args:\n",
    "        visualizer: Initialized CodebaseVisualizer instance\n",
    "        symbol_name: Name of the symbol to visualize\n",
    "        output_html: Output HTML file for the visualization\n",
    "    \"\"\"\n",
    "    # Get the full graph\n",
    "    full_graph = visualizer.graph\n",
    "    \n",
    "    # Find all nodes related to this symbol\n",
    "    symbol_nodes = [node for node in full_graph.nodes() if f\"::{symbol_name}\" in node]\n",
    "    \n",
    "    if not symbol_nodes:\n",
    "        print(f\"Symbol '{symbol_name}' not found in the graph.\")\n",
    "        return None\n",
    "    \n",
    "    # Get nodes that are connected to symbol nodes (1-hop neighborhood)\n",
    "    related_nodes = set(symbol_nodes)\n",
    "    for node in symbol_nodes:\n",
    "        # Add predecessors (nodes that reference this symbol)\n",
    "        related_nodes.update(full_graph.predecessors(node))\n",
    "        # Add successors (nodes that this symbol references)\n",
    "        related_nodes.update(full_graph.successors(node))\n",
    "    \n",
    "    # Create a subgraph\n",
    "    subgraph = full_graph.subgraph(related_nodes)\n",
    "    \n",
    "    # Create a pyvis network\n",
    "    net = Network(height=\"750px\", width=\"100%\", bgcolor=\"#222222\", font_color=\"white\")\n",
    "    net.barnes_hut(gravity=-2000, central_gravity=0.3, spring_length=150)\n",
    "    \n",
    "    # Node colors by type\n",
    "    node_colors = {\n",
    "        'file': '#4287f5',\n",
    "        'directory': '#42f5a7',\n",
    "        'symbol': '#f542cb',\n",
    "        'snippet': '#f5a742'\n",
    "    }\n",
    "    \n",
    "    # Add nodes with appropriate styles\n",
    "    for node, node_data in subgraph.nodes(data=True):\n",
    "        node_type = node_data.get('type', 'unknown')\n",
    "        label = os.path.basename(node) if '/' in node else node\n",
    "        \n",
    "        # Make the focus symbol nodes larger and highlighted\n",
    "        if node in symbol_nodes:\n",
    "            size = 30\n",
    "            color = '#ff0000'  # Bright red for focus\n",
    "        else:\n",
    "            size = 15\n",
    "            color = node_colors.get(node_type, '#999999')\n",
    "        \n",
    "        # Add hover information\n",
    "        title = f\"<div style='max-width:300px;'>\"\n",
    "        title += f\"<b>{node}</b><br>\"\n",
    "        title += f\"Type: {node_type}<br>\"\n",
    "        \n",
    "        if node_type == 'symbol':\n",
    "            title += f\"Symbol type: {node_data.get('symbol_type', 'N/A')}<br>\"\n",
    "            if node_data.get('docstring'):\n",
    "                title += f\"Docstring: {node_data.get('docstring', '')}<br>\"\n",
    "                \n",
    "        title += \"</div>\"\n",
    "        \n",
    "        # Add node with styling\n",
    "        net.add_node(\n",
    "            node,\n",
    "            label=label,\n",
    "            title=title,\n",
    "            color=color,\n",
    "            size=size\n",
    "        )\n",
    "    \n",
    "    # Add edges with styles\n",
    "    for source, target, edge_data in subgraph.edges(data=True):\n",
    "        edge_type = edge_data.get('edge_type', 'unknown')\n",
    "        \n",
    "        # Style edges by type\n",
    "        if edge_type == 'import':\n",
    "            color = '#f5f542'  # Yellow\n",
    "        elif edge_type == 'references':\n",
    "            color = '#f54242'  # Red\n",
    "        elif edge_type == 'defines':\n",
    "            color = '#42f55a'  # Green\n",
    "        else:\n",
    "            color = '#999999'  # Gray\n",
    "        \n",
    "        # Add edge with details in hover\n",
    "        title = f\"{edge_type}\"\n",
    "        if edge_data.get('line_number'):\n",
    "            title += f\" (line {edge_data['line_number']})\"\n",
    "            \n",
    "        net.add_edge(source, target, title=title, color=color)\n",
    "    \n",
    "    # Enable physics and navigation\n",
    "    net.toggle_physics(True)\n",
    "    net.show_buttons(filter_=['physics'])\n",
    "    \n",
    "    # Save the visualization\n",
    "    net.save_graph(output_html)\n",
    "    print(f\"Symbol graph visualization saved to {output_html}\")\n",
    "    \n",
    "    return output_html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvis.network import Network\n",
    "\n",
    "def visualize_codebase_graph(codebase_path, output_html=\"codebase_graph.html\", limit_nodes=None):\n",
    "    \"\"\"\n",
    "    Visualize the codebase graph using pyvis.\n",
    "    \n",
    "    Args:\n",
    "        codebase_path: Path to the codebase directory\n",
    "        output_html: Output HTML file for the visualization/\n",
    "        limit_nodes: Optional limit on the number of nodes to display (for large codebases)\n",
    "    \"\"\"\n",
    "    # Initialize and build the graph\n",
    "    visualizer = CodebaseVisualizer(codebase_path)\n",
    "    visualizer.parse_files()\n",
    "    graph = visualizer.build_graph()\n",
    "    \n",
    "    # Export graph to JSON (optional)\n",
    "    visualizer.export_graph_json('codebase_graph.json')\n",
    "    \n",
    "    # Create a pyvis network\n",
    "    net = Network(height=\"900px\", width=\"100%\", bgcolor=\"#222222\", font_color=\"white\")\n",
    "    \n",
    "    # Configure physics\n",
    "    net.barnes_hut(gravity=-5000, central_gravity=0.3, spring_length=200)\n",
    "    \n",
    "    # Define node groups and colors\n",
    "    node_colors = {\n",
    "        'file': '#4287f5',\n",
    "        'directory': '#42f5a7',\n",
    "        'symbol': '#f542cb',\n",
    "        'snippet': '#f5a742'\n",
    "    }\n",
    "    \n",
    "    # If we need to limit nodes for performance\n",
    "    if limit_nodes and len(graph.nodes()) > limit_nodes:\n",
    "        # Focus on file and directory nodes, and limit symbol nodes\n",
    "        important_nodes = [node for node, data in graph.nodes(data=True) \n",
    "                          if data.get('type') in ['file', 'directory']]\n",
    "        \n",
    "        # Add some symbol nodes to reach the limit\n",
    "        symbols = [node for node, data in graph.nodes(data=True) \n",
    "                  if data.get('type') == 'symbol']\n",
    "        \n",
    "        # Take a subset of symbols based on connectivity\n",
    "        symbol_importance = sorted(\n",
    "            [(node, graph.degree(node)) for node in symbols],\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        important_symbols = [node for node, _ in symbol_importance[:limit_nodes - len(important_nodes)]]\n",
    "        selected_nodes = important_nodes + important_symbols\n",
    "        \n",
    "        # Create a subgraph\n",
    "        graph = graph.subgraph(selected_nodes)\n",
    "    \n",
    "    # Add nodes with appropriate styles\n",
    "    for node, node_data in graph.nodes(data=True):\n",
    "        node_type = node_data.get('type', 'unknown')\n",
    "        label = os.path.basename(node) if '/' in node else node\n",
    "        \n",
    "        # Truncate very long labels\n",
    "        if len(label) > 30:\n",
    "            label = label[:27] + \"...\"\n",
    "        \n",
    "        # Create hover title with more details\n",
    "        title = f\"<div style='max-width:300px;'>\"\n",
    "        title += f\"<b>{node}</b><br>\"\n",
    "        title += f\"Type: {node_type}<br>\"\n",
    "        \n",
    "        if node_type == 'file':\n",
    "            title += f\"Directory: {node_data.get('directory', 'N/A')}<br>\"\n",
    "            \n",
    "        elif node_type == 'symbol':\n",
    "            title += f\"Symbol type: {node_data.get('symbol_type', 'N/A')}<br>\"\n",
    "            title += f\"Line: {node_data.get('line_number', 'N/A')}<br>\"\n",
    "            \n",
    "            if node_data.get('docstring'):\n",
    "                docstring = node_data['docstring']\n",
    "                if len(docstring) > 200:\n",
    "                    docstring = docstring[:197] + \"...\"\n",
    "                title += f\"Docstring: {docstring}<br>\"\n",
    "                \n",
    "        elif node_type == 'snippet':\n",
    "            title += f\"Lines: {node_data.get('start_line', 'N/A')}-{node_data.get('end_line', 'N/A')}<br>\"\n",
    "            \n",
    "            if node_data.get('code_snippet'):\n",
    "                snippet = node_data['code_snippet'].replace('\\n', '<br>')\n",
    "                if len(snippet) > 300:\n",
    "                    snippet = snippet[:297] + \"...\"\n",
    "                title += f\"Code:<br><pre>{snippet}</pre>\"\n",
    "                \n",
    "        title += \"</div>\"\n",
    "        \n",
    "        # Determine node size based on type and connections\n",
    "        size = 15  # Default size\n",
    "        if node_type == 'directory':\n",
    "            size = 25\n",
    "        elif node_type == 'file':\n",
    "            size = 20\n",
    "        elif node_type == 'symbol' and node_data.get('symbol_type') == 'class':\n",
    "            size = 18\n",
    "        \n",
    "        # Add node with appropriate styling\n",
    "        net.add_node(\n",
    "            node, \n",
    "            label=label, \n",
    "            title=title,\n",
    "            color=node_colors.get(node_type, '#999999'),\n",
    "            size=size,\n",
    "            shape='dot' if node_type != 'directory' else 'diamond'\n",
    "        )\n",
    "    \n",
    "    # Add edges with appropriate styles\n",
    "    for source, target, edge_data in graph.edges(data=True):\n",
    "        edge_type = edge_data.get('edge_type', 'unknown')\n",
    "        \n",
    "        # Style edges differently based on type\n",
    "        if edge_type == 'import':\n",
    "            color = '#f5f542'\n",
    "            width = 2\n",
    "            dashes = False\n",
    "        elif edge_type == 'references':\n",
    "            color = '#f54242'\n",
    "            width = 1\n",
    "            dashes = [5, 5]\n",
    "        elif edge_type == 'defines':\n",
    "            color = '#42f55a'\n",
    "            width = 3\n",
    "            dashes = False\n",
    "        elif edge_type == 'contains_snippet':\n",
    "            color = '#42c8f5'\n",
    "            width = 1\n",
    "            dashes = [2, 2]\n",
    "        else:\n",
    "            color = '#999999'\n",
    "            width = 1\n",
    "            dashes = False\n",
    "        \n",
    "        # Create hover title with edge details\n",
    "        title = f\"<div><b>{edge_type}</b><br>\"\n",
    "        \n",
    "        if edge_data.get('line_number'):\n",
    "            title += f\"Line: {edge_data['line_number']}<br>\"\n",
    "            \n",
    "        if edge_data.get('context'):\n",
    "            context = edge_data['context']\n",
    "            if len(context) > 200:\n",
    "                context = context[:197] + \"...\"\n",
    "            title += f\"Context: {context}\"\n",
    "            \n",
    "        title += \"</div>\"\n",
    "        \n",
    "        # Add edge with styling\n",
    "        net.add_edge(\n",
    "            source, \n",
    "            target, \n",
    "            title=title,\n",
    "            color=color,\n",
    "            width=width,\n",
    "            dashes=dashes\n",
    "        )\n",
    "    \n",
    "    # Enable physics, navigation and interaction options\n",
    "    net.toggle_physics(True)\n",
    "    net.show_buttons(filter_=['physics'])\n",
    "    \n",
    "    # Save the visualization\n",
    "    net.save_graph(output_html)\n",
    "    print(f\"Graph visualization saved to {output_html}\")\n",
    "    \n",
    "    return output_html\n",
    "\n",
    "def visualize_symbol_subgraph(visualizer, symbol_name, output_html=\"symbol_graph.html\"):\n",
    "    \"\"\"\n",
    "    Create a focused visualization of a symbol and its relationships.\n",
    "    \n",
    "    Args:\n",
    "        visualizer: Initialized CodebaseVisualizer instance\n",
    "        symbol_name: Name of the symbol to visualize\n",
    "        output_html: Output HTML file for the visualization\n",
    "    \"\"\"\n",
    "    # Get the full graph\n",
    "    full_graph = visualizer.graph\n",
    "    \n",
    "    # Find all nodes related to this symbol\n",
    "    symbol_nodes = [node for node in full_graph.nodes() if f\"::{symbol_name}\" in node]\n",
    "    \n",
    "    if not symbol_nodes:\n",
    "        print(f\"Symbol '{symbol_name}' not found in the graph.\")\n",
    "        return None\n",
    "    \n",
    "    # Get nodes that are connected to symbol nodes (1-hop neighborhood)\n",
    "    related_nodes = set(symbol_nodes)\n",
    "    for node in symbol_nodes:\n",
    "        # Add predecessors (nodes that reference this symbol)\n",
    "        related_nodes.update(full_graph.predecessors(node))\n",
    "        # Add successors (nodes that this symbol references)\n",
    "        related_nodes.update(full_graph.successors(node))\n",
    "    \n",
    "    # Create a subgraph\n",
    "    subgraph = full_graph.subgraph(related_nodes)\n",
    "    \n",
    "    # Create a pyvis network\n",
    "    net = Network(height=\"750px\", width=\"100%\", bgcolor=\"#222222\", font_color=\"white\")\n",
    "    net.barnes_hut(gravity=-2000, central_gravity=0.3, spring_length=150)\n",
    "    \n",
    "    # Node colors by type\n",
    "    node_colors = {\n",
    "        'file': '#4287f5',\n",
    "        'directory': '#42f5a7',\n",
    "        'symbol': '#f542cb',\n",
    "        'snippet': '#f5a742'\n",
    "    }\n",
    "    \n",
    "    # Add nodes with appropriate styles\n",
    "    for node, node_data in subgraph.nodes(data=True):\n",
    "        node_type = node_data.get('type', 'unknown')\n",
    "        label = os.path.basename(node) if '/' in node else node\n",
    "        \n",
    "        # Make the focus symbol nodes larger and highlighted\n",
    "        if node in symbol_nodes:\n",
    "            size = 30\n",
    "            color = '#ff0000'  # Bright red for focus\n",
    "        else:\n",
    "            size = 15\n",
    "            color = node_colors.get(node_type, '#999999')\n",
    "        \n",
    "        # Add hover information\n",
    "        title = f\"<div style='max-width:300px;'>\"\n",
    "        title += f\"<b>{node}</b><br>\"\n",
    "        title += f\"Type: {node_type}<br>\"\n",
    "        \n",
    "        if node_type == 'symbol':\n",
    "            title += f\"Symbol type: {node_data.get('symbol_type', 'N/A')}<br>\"\n",
    "            if node_data.get('docstring'):\n",
    "                title += f\"Docstring: {node_data.get('docstring', '')}<br>\"\n",
    "                \n",
    "        title += \"</div>\"\n",
    "        \n",
    "        # Add node with styling\n",
    "        net.add_node(\n",
    "            node,\n",
    "            label=label,\n",
    "            title=title,\n",
    "            color=color,\n",
    "            size=size\n",
    "        )\n",
    "    \n",
    "    # Add edges with styles\n",
    "    for source, target, edge_data in subgraph.edges(data=True):\n",
    "        edge_type = edge_data.get('edge_type', 'unknown')\n",
    "        \n",
    "        # Style edges by type\n",
    "        if edge_type == 'import':\n",
    "            color = '#f5f542'  # Yellow\n",
    "        elif edge_type == 'references':\n",
    "            color = '#f54242'  # Red\n",
    "        elif edge_type == 'defines':\n",
    "            color = '#42f55a'  # Green\n",
    "        else:\n",
    "            color = '#999999'  # Gray\n",
    "        \n",
    "        # Add edge with details in hover\n",
    "        title = f\"{edge_type}\"\n",
    "        if edge_data.get('line_number'):\n",
    "            title += f\" (line {edge_data['line_number']})\"\n",
    "            \n",
    "        net.add_edge(source, target, title=title, color=color)\n",
    "    \n",
    "    # Enable physics and navigation\n",
    "    net.toggle_physics(True)\n",
    "    net.show_buttons(filter_=['physics'])\n",
    "    \n",
    "    # Save the visualization\n",
    "    net.save_graph(output_html)\n",
    "    print(f\"Symbol graph visualization saved to {output_html}\")\n",
    "    \n",
    "    return output_html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T06:37:04.449981Z",
     "iopub.status.busy": "2025-03-04T06:37:04.449636Z",
     "iopub.status.idle": "2025-03-04T06:37:04.454585Z",
     "shell.execute_reply": "2025-03-04T06:37:04.453434Z",
     "shell.execute_reply.started": "2025-03-04T06:37:04.449949Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"MISTRAL_API_KEY\"]=\"jJAuJZkjVcy2ynUhan375sHNviHiBeJU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T06:37:06.357854Z",
     "iopub.status.busy": "2025-03-04T06:37:06.357510Z",
     "iopub.status.idle": "2025-03-04T06:37:07.616190Z",
     "shell.execute_reply": "2025-03-04T06:37:07.615006Z",
     "shell.execute_reply.started": "2025-03-04T06:37:06.357826Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={}, response_metadata={'token_usage': {'prompt_tokens': 4, 'total_tokens': 13, 'completion_tokens': 9}, 'model': 'mistral-large-latest', 'finish_reason': 'stop'}, id='run-887dd047-0991-4203-8f39-9aeae0814f00-0', usage_metadata={'input_tokens': 4, 'output_tokens': 9, 'total_tokens': 13})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_mistralai import ChatMistralAI\n",
    "\n",
    "llm = ChatMistralAI(\n",
    "    model=\"mistral-large-latest\",\n",
    "    temperature=0,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "llm.invoke(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T06:37:09.976501Z",
     "iopub.status.busy": "2025-03-04T06:37:09.976151Z",
     "iopub.status.idle": "2025-03-04T06:37:09.981764Z",
     "shell.execute_reply": "2025-03-04T06:37:09.980571Z",
     "shell.execute_reply.started": "2025-03-04T06:37:09.976473Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# @tool\n",
    "# def text_to_nx_algorithm_to_text(query):\n",
    "#     \"\"\"This tool is available to invoke a NetworkX Algorithm on\n",
    "#     the ArangoDB Graph. You are responsible for accepting the\n",
    "#     Natural Language Query, establishing which algorithm needs to\n",
    "#     be executed, executing the algorithm, and translating the results back\n",
    "#     to Natural Language, with respect to the original query.\n",
    "\n",
    "#     If the query (e.g traversals, shortest path, etc.) can be solved using the Arango Query Language, then do not use\n",
    "#     this tool.\n",
    "#     \"\"\"\n",
    "#     llm = ChatMistralAI(\n",
    "#         model=\"mistral-large-latest\",\n",
    "#         temperature=0,\n",
    "#         max_retries=2,\n",
    "#         # other params...\n",
    "#     )\n",
    "#     ######################\n",
    "#     print(\"1) Generating NetworkX code\")\n",
    "\n",
    "#     text_to_nx = llm.invoke(f\"\"\"\n",
    "#     I have a NetworkX Graph called `G_adb`. It has the following schema: {arango_graph.schema}\n",
    "\n",
    "#     I have the following graph analysis query: {query}.\n",
    "\n",
    "#     Generate the Python Code required to answer the query using the `G_adb` object.\n",
    "    \n",
    "#     It should give code so that is gives ALL the necessary contents that use this part of the code in terms of ALL nested imports. Consider the nested directories and sub-directory informations as well.\n",
    "\n",
    "#     Be very precise on the NetworkX algorithm you select to answer this query. Think step by step.\n",
    "\n",
    "#     Only assume that networkx is installed, and other base python dependencies.\n",
    "\n",
    "#     Always set the last variable as `FINAL_RESULT`, which represents the answer to the original query.\n",
    "\n",
    "#     Only provide python code that I can directly execute via `exec()`. Do not provide any instructions.\n",
    "\n",
    "#     Make sure that `FINAL_RESULT` stores a short & consice answer. Avoid setting this variable to a long sequence.\n",
    "\n",
    "#     Your code:\n",
    "#     \"\"\").content\n",
    "\n",
    "#     text_to_nx_cleaned = re.sub(r\"^```python\\n|```$\", \"\", text_to_nx, flags=re.MULTILINE).strip()\n",
    "    \n",
    "#     print('-'*10)\n",
    "#     print(text_to_nx_cleaned)\n",
    "#     print('-'*10)\n",
    "\n",
    "#     ######################\n",
    "\n",
    "#     print(\"\\n2) Executing NetworkX code\")\n",
    "#     global_vars = {\"G_adb\": G_adb, \"nx\": nx}\n",
    "#     local_vars = {}\n",
    "\n",
    "#     try:\n",
    "#         exec(text_to_nx_cleaned, global_vars, local_vars)\n",
    "#         text_to_nx_final = text_to_nx\n",
    "#     except Exception as e:\n",
    "#         print(f\"EXEC ERROR: {e}\")\n",
    "#         return f\"EXEC ERROR: {e}\"\n",
    "\n",
    "#         # TODO: Consider experimenting with a code corrector!\n",
    "#         attempt = 1\n",
    "#         MAX_ATTEMPTS = 3\n",
    "\n",
    "#         # while attempt <= MAX_ATTEMPTS\n",
    "#             # ...\n",
    "\n",
    "#     print('-'*10)\n",
    "#     FINAL_RESULT = local_vars[\"FINAL_RESULT\"]\n",
    "#     print(f\"FINAL_RESULT: {FINAL_RESULT}\")\n",
    "#     print('-'*10)\n",
    "\n",
    "#     ######################\n",
    "\n",
    "#     print(\"3) Formulating final answer\")\n",
    "\n",
    "#     nx_to_text = llm.invoke(f\"\"\"\n",
    "#         I have a NetworkX Graph called `G_adb`. It has the following schema: {arango_graph.schema}\n",
    "\n",
    "#         I have the following graph analysis query: {query}.\n",
    "\n",
    "#         I have executed the following python code to help me answer my query:\n",
    "\n",
    "#         ---\n",
    "#         {text_to_nx_final}\n",
    "#         ---\n",
    "\n",
    "#         The `FINAL_RESULT` variable is set to the following: {FINAL_RESULT}.\n",
    "\n",
    "#         Based on my original Query and FINAL_RESULT, generate a short and concise response to\n",
    "#         answer my query.\n",
    "        \n",
    "#         Your response:\n",
    "#     \"\"\").content\n",
    "\n",
    "#     return nx_to_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing the imported Graph via the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T06:36:13.720607Z",
     "iopub.status.busy": "2025-03-04T06:36:13.720223Z",
     "iopub.status.idle": "2025-03-04T06:36:13.742476Z",
     "shell.execute_reply": "2025-03-04T06:36:13.741250Z",
     "shell.execute_reply.started": "2025-03-04T06:36:13.720574Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from arango import ArangoClient\n",
    "import re\n",
    "\n",
    "def analyze_networkx_graph(G, query_text):\n",
    "    \"\"\"\n",
    "    Process a natural language query directly against a NetworkX graph\n",
    "    \n",
    "    Args:\n",
    "        G: NetworkX graph object\n",
    "        query_text: Natural language query about the codebase\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with analysis results in a human-readable format\n",
    "    \"\"\"\n",
    "    # Clean the query to extract the actual symbol being sought\n",
    "    clean_query = query_text.lower().strip()\n",
    "    \n",
    "    # Common English words to filter out\n",
    "    common_words = {\"what\", \"is\", \"the\", \"use\", \"of\", \"in\", \"where\", \"how\", \"why\", \"when\", \n",
    "                    \"who\", \"which\", \"does\", \"do\", \"are\", \"can\", \"could\", \"would\", \"should\", \n",
    "                    \"function\", \"method\", \"class\", \"variable\", \"codebase\", \"code\", \"used\", \n",
    "                    \"defined\", \"implemented\", \"called\", \"referenced\"}\n",
    "    \n",
    "    # Find all words that could be symbols\n",
    "    potential_symbols = []\n",
    "    for word in clean_query.split():\n",
    "        word = word.strip(\".,?!()[]{}'\\\"\\n\\t\")\n",
    "        if len(word) > 2 and word.lower() not in common_words:\n",
    "            potential_symbols.append(word)\n",
    "    \n",
    "    # Also look for multi-word symbols with underscores\n",
    "    for i in range(len(clean_query.split()) - 1):\n",
    "        compound = '_'.join(clean_query.split()[i:i+2])\n",
    "        if '_' in compound and compound not in potential_symbols:\n",
    "            potential_symbols.append(compound)\n",
    "    \n",
    "    # Extract exact symbol if passed directly\n",
    "    if query_text.strip() and len(query_text.strip().split()) == 1 and '_' in query_text:\n",
    "        # User likely just passed the symbol name directly\n",
    "        potential_symbols = [query_text.strip()]\n",
    "    \n",
    "    # Find matches in the graph\n",
    "    candidates = []\n",
    "    \n",
    "    # First, look for exact node IDs or node names\n",
    "    for symbol in potential_symbols:\n",
    "        for node_id, node_data in G.nodes(data=True):\n",
    "            # Check if this is a symbol node\n",
    "            if node_data.get('type') == 'symbol':\n",
    "                # Check the node ID\n",
    "                if isinstance(node_id, str) and symbol.lower() in node_id.lower():\n",
    "                    candidates.append((node_id, symbol, 1.0))  # 1.0 = high confidence\n",
    "                \n",
    "                # Check if symbol appears in the context (code snippet)\n",
    "                context = node_data.get('context', '')\n",
    "                if context and symbol.lower() in context.lower():\n",
    "                    # Higher confidence if it appears as a variable assignment\n",
    "                    patterns = [\n",
    "                        f\"self.{symbol}\", \n",
    "                        f\"{symbol} =\", \n",
    "                        f\"def {symbol}\", \n",
    "                        f\"class {symbol}\"\n",
    "                    ]\n",
    "                    score = 0.8  # Base score\n",
    "                    for pattern in patterns:\n",
    "                        if pattern.lower() in context.lower():\n",
    "                            score = 0.9  # Higher confidence\n",
    "                            break\n",
    "                    candidates.append((node_id, symbol, score))\n",
    "    \n",
    "    # Second, check for symbol references in edge contexts\n",
    "    if not candidates:\n",
    "        for source, target, edge_data in G.edges(data=True):\n",
    "            edge_type = edge_data.get('edge_type')\n",
    "            if edge_type in ['references', 'defines']:\n",
    "                context = edge_data.get('context', '')\n",
    "                for symbol in potential_symbols:\n",
    "                    if context and symbol.lower() in context.lower():\n",
    "                        patterns = [\n",
    "                            f\"self.{symbol}\", \n",
    "                            f\"{symbol} =\", \n",
    "                            f\"def {symbol}\", \n",
    "                            f\"class {symbol}\"\n",
    "                        ]\n",
    "                        score = 0.7  # Base score for edges\n",
    "                        for pattern in patterns:\n",
    "                            if pattern.lower() in context.lower():\n",
    "                                score = 0.8  # Higher confidence\n",
    "                                break\n",
    "                        candidates.append((target, symbol, score))\n",
    "    \n",
    "    # Third, search in snippets\n",
    "    if not candidates:\n",
    "        for node_id, node_data in G.nodes(data=True):\n",
    "            if node_data.get('type') == 'snippet':\n",
    "                code_snippet = node_data.get('code_snippet', '')\n",
    "                for symbol in potential_symbols:\n",
    "                    if code_snippet and symbol.lower() in code_snippet.lower():\n",
    "                        patterns = [\n",
    "                            f\"self.{symbol}\", \n",
    "                            f\"{symbol} =\", \n",
    "                            f\"def {symbol}\", \n",
    "                            f\"class {symbol}\"\n",
    "                        ]\n",
    "                        score = 0.6  # Base score for snippets\n",
    "                        for pattern in patterns:\n",
    "                            if pattern.lower() in code_snippet.lower():\n",
    "                                score = 0.7  # Higher confidence\n",
    "                                break\n",
    "                        # Find the file this snippet belongs to\n",
    "                        file_nodes = []\n",
    "                        for source, target, edge_data in G.in_edges(node_id, data=True):\n",
    "                            if edge_data.get('edge_type') == 'contains_snippet':\n",
    "                                file_nodes.append(source)\n",
    "                        \n",
    "                        # Create a pseudo symbol node ID using the file\n",
    "                        if file_nodes:\n",
    "                            pseudo_id = f\"{file_nodes[0]}::{symbol}\"\n",
    "                            candidates.append((pseudo_id, symbol, score))\n",
    "                        else:\n",
    "                            candidates.append((node_id, symbol, score))\n",
    "    \n",
    "    # No matches found\n",
    "    if not candidates:\n",
    "        return {\"response\": f\"Could not find a matching symbol in the codebase for '{query_text}'. Please try rephrasing your query with a specific function, class, or variable name.\"}\n",
    "    \n",
    "    # Sort candidates by confidence score\n",
    "    candidates.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    # Choose the best candidate\n",
    "    node_id, symbol_name, _ = candidates[0]\n",
    "    \n",
    "    # Find all usages of the identified symbol\n",
    "    symbol_usages = find_symbol_usage_nx(G, node_id, symbol_name)\n",
    "    \n",
    "    # Generate response\n",
    "    if \"error\" in symbol_usages:\n",
    "        # Try a broader search if the specific node wasn't found\n",
    "        broader_usages = search_symbol_in_contexts(G, symbol_name)\n",
    "        if broader_usages:\n",
    "            return {\"response\": broader_usages}\n",
    "        return {\"response\": symbol_usages[\"error\"]}\n",
    "    \n",
    "    # Collect information about the symbol\n",
    "    definition_files = symbol_usages.get(\"defined_in\", [])\n",
    "    usage_info = symbol_usages.get(\"usages\", {})\n",
    "    symbol_type = symbol_usages.get(\"symbol_type\", \"variable\")  # Default to variable\n",
    "    \n",
    "    # Build human-readable response\n",
    "    response = f\"Symbol: '{symbol_name}' (Type: {symbol_type})\\n\\n\"\n",
    "    response += f\"Defined in: {', '.join(definition_files) if definition_files else 'No definition location found'}\\n\\n\"\n",
    "    \n",
    "    if symbol_usages.get(\"docstring\"):\n",
    "        response += f\"Documentation:\\n{symbol_usages['docstring']}\\n\\n\"\n",
    "    \n",
    "    response += \"Used in the following locations:\\n\"\n",
    "    \n",
    "    if not usage_info:\n",
    "        response += \"\\nNo usage information found for this symbol in graph nodes.\\n\"\n",
    "        \n",
    "        # Try a broader search in contexts\n",
    "        broader_usages = search_symbol_in_contexts(G, symbol_name)\n",
    "        if broader_usages:\n",
    "            response += \"\\nHowever, found these mentions in code snippets:\\n\\n\"\n",
    "            response += broader_usages\n",
    "    else:\n",
    "        for file, usages in usage_info.items():\n",
    "            response += f\"\\nFile: {file}\\n\"\n",
    "            for usage in usages:\n",
    "                line = usage.get('line', 'unknown line')\n",
    "                context = usage.get('context', 'No context available')\n",
    "                response += f\"- Line {line}: {context}\\n\"\n",
    "    \n",
    "    # Add related symbols if available\n",
    "    if symbol_usages.get(\"related_symbols\"):\n",
    "        response += \"\\nRelated symbols:\\n\"\n",
    "        for related in symbol_usages[\"related_symbols\"][:5]:  # Limit to top 5\n",
    "            response += f\"- {related}\\n\"\n",
    "    \n",
    "    # Add graph statistics\n",
    "    response += f\"\\nAnalysis performed on graph with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\"\n",
    "    \n",
    "    return {\"response\": response}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching for the symbol in both context, networkx format and file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T11:56:17.150825Z",
     "iopub.status.busy": "2025-03-01T11:56:17.150480Z",
     "iopub.status.idle": "2025-03-01T11:56:17.164834Z",
     "shell.execute_reply": "2025-03-01T11:56:17.163909Z",
     "shell.execute_reply.started": "2025-03-01T11:56:17.150798Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def search_symbol_in_contexts(G, symbol_name):\n",
    "    \"\"\"\n",
    "    Search for a symbol in all contexts (code snippets) in the graph\n",
    "    \n",
    "    Args:\n",
    "        G: NetworkX graph\n",
    "        symbol_name: Name of the symbol to search\n",
    "    \n",
    "    Returns:\n",
    "        String with usage information\n",
    "    \"\"\"\n",
    "    response = \"\"\n",
    "    found = False\n",
    "    \n",
    "    # Look in snippet nodes\n",
    "    for node_id, node_data in G.nodes(data=True):\n",
    "        if node_data.get('type') == 'snippet':\n",
    "            code_snippet = node_data.get('code_snippet', '')\n",
    "            if code_snippet and symbol_name.lower() in code_snippet.lower():\n",
    "                found = True\n",
    "                file_name = \"Unknown\"\n",
    "                # Find which file this snippet belongs to\n",
    "                for source, target, edge_data in G.in_edges(node_id, data=True):\n",
    "                    if edge_data.get('edge_type') == 'contains_snippet':\n",
    "                        file_name = source\n",
    "                        break\n",
    "                \n",
    "                start_line = node_data.get('start_line', 'unknown')\n",
    "                response += f\"\\nFile: {file_name} (Lines {start_line}-{node_data.get('end_line', 'unknown')})\\n\"\n",
    "                \n",
    "                # Extract the lines containing the symbol\n",
    "                lines = code_snippet.split('\\n')\n",
    "                for i, line in enumerate(lines):\n",
    "                    if symbol_name.lower() in line.lower():\n",
    "                        line_num = int(start_line) + i if isinstance(start_line, int) else \"?\"\n",
    "                        response += f\"- Line {line_num}: {line.strip()}\\n\"\n",
    "    \n",
    "    # Look in edge contexts\n",
    "    for source, target, edge_data in G.edges(data=True):\n",
    "        context = edge_data.get('context', '')\n",
    "        if context and symbol_name.lower() in context.lower():\n",
    "            found = True\n",
    "            edge_type = edge_data.get('edge_type', 'unknown')\n",
    "            line_num = edge_data.get('line_number', 'unknown')\n",
    "            \n",
    "            # For references or defines edges, source is usually the file\n",
    "            file_name = source if edge_type in ['references', 'defines'] else \"Unknown\"\n",
    "            \n",
    "            response += f\"\\nFile: {file_name} (Line {line_num})\\n\"\n",
    "            response += f\"- {context.strip()}\\n\"\n",
    "    \n",
    "    if found:\n",
    "        return response\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def find_symbol_usage_nx(graph, node_id, symbol_name):\n",
    "    \"\"\"\n",
    "    Find all usages of a specific symbol across the codebase using NetworkX graph\n",
    "    \n",
    "    Args:\n",
    "        graph: The NetworkX graph object\n",
    "        node_id: The node ID of the symbol node in the graph\n",
    "        symbol_name: The name of the symbol for display purposes\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with files and line numbers where the symbol is used\n",
    "    \"\"\"\n",
    "    # Check if this is a valid node\n",
    "    if node_id not in graph:\n",
    "        # This could be a pseudo node ID we created for snippet matches\n",
    "        if isinstance(node_id, str) and '::' in node_id:\n",
    "            file_path = node_id.split('::')[0]\n",
    "            # Return information based on file path and symbol name\n",
    "            return {\n",
    "                \"symbol\": symbol_name,\n",
    "                \"symbol_type\": \"variable\",  # Assuming variable as default\n",
    "                \"defined_in\": [file_path],\n",
    "                \"docstring\": \"\",\n",
    "                \"usages\": search_symbol_in_file(graph, file_path, symbol_name),\n",
    "                \"related_symbols\": []\n",
    "            }\n",
    "        return {\"error\": f\"Symbol '{symbol_name}' not found in the codebase as a specific node\"}\n",
    "    \n",
    "    # Get symbol node data\n",
    "    node_data = graph.nodes[node_id]\n",
    "    \n",
    "    # Extract symbol type and other metadata\n",
    "    symbol_type = node_data.get('symbol_type', 'variable')  # Default to variable\n",
    "    docstring = node_data.get('docstring', '')\n",
    "    \n",
    "    # Find definition locations\n",
    "    definitions = []\n",
    "    for source, target, edge_data in graph.in_edges(node_id, data=True):\n",
    "        if edge_data.get('edge_type') == 'defines':\n",
    "            # Source should be a file node\n",
    "            definitions.append(source)\n",
    "    \n",
    "    # If no definitions found through edges, extract from node ID\n",
    "    if not definitions and isinstance(node_id, str) and '::' in node_id:\n",
    "        file_path = node_id.split('::')[0]\n",
    "        definitions.append(file_path)\n",
    "    \n",
    "    # Find all references to the symbol\n",
    "    usages = {}\n",
    "    for source, target, edge_data in graph.in_edges(node_id, data=True):\n",
    "        edge_type = edge_data.get('edge_type')\n",
    "        \n",
    "        # Consider references\n",
    "        if edge_type == 'references':\n",
    "            # Source should be a file node\n",
    "            file_name = source\n",
    "            \n",
    "            # Extract line info and context\n",
    "            line_info = edge_data.get('line_number', 'unknown line')\n",
    "            context = edge_data.get('context', 'No context available')\n",
    "            \n",
    "            if file_name not in usages:\n",
    "                usages[file_name] = []\n",
    "            \n",
    "            usages[file_name].append({\n",
    "                'line': line_info,\n",
    "                'context': context\n",
    "            })\n",
    "    \n",
    "    # Find related symbols (e.g., symbols in the same file)\n",
    "    related_symbols = []\n",
    "    for def_file in definitions:\n",
    "        for source, target, edge_data in graph.out_edges(def_file, data=True):\n",
    "            if edge_data.get('edge_type') == 'defines' and target != node_id:\n",
    "                # Extract symbol name from target node ID\n",
    "                if isinstance(target, str) and '::' in target:\n",
    "                    related_symbol = target.split('::')[1]\n",
    "                    related_symbols.append(related_symbol)\n",
    "    \n",
    "    return {\n",
    "        \"symbol\": symbol_name,\n",
    "        \"symbol_type\": symbol_type,\n",
    "        \"defined_in\": definitions,\n",
    "        \"docstring\": docstring,\n",
    "        \"usages\": usages,\n",
    "        \"related_symbols\": related_symbols\n",
    "    }\n",
    "\n",
    "def search_symbol_in_file(graph, file_path, symbol_name):\n",
    "    \"\"\"\n",
    "    Search for uses of a symbol within a specific file\n",
    "    \n",
    "    Args:\n",
    "        graph: NetworkX graph\n",
    "        file_path: Path of the file to search in\n",
    "        symbol_name: Name of the symbol to search for\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with usage information\n",
    "    \"\"\"\n",
    "    usages = {}\n",
    "    \n",
    "    # Look for snippet nodes from this file\n",
    "    for node_id, node_data in graph.nodes(data=True):\n",
    "        if node_data.get('type') == 'snippet':\n",
    "            # Check if this snippet belongs to the file\n",
    "            belongs_to_file = False\n",
    "            for source, target, edge_data in graph.in_edges(node_id, data=True):\n",
    "                if edge_data.get('edge_type') == 'contains_snippet' and source == file_path:\n",
    "                    belongs_to_file = True\n",
    "                    break\n",
    "            \n",
    "            if belongs_to_file:\n",
    "                code_snippet = node_data.get('code_snippet', '')\n",
    "                if code_snippet and symbol_name.lower() in code_snippet.lower():\n",
    "                    start_line = node_data.get('start_line', 'unknown')\n",
    "                    \n",
    "                    if file_path not in usages:\n",
    "                        usages[file_path] = []\n",
    "                    \n",
    "                    # Extract the lines containing the symbol\n",
    "                    lines = code_snippet.split('\\n')\n",
    "                    for i, line in enumerate(lines):\n",
    "                        if symbol_name.lower() in line.lower():\n",
    "                            line_num = int(start_line) + i if isinstance(start_line, int) else \"unknown\"\n",
    "                            usages[file_path].append({\n",
    "                                'line': line_num,\n",
    "                                'context': line.strip()\n",
    "                            })\n",
    "    \n",
    "    return usages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The text LLM file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T11:56:24.257102Z",
     "iopub.status.busy": "2025-03-01T11:56:24.256777Z",
     "iopub.status.idle": "2025-03-01T11:56:24.265398Z",
     "shell.execute_reply": "2025-03-01T11:56:24.264465Z",
     "shell.execute_reply.started": "2025-03-01T11:56:24.257079Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def convert_arango_to_networkx(db, graph_name):\n",
    "    \"\"\"\n",
    "    Convert an ArangoDB graph to NetworkX format\n",
    "    \n",
    "    Args:\n",
    "        db: ArangoDB database connection\n",
    "        graph_name: Name of the graph in ArangoDB\n",
    "    \n",
    "    Returns:\n",
    "        NetworkX graph object\n",
    "    \"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    try:\n",
    "        # Get graph from ArangoDB\n",
    "        arango_graph = db.graph(graph_name)\n",
    "        \n",
    "        # Get graph properties\n",
    "        try:\n",
    "            graph_properties = arango_graph.properties()\n",
    "            edge_definitions = graph_properties.get('edgeDefinitions', [])\n",
    "            \n",
    "            # Process each vertex collection\n",
    "            vertex_collections = set()\n",
    "            for edge_def in edge_definitions:\n",
    "                vertex_collections.update(edge_def.get('from', []))\n",
    "                vertex_collections.update(edge_def.get('to', []))\n",
    "                \n",
    "            # Add nodes from vertex collections\n",
    "            for collection_name in vertex_collections:\n",
    "                try:\n",
    "                    collection = db.collection(collection_name)\n",
    "                    for doc in collection:\n",
    "                        node_id = doc['_id']\n",
    "                        G.add_node(node_id, **doc)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error with vertex collection {collection_name}: {str(e)}\")\n",
    "            \n",
    "            # Process edge collections\n",
    "            for edge_def in edge_definitions:\n",
    "                edge_collection_name = edge_def.get('collection')\n",
    "                if edge_collection_name:\n",
    "                    try:\n",
    "                        collection = db.collection(edge_collection_name)\n",
    "                        for doc in collection:\n",
    "                            from_id = doc['_from']\n",
    "                            to_id = doc['_to']\n",
    "                            G.add_edge(from_id, to_id, **doc)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error with edge collection {edge_collection_name}: {str(e)}\")\n",
    "                        \n",
    "        except Exception as e:\n",
    "            # Alternative approach: try to infer collections from naming convention\n",
    "            print(f\"Error getting graph properties, trying alternate approach: {str(e)}\")\n",
    "            \n",
    "            # Common naming patterns for ArangoDB collections\n",
    "            node_collection_pattern = re.compile(f\"{re.escape(graph_name)}_node\")\n",
    "            edge_collection_pattern = re.compile(f\"{re.escape(graph_name)}_.*_to_.*\")\n",
    "            \n",
    "            # Try to find matching collections\n",
    "            for collection_name in db.collections():\n",
    "                if node_collection_pattern.match(collection_name):\n",
    "                    try:\n",
    "                        collection = db.collection(collection_name)\n",
    "                        for doc in collection:\n",
    "                            node_id = doc['_id']\n",
    "                            G.add_node(node_id, **doc)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error with inferred vertex collection {collection_name}: {str(e)}\")\n",
    "                        \n",
    "                elif edge_collection_pattern.match(collection_name):\n",
    "                    try:\n",
    "                        collection = db.collection(collection_name)\n",
    "                        for doc in collection:\n",
    "                            from_id = doc['_from']\n",
    "                            to_id = doc['_to']\n",
    "                            G.add_edge(from_id, to_id, **doc)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error with inferred edge collection {collection_name}: {str(e)}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing ArangoDB graph: {str(e)}\")\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T11:56:20.296881Z",
     "iopub.status.busy": "2025-03-01T11:56:20.296554Z",
     "iopub.status.idle": "2025-03-01T11:56:20.304595Z",
     "shell.execute_reply": "2025-03-01T11:56:20.303580Z",
     "shell.execute_reply.started": "2025-03-01T11:56:20.296856Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def text_to_nx_algorithm_to_text(query_text, graph=None, db_connection=None, graph_name=None):\n",
    "    \"\"\"\n",
    "    Universal entry point for processing natural language queries against a code graph\n",
    "    \n",
    "    Args:\n",
    "        query_text: Natural language query about the codebase\n",
    "        graph: NetworkX graph object (if already available)\n",
    "        db_connection: ArangoDB connection (if graph should be loaded from ArangoDB)\n",
    "        graph_name: Name of the graph in ArangoDB (if db_connection is provided)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with analysis results in a human-readable format\n",
    "    \"\"\"\n",
    "    # Case 1: NetworkX graph is directly provided\n",
    "    if graph is not None:\n",
    "        if isinstance(graph, nx.Graph):\n",
    "            return analyze_networkx_graph(graph, query_text)\n",
    "        else:\n",
    "            return {\"response\": \"Provided graph object is not a valid NetworkX graph\"}\n",
    "    \n",
    "    # Case 2: ArangoDB connection is provided but no graph\n",
    "    elif db_connection is not None and graph_name is None:\n",
    "        try:\n",
    "            # Try to list available graphs\n",
    "            try:\n",
    "                available_graphs = db_connection.graphs()\n",
    "                if available_graphs:\n",
    "                    graph_name = available_graphs[0][\"name\"]\n",
    "                else:\n",
    "                    return {\"response\": \"No graphs found in the ArangoDB database\"}\n",
    "            except:\n",
    "                # Different API for networkx-arangodb\n",
    "                try:\n",
    "                    import networkx_arangodb as nxadb\n",
    "                    # Try a different approach for networkx-arangodb\n",
    "                    available_graphs = [g for g in db_connection.graphs()]\n",
    "                    if available_graphs:\n",
    "                        graph_name = available_graphs[0]\n",
    "                    else:\n",
    "                        return {\"response\": \"No graphs found in the ArangoDB database\"}\n",
    "                except:\n",
    "                    return {\"response\": \"Could not retrieve graph list from ArangoDB\"}\n",
    "        except Exception as e:\n",
    "            return {\"response\": f\"Error retrieving graphs from ArangoDB: {str(e)}\"}\n",
    "        \n",
    "    \n",
    "    # Case 3: ArangoDB connection and graph name are provided\n",
    "    if db_connection is not None and graph_name is not None:\n",
    "        try:\n",
    "            # Try to load the graph using standard ArangoDB driver\n",
    "            try:\n",
    "                # Check if graph_name is a string\n",
    "                if not isinstance(graph_name, str):\n",
    "                    if hasattr(graph_name, 'name'):\n",
    "                        # It might be a graph object with a name attribute\n",
    "                        graph_name = graph_name.name\n",
    "                    else:\n",
    "                        return {\"response\": \"Graph name must be a string or an object with a name attribute\"}\n",
    "                \n",
    "                # Try to get the graph from ArangoDB\n",
    "                arango_graph = db_connection.graph(graph_name)\n",
    "                # Convert to NetworkX using our custom function\n",
    "                nx_graph = convert_arango_to_networkx(db_connection, graph_name)\n",
    "                \n",
    "                return analyze_networkx_graph(nx_graph, query_text)\n",
    "            except Exception as e1:\n",
    "                # If standard approach fails, try networkx-arangodb\n",
    "                try:\n",
    "                    import networkx_arangodb as nxadb\n",
    "                    graph = nxadb.Graph(name=graph_name, db=db_connection)\n",
    "                    nx_graph = graph.to_networkx()\n",
    "                    print(type(nx_graph))\n",
    "                    return analyze_networkx_graph(nx_graph, query_text)\n",
    "                except Exception as e2:\n",
    "                    return {\"response\": f\"Error loading graph from ArangoDB: {str(e1)}\\nAlternative method error: {str(e2)}\"}\n",
    "        except Exception as e:\n",
    "            return {\"response\": f\"Error accessing ArangoDB: {str(e)}\"}\n",
    "    \n",
    "    # Case 4: No graph info provided at all\n",
    "    return {\"response\": \"Please provide either a NetworkX graph or ArangoDB connection details\"}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting it to NETWORKX, can be removed for effiency and maintained in arango"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nx_arangodb.classes.graph.Graph"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(G_adb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_Graph__db', '_Graph__graph_exists_in_db', '_Graph__is_smart', '_Graph__name', '_Graph__set_arangodb_backend_config', '_Graph__set_db', '_Graph__set_edge_collections_attributes', '_Graph__set_graph', '_Graph__smart_field', '_Graph__use_arango_views', '__annotations__', '__class__', '__contains__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__networkx_backend__', '__networkx_cache__', '__networkx_plugin__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_adj', '_db_name', '_edge_collections_attributes', '_hosts', '_load_nx_graph', '_loaded_incoming_graph_data', '_node', '_password', '_set_factory_methods', '_username', 'adb_graph', 'add_edge', 'add_edges_from', 'add_node', 'add_node_override', 'add_nodes_from', 'add_nodes_from_override', 'add_weighted_edges_from', 'adj', 'adjacency', 'adjlist_inner_dict_factory', 'adjlist_outer_dict_factory', 'chat', 'clear', 'clear_edges', 'clear_edges_override', 'clear_nxcg_cache', 'clear_override', 'copy', 'copy_override', 'db', 'default_node_type', 'degree', 'edge_attr_dict_factory', 'edge_attributes', 'edge_subgraph', 'edge_type_func', 'edge_type_key', 'edges', 'get_edge_data', 'graph', 'graph_attr_dict_factory', 'graph_exists_in_db', 'has_edge', 'has_node', 'is_directed', 'is_multigraph', 'is_smart', 'name', 'nbunch_iter', 'nbunch_iter_override', 'neighbors', 'node_attr_dict_factory', 'node_dict_factory', 'nodes', 'number_of_edges', 'number_of_edges_override', 'number_of_nodes', 'nxcg_graph', 'order', 'query', 'read_batch_size', 'read_parallelism', 'remove_edge', 'remove_edges_from', 'remove_node', 'remove_nodes_from', 'size', 'smart_field', 'subgraph', 'subgraph_override', 'symmetrize_edges', 'to_directed', 'to_directed_class', 'to_networkx_class', 'to_undirected', 'to_undirected_class', 'update', 'use_nxcg_cache']\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# Create an empty NetworkX DiGraph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Check what methods and attributes are available on your G_adb object\n",
    "# Uncomment this line to inspect what's available:\n",
    "print(dir(G_adb))\n",
    "\n",
    "# Try accessing nodes and edges directly\n",
    "# Option 1: If G_adb has nodes() and edges() methods\n",
    "try:\n",
    "    # Add nodes\n",
    "    for node_id, node_data in G_adb.nodes(data=True):\n",
    "        G.add_node(node_id, **node_data)\n",
    "    \n",
    "    # Add edges\n",
    "    for source, target, edge_data in G_adb.edges(data=True):\n",
    "        G.add_edge(source, target, **edge_data)\n",
    "except AttributeError:\n",
    "    # Option 2: If G_adb has direct node and edge collections\n",
    "    try:\n",
    "        # Check if G_adb.nodes and G_adb.edges are iterable\n",
    "        for node_id, node_data in G_adb.nodes.items():\n",
    "            G.add_node(node_id, **node_data)\n",
    "        \n",
    "        for edge_id, edge_data in G_adb.edges.items():\n",
    "            # Assuming edge_data has 'from' and 'to' attributes\n",
    "            G.add_edge(edge_data['from'], edge_data['to'], **edge_data)\n",
    "    except AttributeError:\n",
    "        print(\"Could not determine how to access nodes and edges. Please inspect G_adb structure.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T11:59:05.742863Z",
     "iopub.status.busy": "2025-03-01T11:59:05.742479Z",
     "iopub.status.idle": "2025-03-01T11:59:05.770985Z",
     "shell.execute_reply": "2025-03-01T11:59:05.770030Z",
     "shell.execute_reply.started": "2025-03-01T11:59:05.742833Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symbol: 'has_level_handler()' (Type: function)\n",
      "\n",
      "Defined in: No definition location found\n",
      "\n",
      "Used in the following locations:\n",
      "\n",
      "No usage information found for this symbol in graph nodes.\n",
      "\n",
      "However, found these mentions in code snippets:\n",
      "\n",
      "\n",
      "File: Unknown (Lines 61-80)\n",
      "- Line 70: def test_has_level_handler():\n",
      "\n",
      "File: FlaskRepv1_node/213 (Line 67)\n",
      "- assert wsgi_errors_stream._get_current_object() is sys.stderr\n",
      "\n",
      "    with app.test_request_context(errors_stream=stream):\n",
      "        assert wsgi_errors_stream._get_current_object() is stream\n",
      "\n",
      "\n",
      "def test_has_level_handler():\n",
      "\n",
      "Analysis performed on graph with 2975 nodes and 8651 edges\n"
     ]
    }
   ],
   "source": [
    "result = text_to_nx_algorithm_to_text(\n",
    "    \"has_level_handler()\",\n",
    "    graph=G,  # Your NetworkX graph\n",
    "    #graph_name=\"FlaskRepv1\",\n",
    "    db_connection=db\n",
    ")\n",
    "print(result['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T04:47:01.096433Z",
     "iopub.status.busy": "2025-03-01T04:47:01.096097Z",
     "iopub.status.idle": "2025-03-01T04:47:02.298827Z",
     "shell.execute_reply": "2025-03-01T04:47:02.298082Z",
     "shell.execute_reply.started": "2025-03-01T04:47:01.096408Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(G_adb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T06:37:40.954596Z",
     "iopub.status.busy": "2025-03-04T06:37:40.954199Z",
     "iopub.status.idle": "2025-03-04T06:37:40.963268Z",
     "shell.execute_reply": "2025-03-04T06:37:40.961965Z",
     "shell.execute_reply.started": "2025-03-04T06:37:40.954567Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'arango_graph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43marango_graph\u001b[49m\u001b[38;5;241m.\u001b[39mschema\n",
      "\u001b[0;31mNameError\u001b[0m: name 'arango_graph' is not defined"
     ]
    }
   ],
   "source": [
    "arango_graph.schema"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
